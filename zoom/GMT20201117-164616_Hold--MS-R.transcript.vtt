WEBVTT

1
00:00:03.210 --> 00:00:08.340
Joe Hellerstein: And Chang, perhaps we can find some pointers to talks online for Anna and Cloudburst to point people to

2
00:00:09.059 --> 00:00:09.750
Yet

3
00:00:12.900 --> 00:00:14.490
Joe Hellerstein: Right, off you go. Michael

4
00:00:16.289 --> 00:00:29.700
Michael Whittaker: All right. Thanks, Joe. Um, yeah. So hi, everyone. My name is Michael, I'm going to give about a 15 minute or so talk on compartmentalize Paxos and then I'm going to hand it over to Kaushik and David to talk about some of the stuff they're working on, which is very much related

5
00:00:31.080 --> 00:00:39.540
Michael Whittaker: So I think that probably most of you, if not all of you are at least somewhat familiar with the machine application and consensus and multi Paxos and stuff like that.

6
00:00:39.630 --> 00:00:45.600
Michael Whittaker: But I'm going to very quickly go over a little bit of background. Anyway, just to make sure we're all on the same page. Make sure that it's fresh in our minds.

7
00:00:46.230 --> 00:00:50.040
Michael Whittaker: Alright, so let's say we want to implement some key value store.

8
00:00:50.880 --> 00:01:00.060
Michael Whittaker: The most obvious and the most simple way to implement that is to have a single server that runs the key value store. So here I have a single server s which implements a key value service.

9
00:01:00.900 --> 00:01:11.760
Michael Whittaker: Now there's some client wants to perform some operation on the key value store, for example, six. The one. It just sends the operation to the server. The server does it sends back a response, maybe another client wants to get it just sends the get

10
00:01:12.300 --> 00:01:26.070
Michael Whittaker: The server does it sends back a response. It's very straightforward. Unfortunately machines fail. So this approach, although simple is not fault tolerant, if our single server fails, then we're hosed. We've lost all of our data and our system is completely dead.

11
00:01:27.450 --> 00:01:36.180
Michael Whittaker: All right, we can try to address this laugh at fault tolerance by instead of having a single key value server, we can have multiple copies or multiple replicas of the key value server.

12
00:01:36.510 --> 00:01:39.420
Michael Whittaker: So here I have three replicas of the key value server.

13
00:01:39.960 --> 00:01:47.760
Michael Whittaker: And when a client wants to perform an operation, instead of sending it to one server sends it to all of them. They all perform the operation and then maybe one of them sends back a response.

14
00:01:48.420 --> 00:01:56.580
Michael Whittaker: So this is fault tolerance, if one of the servers fails on. No problem. I have to order copies, ready to go. So it is fault tolerant.

15
00:01:57.360 --> 00:02:08.460
Michael Whittaker: But maybe obvious to some of you. It's not a very consistent. So you can imagine if to clients simultaneously send to request at the same time, one wants to set x, the one wants one wants to sit next to

16
00:02:08.850 --> 00:02:19.620
Michael Whittaker: The different replicas can receive these requests in different orders and if they set one and then two or two and then one they're going to end up in different states, build a bridge. So we were fault tolerant, but unfortunately we're also inconsistent.

17
00:02:20.670 --> 00:02:32.010
Michael Whittaker: At a very high level. If I view multi-packs us, which is the same machine application protocol as a black box multi-packs else is trying to get the best of both of these things we want to be both fault tolerant and consistent

18
00:02:32.760 --> 00:02:40.860
Michael Whittaker: Right. So if we look at multiplex us at a high level clients are going to send some requests to some state machine here I've

19
00:02:41.340 --> 00:02:45.450
Michael Whittaker: Showed key value stores will get some sets, but it could be some arbitrary state machine with arbitrary request.

20
00:02:45.990 --> 00:02:51.300
Michael Whittaker: The clients will send those requests the multi Paxos multi-packs hosts will do some sort of magic to decide.

21
00:02:51.690 --> 00:02:59.340
Michael Whittaker: What order should execute these requests and then they'll forward the requests to the replicas, and make sure that they all execute them in the same order here. It's blue and the green

22
00:03:00.120 --> 00:03:09.690
Michael Whittaker: And by having the replicas execute all the same commands and the same exact order, we always make sure we're consistent and because I have multiple copies. If one of them bills. It's not a big deal. So I'm also called Pollard

23
00:03:12.090 --> 00:03:12.390
Okay.

24
00:03:13.440 --> 00:03:21.000
Michael Whittaker: So fault tolerant inconsistent. That's exactly what we want. Alright, so now let's take a closer look at multi Paxos unwrap the black box and

25
00:03:21.600 --> 00:03:26.970
Michael Whittaker: See how the protocol works. So a multi packs us deployment consists of some number of clients.

26
00:03:27.660 --> 00:03:35.850
Michael Whittaker: At plus one nodes which I call proposers and here F is the number of machines is infant fail. It's honestly not that important for this talk, you can just ignore it.

27
00:03:36.480 --> 00:03:48.000
Michael Whittaker: One of these proposals, I'm going to elect to be the leader. So I drive with the crown. I'll have to have plus one knows I call except there's and f plus one replica. So these would be like the key value stores the replicas of our staging

28
00:03:49.440 --> 00:03:57.630
Michael Whittaker: Alright, so how does the protocol work if I have some clients and I want to perform some operation here, I just say a you can imagine this is like a set request I send it to the leader.

29
00:03:58.500 --> 00:04:05.010
Michael Whittaker: Right. And now the leader and this protocol. What are they going to do well they need to make sure that every rep that executes every command in the same order.

30
00:04:05.490 --> 00:04:19.260
Michael Whittaker: To do that multi packs us is going to place every command in a log and then the replicas are going to execute the commands in log word and remember from our high level view of multiplexes as black box that's exactly we want everyone execute everything in the same order.

31
00:04:20.280 --> 00:04:26.550
Michael Whittaker: Alright so this leader when it gets a it's going to put it in some log. It's going to put it in a log entry here, it puts it in log entry zero

32
00:04:27.960 --> 00:04:38.010
Michael Whittaker: Then the leader is going to perform one round trip of communication with the exhibitors and. It's at this point that I have to tell you that I'm actually not really going to be explaining multi-packs us in detail.

33
00:04:38.700 --> 00:04:43.920
Michael Whittaker: If you don't know multiple access this probably won't make sense. But thankfully, you don't have to understand it for the talk.

34
00:04:44.850 --> 00:04:50.940
Michael Whittaker: You're sort of just going to have to trust me when I go over this high level flow of communication. That's really all you have to know for this talk.

35
00:04:51.660 --> 00:04:56.190
Michael Whittaker: Alright, so the leader, for some reason, which I'm not explaining performs the round trip of communication with a sectors.

36
00:04:56.460 --> 00:05:08.430
Michael Whittaker: And then forwards the command over to the replicas, the replica say okay I guess I should put a and log entry zero. That's what the leaders telling me, then the execute that command and one of them sense back the response on Thanksgiving. The command to the quiet.

37
00:05:09.930 --> 00:05:17.970
Michael Whittaker: All right, let's take a look at another one. So we go be the leader leader puts it in log round trip to the exhibitors since the replicas, put it in log executed sent back

38
00:05:20.550 --> 00:05:24.930
Michael Whittaker: Okay. Is there any questions so far at least at a high level, on the flow of messages.

39
00:05:28.230 --> 00:05:28.830
Okay, perfect.

40
00:05:29.970 --> 00:05:34.470
Michael Whittaker: Okay, so that's multi Paxos. It is well known that the multiplexes leader.

41
00:05:35.040 --> 00:05:40.290
Michael Whittaker: Is a throughput bottleneck. If you pick up the passes paper nine times out of 10 the people will say this.

42
00:05:40.680 --> 00:05:51.810
Michael Whittaker: For example, one paper says it impairs scalability by placing a disproportionately high load on the master the leader impacts us as a bottleneck that limits throughput in practice multi-packs else's performance is tied to the performance level here.

43
00:05:53.310 --> 00:06:08.370
Michael Whittaker: So let's take a look at why that is. It might be obvious to some of you to figure out which node is the throughput bottleneck. We're going to do a sort of back of the envelope calculation and look at for every kind of node. How many messages doesn't have to touch per state machine command.

44
00:06:09.810 --> 00:06:21.630
Michael Whittaker: Will the leader here has to touch for half plus three plus r where r is the number of replicas. You can double check if you want, but it's a lot. That's the point. It's a big number, except for his have to do to each so they received one and they send one

45
00:06:22.800 --> 00:06:29.490
Michael Whittaker: And they're going to either do two or one either they receive and Sandra they receive and do nothing. Well, where they receive execute the commanded don't send a response.

46
00:06:31.620 --> 00:06:32.100
Michael Whittaker: Alright.

47
00:06:32.370 --> 00:06:33.180
Michael Whittaker: So,

48
00:06:33.420 --> 00:06:35.700
Joe Hellerstein: Yeah, go ahead. Joey, will you remind us with F is

49
00:06:36.360 --> 00:06:52.230
Michael Whittaker: Yes, as is I need to make some assumptions on the total allowable number of failures. If I make no such assumption. If everything fails, my protocol, obviously, is not going to work. So here I've assumed that it most F machines can fail and these examples, F equals one.

50
00:06:53.820 --> 00:06:57.570
Michael Whittaker: But yeah, just think about this, some small number, and you'll be good for this talk.

51
00:06:58.800 --> 00:07:13.140
Michael Whittaker: Good question. Joe, thank you. All right. So, from this it should be obvious that the leader is the bottleneck one it touches every request and to it performs disproportionately more work than every other node opponent. So it has to be the bottleneck.

52
00:07:15.240 --> 00:07:22.620
Michael Whittaker: Alright, how are we going to fix this throughput bottleneck. Well, this is a well known thing, like I said,

53
00:07:22.920 --> 00:07:33.300
Michael Whittaker: And access to the particular area of research. So there's been a lot of protocols which I've tried to solve this problem. There's almost one protocol per letter of the alphabet. We have Deepak so see types of deep access and so

54
00:07:34.620 --> 00:07:41.880
Michael Whittaker: Unfortunately, a lot of these papers which try to avoid the leader bottleneck among doing other things are a little complicated.

55
00:07:42.360 --> 00:07:50.580
Michael Whittaker: So if you look at E Paxos, for example, that's a relatively famous protocol and the proof of its correctness is like 10 or so pages long.

56
00:07:51.420 --> 00:07:59.490
Michael Whittaker: The Protocols pretty complicated. It's not easy for me to convince you that it's a complicated protocol, you're going to sort of just have to trust me that it is

57
00:08:00.480 --> 00:08:08.400
Michael Whittaker: And we have some anecdotal evidence that it is, for example, there are some papers published showing these bugs in the protocol that were undiscovered for a long time.

58
00:08:08.700 --> 00:08:15.120
Michael Whittaker: We personally found two bugs in the protocol which we confirm that the authors as well. So trust me when I say these particles get pretty complicated.

59
00:08:16.320 --> 00:08:24.090
Michael Whittaker: Alright, so then my goal is, can I eliminate the leader bottleneck without having to do something super complicated

60
00:08:25.140 --> 00:08:36.630
Michael Whittaker: Answers. Yes, I'm going to show you. All right, so I'm gonna eliminate the leader bottleneck and it's going to be simple. It's going to be hard for me to prove to you that it's simple. I'm just going to show you, and then I'll let you think for yourself if it's simple.

61
00:08:37.950 --> 00:08:45.690
Michael Whittaker: But our reviewers gave us some very kind words. They thought it was so simple. They said things like, it's not novel. So I hope you guys also feel the same way.

62
00:08:46.860 --> 00:08:53.430
Michael Whittaker: Alright, so here's the trick. All I'm going to do is introduce a new set of notes that I call proxy leaders.

63
00:08:54.630 --> 00:08:56.580
Michael Whittaker: Alright, so at least athletes one proxy leaders.

64
00:08:57.990 --> 00:09:01.260
Michael Whittaker: And then the protocol is going to run pretty much the same as before with only a slight change.

65
00:09:02.130 --> 00:09:12.030
Michael Whittaker: So the client wants to send a request that sends it to the leader, the leader puts into this log it forms a message, but now instead of sending to the accelerators, the leader is going to send this message to one of the proxy leaders.

66
00:09:13.230 --> 00:09:14.520
Michael Whittaker: So here it sends it to L three

67
00:09:16.110 --> 00:09:28.560
Michael Whittaker: Now the proxy leader is going to be the one to perform the round trip of communication, the exception rather than the leader and then the leader will pick up. Sorry. The proximity to will pick up the protocol as before, sent to the replica is putting your log execute Santa Clara.

68
00:09:31.050 --> 00:09:43.740
Michael Whittaker: Alright, so one thing to note here is that the leader can choose any proxies leader. So here I sent be puts it in the log and I sent it out to instead of L three like I did before I can send any proxy there doesn't matter which and then we run the protocol.

69
00:09:45.300 --> 00:09:57.450
Michael Whittaker: Okay, so how does this help. Well, now if we do the same sort of back of the envelope calculations as before, we see that the leader now only processes to messages they received one from the client and it sends one to randomly chosen proximity to

70
00:09:58.410 --> 00:10:05.340
Michael Whittaker: Be accepted and replicas are unchanged and the proxy that are now processes. That's big for athletes to people. Sorry. But the thing to note now.

71
00:10:05.820 --> 00:10:18.750
Michael Whittaker: Is that the load can be load balanced across the proxy leaders. Alright, so if the proxy leader is ever throughput bottleneck. I just add another. And if it's still a bottleneck I on another. Alright, so the proxy leaders can't be a leader bottleneck, they scale.

72
00:10:20.460 --> 00:10:24.210
Michael Whittaker: Alright. So in summary, what we've done is we've realized the leader really had to responsibilities.

73
00:10:24.480 --> 00:10:35.220
Michael Whittaker: It sequences commands into the log and it communicates with the accessories and we're noting that those responsibilities don't have to be done by the same issue. So we decouple those the responsibilities and then we scale up the number of Fox News.

74
00:10:36.930 --> 00:10:37.800
Michael Whittaker: Any questions so far.

75
00:10:42.330 --> 00:10:42.960
Michael Whittaker: No questions.

76
00:10:44.460 --> 00:10:45.120
Joe Hellerstein: Pretty simple.

77
00:10:46.290 --> 00:10:57.150
Michael Whittaker: Good. Yeah, I think so. Alright, so now that we've eliminated or at least alleviated the leader as a few bottleneck. What is the new throughput bottleneck.

78
00:10:58.020 --> 00:11:07.140
Michael Whittaker: Now, it's not so obvious. It's definitely not the proxy leaders on but the leader the accelerators and the replicas all touch either one or two messages so

79
00:11:07.680 --> 00:11:16.920
Michael Whittaker: The component which is going to be the bottleneck really depends on a lot of things. It depends on your implementation, the optimizations you perform your workload, the network, etc.

80
00:11:17.850 --> 00:11:28.950
Michael Whittaker: So it's going to depend. I can't say for sure which one will be the bottleneck. But in our experience. It's not unrealistic that the acceptor is could become the bottleneck, they touched two messages, just like the other nodes and they're doing a little bit more work than later.

81
00:11:30.030 --> 00:11:40.830
Michael Whittaker: So let's try to avoid the accelerators as a throughput bottleneck. I'd like to be able to scale up the number of exceptions, but unfortunately there's this fact that the multi packs of exceptions don't scale.

82
00:11:42.120 --> 00:11:55.020
Michael Whittaker: Paper explains it nicely. It says using more than two plus one acceptance for failures as possible but you logical because it requires a larger size with no additional benefit essentially that says of add more accepted as my protocol gets worse, not better.

83
00:11:56.610 --> 00:12:03.930
Michael Whittaker: Well, it turns out that's not true. Actually this is false. The multi Paxos except there's do scale. You just have to add more except there's in a slightly clever way.

84
00:12:04.920 --> 00:12:14.550
Michael Whittaker: So all we're going to do is instead of having one single group of executors I'm going to have multiple groups of accelerators. So we are here, I have three groups, the top group. The middle group and the bottom group.

85
00:12:16.440 --> 00:12:24.720
Michael Whittaker: And all I'm going to do is I'm going to take this log. Right. Remember, we're replicating this log and I'm going to round robin partition ownership of the log between the different except for groups.

86
00:12:25.500 --> 00:12:31.320
Michael Whittaker: So let's say I have three except for groups, a one, a two, and a three. The first log entry or the zero log entry will be assigned to

87
00:12:39.300 --> 00:12:41.100
Joe Hellerstein: Is Michael frozen or my frozen.

88
00:12:43.050 --> 00:12:43.950
Matthew Milano: I see Michael frozen.

89
00:12:44.940 --> 00:12:45.930
Jonathan Goldstein: Frozen each

90
00:12:47.400 --> 00:12:47.850
Joe Hellerstein: Ad

91
00:12:49.020 --> 00:12:52.020
Joe Hellerstein: Is round robin partitioning the log across acceptor groups.

92
00:12:52.140 --> 00:12:57.690
Carlo Aldo Curino: But you see, most sense of that play right here. You didn't hear from everyone by most me know if you told you

93
00:12:58.230 --> 00:12:58.980
A census.

94
00:13:00.030 --> 00:13:01.470
Joe Hellerstein: Genius. Man, I love it.

95
00:13:04.350 --> 00:13:05.010
Natacha Crooks: Petitions.

96
00:13:08.400 --> 00:13:13.950
Joe Hellerstein: Yes, this is a this is the example that proves the rule that you need unanimity here.

97
00:13:16.800 --> 00:13:18.870
Carlo Aldo Curino: I'm just not gonna acknowledge that one and leave you

98
00:13:19.770 --> 00:13:20.880
Joe Hellerstein: All you so meta

99
00:13:24.630 --> 00:13:26.250
Joe Hellerstein: Well. Cuz everybody a chance to breathe.

100
00:13:34.050 --> 00:13:39.690
Jonathan Goldstein: So I don't really use zoom that often does this happen frequently would super is this kind of a rare occurrence.

101
00:13:40.170 --> 00:13:44.580
Joe Hellerstein: It's a rare occurrence. I'd say I haven't had this problem with Michael or zoom. So a

102
00:13:45.480 --> 00:13:49.110
Natacha Crooks: Couple of days has kicked me out of zoom info into my zoom pretty frequently actually

103
00:13:50.400 --> 00:13:51.720
Natacha Crooks: Makes sense. Friday. It's been pretty

104
00:13:53.430 --> 00:13:53.880
Interesting.

105
00:13:55.410 --> 00:13:56.370
Carlo Aldo Curino: We have a product for that.

106
00:13:57.150 --> 00:13:59.970
Joe Hellerstein: It's true. We don't have a license to it, I think.

107
00:14:01.650 --> 00:14:05.280
Joe Hellerstein: We for in their infinite wisdom the campus went Google and zoom

108
00:14:07.140 --> 00:14:09.690
Joe Hellerstein: That was a deal that was done like a decade ago.

109
00:14:11.070 --> 00:14:12.720
Joe Hellerstein: That the zoom part but the Google part

110
00:14:13.500 --> 00:14:20.850
Carlo Aldo Curino: I think on video is fine is is just the fact that the, I think the integration with charts and you know documents and etc in teams. I think it's very convenient.

111
00:14:24.000 --> 00:14:24.630
Joe Hellerstein: Well,

112
00:14:25.650 --> 00:14:39.780
Joe Hellerstein: Um, I can put up Michael slides and if he's really literally unable to come back and either. I can finish out the talk, or I don't know if David, you want to try. I'm happy to do it.

113
00:14:45.090 --> 00:14:55.410
Joe Hellerstein: Let me, let me dig up the link to the slides just one of you, if one of you guys that David or Kaushik has the link to the slides and wants to project that would actually be the simplest otherwise I can find it.

114
00:15:03.060 --> 00:15:06.510
Joe Hellerstein: Okay, David. Thank you. Can you go ahead and project.

115
00:15:10.920 --> 00:15:16.350
Joe Hellerstein: All right, well, I'm going to pretend to be Michael Whitaker, he just selected his Internet died.

116
00:15:17.730 --> 00:15:22.920
Joe Hellerstein: With the fact that I can talk for you if you can't get

117
00:15:24.990 --> 00:15:34.620
Joe Hellerstein: So here we are. So what's going on, obviously, is we're round robin partitioning the log across these acceptor groups and you can think of each log entry as an independent round of Paxos

118
00:15:35.340 --> 00:15:41.940
Joe Hellerstein: So really we haven't changed the nature of the protocol, except to say that every log entry belongs to a different accepted group.

119
00:15:43.320 --> 00:15:43.920
Joe Hellerstein: Next slide.

120
00:15:45.510 --> 00:15:49.470
Joe Hellerstein: So the key idea a single log entry requires a fixed set of accelerators.

121
00:15:51.180 --> 00:16:04.380
Joe Hellerstein: Oops, different login trees can have different sets of acceptor so this issue of scaling the acceptor groups when you think about it as being sort of shorted by log entry it scales just fine. By starting by log entry.

122
00:16:05.850 --> 00:16:17.970
Joe Hellerstein: Alright, next slide. So I here's an illustration of the protocol right the first round is going to the first acceptor group and then the replicas and back to the requesters and the key idea is, is as

123
00:16:19.200 --> 00:16:29.040
Joe Hellerstein: Michael says decouple I use the word shard the log entries from one another. This allows you to scale up the acceptor groups and remove bottlenecks in the way we shared queries or what have you. Yeah.

124
00:16:30.450 --> 00:16:40.260
Joe Hellerstein: So one slide back so Michael uses this term compartmentalization for this general scheme that we're going to try to get in the guts of the protocol.

125
00:16:40.560 --> 00:16:45.780
Joe Hellerstein: decouple the pieces that can be decoupled and scale them out independently. So we're really putting

126
00:16:46.260 --> 00:16:52.500
Joe Hellerstein: A magnifying glass on the protocol and applying scaling techniques that we know from elsewhere to the internals of the protocol.

127
00:16:53.490 --> 00:17:01.110
Joe Hellerstein: Okay. And so some of the tricks that are played in the paper we increase the number of replicas. We look at smarter matching

128
00:17:01.560 --> 00:17:06.270
Joe Hellerstein: Michael's been looking at some fancy forums that are going to make things scale nicer and then

129
00:17:06.840 --> 00:17:18.330
Joe Hellerstein: One of the real tricks. That's fun is for read only workloads are for read mostly workloads, you'd really like the protocol to go very fast. So how can you optimize for read mostly or read only workloads. And so that's part of the work that's going on as well.

130
00:17:20.430 --> 00:17:30.840
Joe Hellerstein: Some performance results, you know, the unwrapped allocated chart right that's sort of ideal behavior where you don't have any replicas at all. So you're going at the speed of

131
00:17:31.500 --> 00:17:39.960
Joe Hellerstein: Of the replica really the speed of the work that needs to get done everything else is going to have that work replicated and so it can't go faster. It could only go slower.

132
00:17:40.830 --> 00:17:51.930
Joe Hellerstein: And what you're seeing is that traditional multi Paxos throughput hits a bottleneck very quickly at that leader bottleneck, whereas compartmentalised multiplexes can go quite a bit further.

133
00:17:54.570 --> 00:18:00.570
Joe Hellerstein: You know, in terms of throughput. And so remember, this is all throughput optimization, as opposed to

134
00:18:02.400 --> 00:18:04.860
Joe Hellerstein: As opposed to link to optimization latency is kind of the same.

135
00:18:07.020 --> 00:18:15.030
Joe Hellerstein: Okay. And I can't tell you what's different in this chart. I don't know. David, if you remember the distinction between these two experimental settings.

136
00:18:20.580 --> 00:18:24.330
Joe Hellerstein: Well, maybe we can now pull Michael back for some of that performance is very good.

137
00:18:27.390 --> 00:18:37.500
Joe Hellerstein: Oh, and this is the read. Read Only optimizations. And so you can see when we've got 100% reads and we didn't get time to to go through the protocol, but with 100% reads

138
00:18:37.740 --> 00:18:45.420
Joe Hellerstein: It really skills very beautifully in terms of throughput and as you ramp down the ratio of reads, there's actually kind of a complicated arithmetic.

139
00:18:46.020 --> 00:18:52.320
Joe Hellerstein: Where it doesn't ramp down. What would look to be linearly because of the arithmetic of proportions between reads and writes and

140
00:18:52.620 --> 00:19:04.020
Joe Hellerstein: That's something I won't get into here. But we wouldn't expect, actually, once you do the math for these lines to be staggered in a linear fashion. They're actually staggered in a in a somewhat more skewed fashion due to the way that the ratio is workout.

141
00:19:06.390 --> 00:19:08.310
Joe Hellerstein: And we'll just work through these quickly.

142
00:19:08.820 --> 00:19:12.480
Joe Hellerstein: Alright, so with that I think we can hand off, Michael. Welcome back.

143
00:19:13.170 --> 00:19:15.750
Michael Whittaker: Yeah, sorry about that my internet decided to kill itself.

144
00:19:16.200 --> 00:19:25.140
Joe Hellerstein: I'm not as good a presenter as you add in particular, I, I didn't get the different experimental setups explained, but I think we should probably press on.

145
00:19:26.100 --> 00:19:29.820
Michael Whittaker: Yeah, yeah, yeah. Moral of the story decouple scaling goes fast.

146
00:19:31.560 --> 00:19:31.920
All right.

147
00:19:38.400 --> 00:19:42.780
Kaushik Shivakumar: Are you able to see my screen. Yes. Okay. Great.

148
00:19:46.950 --> 00:19:47.520
Kaushik Shivakumar: So,

149
00:19:48.570 --> 00:20:08.160
Kaushik Shivakumar: everyone my name is Kaushik and I'll be talking briefly about auto compartmentalization. So the key idea or key question that we're asking here is, can we sort of discover what Michael came up with automatically just based on the program or the protocol itself.

150
00:20:09.510 --> 00:20:13.770
Kaushik Shivakumar: So basically, can we convert any arbitrary protocol into both

151
00:20:14.940 --> 00:20:27.390
Kaushik Shivakumar: I guess a compartmentalised version of itself, and also one that can auto scale. So that's the main question we want to address. So one of the key insights from multi axis is that

152
00:20:28.680 --> 00:20:31.650
Kaushik Shivakumar: as Michael mentioned basically every

153
00:20:32.700 --> 00:20:36.000
Kaushik Shivakumar: log entry is independent of one another so

154
00:20:37.110 --> 00:20:46.230
Kaushik Shivakumar: And this leads to the fact that if you look at multifactor us in terms of just the data and the tables involved all of them after the leader.

155
00:20:46.590 --> 00:20:55.590
Kaushik Shivakumar: When the leader assigns the after the leader assigns a log entry position. All of these tables are essentially going to be keyed by the login free

156
00:20:56.040 --> 00:21:10.590
Kaushik Shivakumar: Index. So the pads that the data flows through are going to be independent and that's the key insight that allows you to do the optimizations that Michael mentioned if those interacted in some way, then it wouldn't be very easy to do that optimization

157
00:21:12.660 --> 00:21:19.200
Kaushik Shivakumar: Yeah, so this is just what I mentioned. So the leader receives an orders messages and after that ordering is done.

158
00:21:19.260 --> 00:21:20.670
Every log entries independent

159
00:21:21.930 --> 00:21:35.910
Kaushik Shivakumar: So I guess the overall high level picture here is that you first one. Identify Canada regions. If you look at the data flow graph for a particular protocol you want identify regions that you can compartmentalize

160
00:21:37.200 --> 00:21:46.500
Kaushik Shivakumar: And just gather that information. So in the case of multiplexes the candidate region would essentially be everything after the leader because the leader is the only point

161
00:21:47.460 --> 00:21:56.910
Kaushik Shivakumar: Where you need that coordination and after you identify those candidate regions, you want to filter all of these and

162
00:21:57.420 --> 00:22:03.930
Kaushik Shivakumar: Based on certain criteria. You don't want super small regions you ideally want to choose the larger regions to compartmentalize and also

163
00:22:04.350 --> 00:22:11.220
Kaushik Shivakumar: Within these regions to compartmentalize you might want to further subdivide and scales some some

164
00:22:11.790 --> 00:22:18.390
Kaushik Shivakumar: Regions over others. And this would be done in order to eliminate bottlenecks. So in the case of multiplexes

165
00:22:19.020 --> 00:22:29.310
Kaushik Shivakumar: You can see that you can scale acceptor groups and proxy leaders independently of one another so that you eliminate any bottlenecks and then for each of the

166
00:22:29.820 --> 00:22:38.550
Kaushik Shivakumar: Regions that you choose to compartmentalize you're gonna have to add some overhead, such as keeping track of the IP addresses. If you add more machines and things like that so

167
00:22:39.150 --> 00:22:47.580
Kaushik Shivakumar: In order to transform the good. There's some work that has to be done in order to convert from the original version to the compartmentalised version.

168
00:22:49.110 --> 00:22:59.160
Kaushik Shivakumar: Um, so just an illustrative example, very, very simple protocol. Here you have a client, which sends a key and value to a function node and also a know that basically is

169
00:23:00.000 --> 00:23:10.080
Kaushik Shivakumar: That you ping and it returns the same thing back. So an identity node. So everything is keyed by the same thing. So all of this would be kind of inside the compartmentalize region.

170
00:23:10.620 --> 00:23:25.020
Kaushik Shivakumar: Because everything is separated by the key. So the client sends these messages back sent send these messages and when it gets back responses that just merges them and puts them in a table or outputs them or whatever.

171
00:23:27.180 --> 00:23:38.070
Kaushik Shivakumar: So I implemented this very, very simple protocol in bloom, which is a language for distributed systems that I think this lab built a while back.

172
00:23:39.090 --> 00:23:42.060
Kaushik Shivakumar: I think, like, seven, eight years back, probably, and

173
00:23:43.080 --> 00:23:45.030
Kaushik Shivakumar: One nice thing about bloom is that

174
00:23:46.350 --> 00:24:05.190
Kaushik Shivakumar: If you write a program you can automatically generate a what what is essentially a data flow graph that shows the dependencies between tables. And one thing to notice is that all of the tables we really care about our I'll keep by case cake like this this key over here and

175
00:24:06.870 --> 00:24:14.940
Kaushik Shivakumar: So here's one path where you send a message to the identity node and the other path goes through the function node so

176
00:24:15.780 --> 00:24:30.120
Kaushik Shivakumar: You can probably imagine a situation where one of these pathways is going to be a lot faster than another. So perhaps take copy computing the function takes longer. So in order to do some sort of throughput matching to eliminate bottlenecks.

177
00:24:31.170 --> 00:24:42.180
Kaushik Shivakumar: Since all of this is within the compartmentalize level region, you can scale different components independently. So you might imagine that you would scale the function node three times as much as the scaled identity node.

178
00:24:43.320 --> 00:25:00.030
Kaushik Shivakumar: In order to eliminate in order to prevent any one of these from becoming a bottleneck. So one way to visualize this is by thinking of it as a flow problem and you want to eliminate the bottleneck in the flow. So, this would correspond to for the multiplexes example.

179
00:25:01.410 --> 00:25:10.530
Kaushik Shivakumar: How you would exactly determine how many acceptor groups to provision and how many proxy leaders to create. So this is the equivalent of that.

180
00:25:12.510 --> 00:25:13.080
Kaushik Shivakumar: And

181
00:25:14.250 --> 00:25:25.680
Kaushik Shivakumar: Implementing a very bare bones version of multi axis gives you a much more complicated data flow graph using bloom. But you can see that at the top.

182
00:25:26.280 --> 00:25:36.960
Kaushik Shivakumar: The area circled in orange or the only areas where coordination is required, where, where this leaders actually ordering the log entry indices and after that everything

183
00:25:37.590 --> 00:25:49.200
Kaushik Shivakumar: Every table here, ends up being keyed by the log entry index. And that's what the key insight is that's what allows compartmentalization as Michael described

184
00:25:51.630 --> 00:26:01.200
Kaushik Shivakumar: And one thing so zooming a little bit further out. So imagine imagining that this there's an arbitrary region that you want to compartmentalize. There are some concerns about

185
00:26:01.620 --> 00:26:10.590
Kaushik Shivakumar: As with any parallelism, if there's some reordering of the messages and some point downstream of that code is sensitive to the ordering of the messages so

186
00:26:11.040 --> 00:26:16.980
Kaushik Shivakumar: If you compartmentalize a particular region of the code, you might have to introduce the sealer based on

187
00:26:17.460 --> 00:26:24.300
Kaushik Shivakumar: If there's an edge in this graph or order rule or some procedure that depends on the ordering of the messages you want to

188
00:26:24.780 --> 00:26:33.690
Kaushik Shivakumar: Essentially only send prefixes complete prefixes of your data downstream. So based on some total, total ordering of the data.

189
00:26:34.230 --> 00:26:44.340
Kaushik Shivakumar: You want to seal your outputs of the compartmentalised regions. So you don't lead to any incorrectness or variability across the runs in your code. So this provides consistency guarantees.

190
00:26:45.540 --> 00:27:00.780
Kaushik Shivakumar: So some areas that I'm currently also investigating. So what I presented with sort of how you determine which areas could be compartmentalised and you do that by looking at common keys that are being partitioned done and dependencies in the data.

191
00:27:01.830 --> 00:27:08.640
Kaushik Shivakumar: But another thing that is worth investigating is can we actually discover where

192
00:27:09.360 --> 00:27:16.530
Kaushik Shivakumar: Additional coordination might be required when you're actively rescaling components. So here I've been assuming that we know the IP addresses of

193
00:27:16.950 --> 00:27:23.520
Kaushik Shivakumar: Kind of everybody in in the protocol. But what if these are changing in real time. So David is now going to talk about

194
00:27:24.330 --> 00:27:37.980
Kaushik Shivakumar: An auto scaling version of Paxos where these things might be changing actively and is there some way to discover where additional coordination is required when you're when you're adding and subtracting IP addresses from tables and

195
00:27:39.210 --> 00:27:58.950
Kaushik Shivakumar: Also finding more algorithmic ways and optimal ways of doing this bottleneck elimination, and, also, if there are ways to kind of simplified the current understanding we have this problem in general. So that's all I have. And know David's going to talk about auto scaling paksas

196
00:28:00.840 --> 00:28:10.680
Joe Hellerstein: Thanks coach and just to note where we're kind of on time issue now, which means we're running a little late. So David's gonna kind of breeze through this. Thank you for doing that, David.

197
00:28:12.540 --> 00:28:14.520
David Chu: Kaushik can you oh never mind.

198
00:28:17.490 --> 00:28:24.210
David Chu: All right. Okay. So I want to go through this as quickly as possible, so leave any questions at the end, for me, please.

199
00:28:24.810 --> 00:28:32.820
David Chu: Alright, so let's zoom out a bit from what Kaushik is doing Kaushik is trying to solve. Like the entire class of distributed systems as a whole and find out where we need

200
00:28:33.660 --> 00:28:41.220
David Chu: To figure out contention and bottlenecks, but just looking at compartmentalize Paxos we can. We have a couple of friends that we can tackle immediately.

201
00:28:41.970 --> 00:28:48.450
David Chu: One of them is that we have to actually determine how large we want our compartmentalize Paxos protocol to be

202
00:28:48.870 --> 00:28:54.600
David Chu: The moment we started up right so we could have a problem where we actually started a compartmentalize passwords and since that's too small.

203
00:28:55.020 --> 00:29:02.820
David Chu: And we have like way too many clients so everyone's being overloaded in the middle and people are waiting. And by the way, these are backers, and I'm badgers, they're just

204
00:29:03.300 --> 00:29:13.410
David Chu: I'm Co leading a lot of messages from into the clients together. So, those weren't mentioned in Michaels talk. Okay and here's a second

205
00:29:14.070 --> 00:29:25.920
David Chu: Example where things might go wrong, right, we only have one client, but we decided to start up a bunch of different nodes for our compartmentalize Paxos protocol, because we want the on safe side. But now we're just wasting money right

206
00:29:27.000 --> 00:29:35.220
David Chu: Okay, so it'd be really cool if we had a protocol way which could figure out when I wanted to or when it needed to grow to

207
00:29:35.820 --> 00:29:45.000
David Chu: Accommodate for the increasing or decreasing number of clients right so if we actually break out break down this question, we have to figure out how

208
00:29:45.960 --> 00:29:53.070
David Chu: We should scale. And when we should scale, right. So the first question of how it's pretty simple. For most components inside compartmentalize passes.

209
00:29:53.400 --> 00:29:59.850
David Chu: The nice thing is that most things are embarrassingly parallel. Right. That means they can join the group and it's okay if no one knows that they joined

210
00:30:00.150 --> 00:30:14.070
David Chu: It can leave the group is OK if people find out pretty late that they joined in the so that's that's pretty trivial to figure out, okay, but adding separate groups are tricky, right, because if you know Paxos very well and it's okay if you don't

211
00:30:15.210 --> 00:30:30.480
David Chu: proposers have to win their leadership by talking to executives, essentially. So the if different proposals see a different set of accelerators. That's going to be a big problem right now. Luckily our good old friend. That's the land port actually came up with a couple of ways to do it.

212
00:30:31.590 --> 00:30:42.870
David Chu: One of them's called horizontal Paxos. The idea is kind of like how everything in UNIX is built on file systems here. We're going to have everything that needs to be consistent in our system.

213
00:30:43.650 --> 00:30:48.900
David Chu: Be committed to accept us right so well. One of the things that we need to be consistent about is

214
00:30:49.680 --> 00:30:55.260
David Chu: Actually, the number of acceptor groups inside our system. Right. So we're just going to commit that information to the old acceptor groups.

215
00:30:55.800 --> 00:31:04.680
David Chu: Now the issue with that is that there is kind of a lot of nasty but edge cases, right, because the current leader doesn't necessarily know about all except for groups. So, you know,

216
00:31:05.400 --> 00:31:09.120
David Chu: You might commit them on data or commit the data only to some groups.

217
00:31:09.570 --> 00:31:17.490
David Chu: And it's really expensive. When you decide to scale the number of separate groups, right, because you're going to have to write every single separate group and, you know, those can scale.

218
00:31:18.030 --> 00:31:25.200
David Chu: And definitely okay so the second option that passes other that import offers this is vertical faxes, which

219
00:31:25.500 --> 00:31:31.710
David Chu: Is kind of a cop out. But you're saying that. Okay, I'm gonna assume there's this strongly consistent storage out there somewhere where we can just tell them

220
00:31:32.100 --> 00:31:38.520
David Chu: Oh hey, we have a new accepted group, right. It's kind of a cop out because it's strongly consistent storage is essentially another Paxos protocol.

221
00:31:40.380 --> 00:31:50.760
David Chu: But luckily Michaels I actually worked on this paper called matchmaker Paxos where you can imagine that instead of scrolling consistent storage, you just have another set of accelerators.

222
00:31:51.570 --> 00:31:59.370
David Chu: very specialized accelerators and all they do is they store the configuration right so that's it from salt we actually figured out how to scale.

223
00:32:00.240 --> 00:32:14.970
David Chu: Okay, so now we have to figure out when we scale, right, because if we want a decentralized system to scale up and scale down, then a lot of different nodes can believe that they need scale, the same time and then you can create an excess of notes right or similarly

224
00:32:16.620 --> 00:32:22.650
David Chu: You can have lost nodes that believe we need to scale down at the same time. And then you destroy too many notes right

225
00:32:23.790 --> 00:32:34.710
David Chu: Okay, so we have two options again here. One is that we want to strongly consistent scaling decision right so you're right that okay you know i'm going to scale of a couple of proxy leaders into these matchmakers or whatever group.

226
00:32:35.400 --> 00:32:42.360
David Chu: And then anyone else who wants to scale. They'll read from the matchmakers will see that the scaling decision has happened and then they want to scale, right.

227
00:32:42.840 --> 00:32:56.130
David Chu: That's pretty expensive. And we already know that the leaders bottleneck and we don't want to overload the leader again so that doesn't sound like a good decision on the second option is to have this decision actually be applied. So we know that

228
00:32:57.300 --> 00:33:01.140
David Chu: A node would want to scale because it's overloaded with work right.

229
00:33:02.310 --> 00:33:14.010
David Chu: What we also know is that once we scale the workload is going to decrease. So in this sense, we can actually tell by the changing workload, whether a scaling decision has occurred.

230
00:33:14.580 --> 00:33:24.600
David Chu: Okay, now we've arrived at a different point of this question we've kind of stalled how knows can in a decentralized manner decide when to scale up or scale down but

231
00:33:24.930 --> 00:33:33.300
David Chu: Now we kind of have to establish this threshold, right. Like, how much is too much work when do you decide when to scale. And that's what we're working on right now we have to

232
00:33:33.690 --> 00:33:45.630
David Chu: Look at the entire graph of compartmentalize Paxos and analyze where the bottlenecks are right because I'm a known might think that needs to scale when it's throughput is above a certain threshold or

233
00:33:46.170 --> 00:33:59.280
David Chu: Scale down when it's below a certain threshold, but it could be that the layer above is having some issues, right. So we have to have some kind of smart knowledge about the topography of the graph to be able to reason about this.

234
00:34:01.410 --> 00:34:07.590
David Chu: And this is the last slide. So Rolla central but one day in the far future we can have something like consensus as a service where

235
00:34:08.430 --> 00:34:24.840
David Chu: A user just uses the service online it launches compartmentalize passes and it scales up and down based on how how you want to use it right the throughput to cost ratio or just like Nana. The cost efficiency is optimal. Right, right. That's, that's it.

236
00:34:27.150 --> 00:34:27.990
Joe Hellerstein: Thank you, David.

237
00:34:28.950 --> 00:34:32.940
Joe Hellerstein: All right. All then you are up next, guys, that was great.

238
00:34:35.010 --> 00:34:38.730
Joe Hellerstein: So I'll take a stretch for a sec only get set up.

239
00:34:39.120 --> 00:34:39.840
Alvin Cheung: You guys hear me.

240
00:34:40.080 --> 00:34:40.620
Yep.

241
00:34:42.150 --> 00:34:52.200
Joe Hellerstein: We're moving up the stack. Now, so, you know, just to riff a little bit while we're stretching chain gangs work was all talking about coordination free systems that are fully monotone

242
00:34:52.650 --> 00:34:59.220
Joe Hellerstein: And the work that Michael and company presented, of course, is strongly consistent and has coordination as its goal.

243
00:34:59.550 --> 00:35:08.190
Joe Hellerstein: But even within the Paxos protocols, essentially what we're finding is buckets of coordination freakiness inside the protocol that allow us to scale and auto scale.

244
00:35:08.760 --> 00:35:25.680
Joe Hellerstein: So definitely thematic here that we're trying to take the lessons from monotonic stuff at coordination freakiness and apply them in settings where we also are trying to get strong consistency. So that's kind of the through theory behind a lot of this work alright Alvin, take it away.

245
00:35:26.760 --> 00:35:35.310
Alvin Cheung: Alright, great. Thanks, Joe. So, um, I noticed that we are running a little bit late, so no worries. I'll just speak twice as fast, and then you can make a fine day

246
00:35:36.180 --> 00:35:40.260
Alvin Cheung: Anyway, so, good morning, everyone. So today, I just want to tell you a little bit about

247
00:35:40.680 --> 00:35:47.580
Alvin Cheung: A vision that we have been putting together playing around with Joe for last couple of months, but next generation cloud programming.

248
00:35:48.030 --> 00:36:00.960
Alvin Cheung: And this obviously has a lot of contributions from Members from the hydro team, some of which you have heard this morning. And this is also an ongoing vision that is hopefully going to be refined by the time cyber happens in January.

249
00:36:02.880 --> 00:36:12.810
Alvin Cheung: So what is the background here. So we're just making block very broad claims here. So I claim that we have been progressing through three different generations of cloud computing

250
00:36:13.620 --> 00:36:18.030
Alvin Cheung: The first generation happened around 2000s, where, like, you know, it was a single

251
00:36:18.630 --> 00:36:25.950
Alvin Cheung: Like your appliance or a website, you can just locked in and then like your host your database or your virtual machine there and then like, you know, start

252
00:36:26.400 --> 00:36:35.850
Alvin Cheung: To figure out what kind of machine, you would like. And after that, you know, figure out how many machines you want and then like based send the code that you want to run after logging in yourself.

253
00:36:37.020 --> 00:36:49.200
Alvin Cheung: The second generation, I claim happened around 2010 where now we go into the realm of elastic computing. So we're basically talking about being able to automatically scale out to any number of servers.

254
00:36:50.070 --> 00:36:59.310
Alvin Cheung: In that world, the user only now needs to provide the number of machines that they would like to execute their programs, along with the program that they want to run

255
00:37:00.360 --> 00:37:12.750
Alvin Cheung: So you can probably see some sort of pattern happening here. Right. And then for the third generation we claim is now server lists. Right. So in fact, we don't even need to worry about like seeing any machine at all.

256
00:37:13.500 --> 00:37:23.580
Alvin Cheung: All we need to do is figure out, like, you know, this is the application. This is the code. I want to run, send it over to chain gang and then everything else is done right. So that's great.

257
00:37:24.540 --> 00:37:33.900
Alvin Cheung: Now, this is all going really well, but let's also try to talk about what happens with cloud programming. So has that task.

258
00:37:34.380 --> 00:37:41.160
Alvin Cheung: Become more easier over the times. Right. I mean, as I was talking to you about all these different generations of cloud computing

259
00:37:42.060 --> 00:37:48.840
Alvin Cheung: And for that, we did a little bit of study we looked at Ruby on Rails, which many of you probably know about is this

260
00:37:49.320 --> 00:37:57.210
Alvin Cheung: Framework for building web applications that talk to different databases and Grails for sure. Like, you know, support all kinds of databases, including sequel server.

261
00:37:58.170 --> 00:38:09.000
Alvin Cheung: And on top of that, we look at one specific example of these Ruby on Rails apps called Shopify. So this is a like you know us very simple

262
00:38:10.680 --> 00:38:20.490
Alvin Cheung: Shopping Cart kind of application that is built on top of Rails, so we tracked through the different versions that like, you know, these two software infrastructure has released over time.

263
00:38:21.030 --> 00:38:28.650
Alvin Cheung: And as a very crude measurements of easy or difficult and programming these apps just take a look at lines of code and

264
00:38:29.250 --> 00:38:41.100
Alvin Cheung: So you can see here, like for this very simple, like, you know, shopping website tower thing over like 10 years basically grown like 10 times and then like, you know, Rails itself like you also grown by a bit

265
00:38:42.120 --> 00:38:55.020
Alvin Cheung: You can do that like you know how much, how much code is actually used to like you know program these things now in my ass. I mean, maybe this is just like you know software blocked right so meal that code, like, you know, people didn't get to take care of.

266
00:38:56.580 --> 00:39:01.320
Alvin Cheung: Take care of, like, you know, eliminating useless or that code that shouldn't be around.

267
00:39:02.100 --> 00:39:06.930
Alvin Cheung: Well, we look at some of the release notes from these two frameworks and then try to understand like what's going on.

268
00:39:07.620 --> 00:39:17.430
Alvin Cheung: For instance, in the latest release notes from Shopify they basically saying that they now need to support lambda. So, therefore, like another couple hundred lines of code.

269
00:39:17.880 --> 00:39:23.520
Alvin Cheung: And then they need to support different backgrounds. So like, you know, more, more features added so therefore more lines of code.

270
00:39:24.210 --> 00:39:35.220
Alvin Cheung: And likewise for Rails, so they now support horizontal shouting and the latest release and then they also support being able to switch between different databases even doing application execution.

271
00:39:35.970 --> 00:39:45.990
Alvin Cheung: So you might ask why. I mean, so why are these things like, you know, why are these different software doing all these things. I mean, isn't that shouldn't be. That'd be like a database job right to

272
00:39:46.470 --> 00:39:57.630
Alvin Cheung: Support these frameworks. So some of these reasons why like these are gases at this point we thing is because, for instance, these frameworks and applications have domain specific knowledge.

273
00:39:58.770 --> 00:40:11.760
Alvin Cheung: They have domain specific knowledge that is that only they know about and they want to be able to ensue that all the way into the database systems and not being able to take control of the database basically means that they need to take up the job on their own.

274
00:40:13.320 --> 00:40:19.860
Alvin Cheung: And as a result, that means like they need to worry about many different things, right, especially in the context of cloud computing

275
00:40:20.160 --> 00:40:26.100
Alvin Cheung: Like, as you can see here on the slide, we need to worry about things like shouting, they need to worry about things like supporting different kind of infrastructures.

276
00:40:26.760 --> 00:40:35.340
Alvin Cheung: And not to say the very least, right. So the fact that these cloud computing frameworks keep changing and adding new hardware. It's just making the problem or more difficult.

277
00:40:36.360 --> 00:40:45.300
Alvin Cheung: What of all of these letter. Well, so now we have everything mango within the same code base. Right. Everything from Supporting. Supporting the functionalities that are

278
00:40:45.870 --> 00:40:56.580
Alvin Cheung: Supposed to be provided by these simple frameworks like shopping or building web apps. And as a result, these co basis now become part of reason and even more difficult to optimize

279
00:40:59.100 --> 00:41:01.500
Alvin Cheung: So, and you might think this is just like you may one

280
00:41:02.220 --> 00:41:09.330
Alvin Cheung: One tiny example here. Well, I would actually claim that there's just the tip of the iceberg. Because if we just look at this shopping cart frameworks that are actually

281
00:41:09.660 --> 00:41:22.860
Alvin Cheung: More and more and more and more coming up. So it's just becoming a problem that all these different frameworks have to build their own infrastructure and then bypass the database one without the other. In order to get the performance or the correctness guarantees that they like

282
00:41:24.540 --> 00:41:39.270
Alvin Cheung: So here's where we come in. So, as Joe mentioned in the very beginning of the hour. We're now building a new framework of your system and also a new language called Hydra logic, where we hope that will basically simplify this task off cloud programming.

283
00:41:40.350 --> 00:41:45.420
Alvin Cheung: So let's take back this example here for building a shopping cart.

284
00:41:46.050 --> 00:41:53.610
Alvin Cheung: So the insight here in capture logic is that like you'll be claimed that we can actually be composed this application. And so a bunch of different aspects.

285
00:41:54.180 --> 00:42:02.130
Alvin Cheung: For example, if there was with a data model that is specific to this application that has its own application logic, obviously.

286
00:42:03.060 --> 00:42:13.560
Alvin Cheung: It has its own isolation level, depending on what Apple. What functionality is being run at the time and then likewise, it has different like aspects like latency and hardware requirements so on so forth.

287
00:42:14.460 --> 00:42:30.180
Alvin Cheung: We call this the composition facets, you think of that as basically different aspects of writing this application. The claim here is that, like if we can somehow asked the developer to restructure the application into these different facets and the world will get better.

288
00:42:31.950 --> 00:42:40.950
Alvin Cheung: Why, because we claim that like you each, each of these different factors that will basically come this is with its own API and subsequently its own code generator his own runtime system.

289
00:42:42.060 --> 00:42:50.880
Alvin Cheung: And I'm going to make a really great claim here that they have by basically saying that this whole code generators, not going to be done using another rule based technique.

290
00:42:51.330 --> 00:43:00.540
Alvin Cheung: It's not going to be done using another set of rule based generators because, as Joe mentioned the infrastructure keeps changing. It's just very hard to keep track of all the latest and greatest

291
00:43:01.260 --> 00:43:15.000
Alvin Cheung: So we're going to basically make use of new techniques like search synthesis machine learning what happier to try to generate optimal code that way. And obviously be composing the application and these into these different facets definitely going to help here.

292
00:43:16.410 --> 00:43:28.470
Alvin Cheung: Our goal, however, is to basically build a data engine generator that will ultimately be able to automatically specialized for different types of these data applications, just ask the Shopify example take you see her on the slide.

293
00:43:30.930 --> 00:43:37.560
Alvin Cheung: So you might think, well now you're asking people to be writing code decomposing these applications are the different things. I mean, isn't that kind of crazy.

294
00:43:37.950 --> 00:43:43.500
Alvin Cheung: Well, part of it, we already do already. So, for example, thing of things like SL A's. Right. So these are all obviously

295
00:43:43.800 --> 00:43:52.680
Alvin Cheung: Policies that people can now specify on different data infrastructures and that is already decoupled from the application. So that in itself is not an entirely new

296
00:43:53.310 --> 00:44:01.860
Alvin Cheung: But also I think that this is basically just an instance of what is known as separation of concerns and that has happened many times over and over again in computer science.

297
00:44:02.340 --> 00:44:09.270
Alvin Cheung: So like, you know, close us to our hearts is probably the separation between what is known as the database server and the US application.

298
00:44:09.660 --> 00:44:17.370
Alvin Cheung: Right means this declarative interface knowing sequel has served us really well over the past couple of decades. And it makes it easy to optimize queries that way.

299
00:44:18.300 --> 00:44:24.210
Alvin Cheung: And taking a broader view, you know, there's also model view controller, which allows people to bilk you is relatively easily.

300
00:44:24.720 --> 00:44:27.990
Alvin Cheung: And then, you know, the obvious example of seven layers of networking

301
00:44:28.350 --> 00:44:36.030
Alvin Cheung: And also, like, you know, something as simple as memory management and garbage collection. Whereas something as simple as, like, you know, new or delete.

302
00:44:36.300 --> 00:44:46.410
Alvin Cheung: Right, it makes it very simple to the couple garbage collection from management different routines. So we're just basically following suit here by, like, you know, this new decoupling for cloud programming.

303
00:44:48.000 --> 00:44:54.510
Alvin Cheung: So details. We're here, we're still trying to work on. But for now, here's the kind of preliminary architecture that we have come up with

304
00:44:55.260 --> 00:45:05.400
Alvin Cheung: So again, the idea is to have applications be specified using these different facets. So I've listed a bunch of them here. So we calling that hydro logic that we're currently designing as a new language.

305
00:45:06.300 --> 00:45:12.360
Alvin Cheung: And then this hydrological specification is going to go through a hydrolysis or analysis phase.

306
00:45:12.840 --> 00:45:20.820
Alvin Cheung: Where our goal is to come up with different ways to to deploy the application that we have received across different platforms.

307
00:45:21.390 --> 00:45:30.900
Alvin Cheung: So for instance, one of them can be a key value store like Anna, can be like, you know, relational store and what have you. And there is also a separate piece that we're also working on right now in terms of

308
00:45:32.160 --> 00:45:40.230
Alvin Cheung: Converting existing applications into this new formula, some that were coming up with, but that I was safe for another talk because of the interest of time.

309
00:45:41.340 --> 00:45:47.010
Alvin Cheung: So for now, let me just give you two case to case examples that we have been working on and this hyper architecture.

310
00:45:47.640 --> 00:45:56.070
Alvin Cheung: The first one is about like how are we executing these programs, assuming we already have, you know, the bits and pieces in place from the upper layer for this language.

311
00:45:57.060 --> 00:46:03.480
Alvin Cheung: And for that, we're going to talk just want to bring you the attention of a new library that we are building for monotonic programs.

312
00:46:04.320 --> 00:46:14.430
Alvin Cheung: It's called Spanish. It's a library for monotonic like construction computation. What does that mean, it means computation where state only grows, they cannot shrink.

313
00:46:15.750 --> 00:46:18.780
Alvin Cheung: It means that we're talking about functions that can preserve ordering

314
00:46:20.370 --> 00:46:29.430
Alvin Cheung: And the guarantees of monotone this today for these programs is that's late of driving inflammation cannot invalidate previous decisions that have already been made.

315
00:46:30.570 --> 00:46:40.860
Alvin Cheung: So these are the you're kind of like aggregation or something of examples. For instance, and it's like, you know, this has to also do with the fact that the message is can also arrive in any arbitrary order.

316
00:46:41.940 --> 00:46:52.890
Alvin Cheung: Why bother writing monotonic programs. Well, I mean, Joe and Matthew lot of experts here, but I will also based for now say that like it was trivial to scale out these programs because they are basically for the nation free

317
00:46:55.320 --> 00:47:06.270
Alvin Cheung: So, but turns out that it's actually hard to write these monotonic programs, for example, is like you know chain gang gave a 20 minute talk like right before, like, you know, right before us and that already talked like you know

318
00:47:06.990 --> 00:47:15.600
Alvin Cheung: 10,000 lines of c++ and we're still actually not sure that is monotonic. We hope that at this. And so, but that's like, you know, something to figure out later on.

319
00:47:16.470 --> 00:47:29.220
Alvin Cheung: And even for an expert right like Martin Clubman which, in case you haven't heard of him before he wrote a book called designing day intensive applications. I would claim that he is the expert in writing these programs here.

320
00:47:30.000 --> 00:47:41.580
Alvin Cheung: Still makes mistakes, right, and then, like, you know, you can see this to hear that he made a couple of days ago. So the bottom line here is that correctness is really difficult even for expert programmers and this domain.

321
00:47:43.260 --> 00:47:54.240
Alvin Cheung: So what are we doing to like your make this easier. Well, we're trying to build this library now using rust, which has a very strong static type system which allows us to basically

322
00:47:54.900 --> 00:48:09.690
Alvin Cheung: Work through some of these verification claims that like, you know, the library that were built is indeed going to be monotonic. And we're also using Ross because it compounds really easily to see. So it makes it basically easy to take advantage of efficiency.

323
00:48:10.920 --> 00:48:27.450
Alvin Cheung: The recording goes in the Spanish library building exercise is to now try to rebuild Anna using this new mechanism in order to demonstrate that monotonic by construction programs indeed can be easily built and also compatible from hydro logic.

324
00:48:28.500 --> 00:48:43.230
Alvin Cheung: And hopefully, this is also going to unify reactive and data flow programming in one single framework would be nice. And then our ultimate goal is to be able to compile different types of user applications into Spanish and then measure the resulting performance game.

325
00:48:44.550 --> 00:48:50.880
Alvin Cheung: So that's the example that wants to say for now in terms of this lower layers right off. Like you know how we actually compiling executing

326
00:48:51.330 --> 00:49:02.790
Alvin Cheung: High caloric hydrological programs. But now let me go back in the remaining five minutes. I have in terms of talking about, like, you know, what about the analysis layer. So what is the benefit of having these different facets.

327
00:49:03.540 --> 00:49:13.200
Alvin Cheung: And also run programs. And for that, I just want to mention a few words about data structure synthesis, which is a piece of work that we've done before called chess nuts.

328
00:49:13.680 --> 00:49:21.540
Alvin Cheung: It's a data layout engine for relational queries. This is actually this work was actually done by Kong yen, who is now working at Microsoft.

329
00:49:22.830 --> 00:49:25.290
Alvin Cheung: And we presented this paper back in 2019

330
00:49:26.550 --> 00:49:32.070
Alvin Cheung: The idea here is that, like, yo, very simply, storing data in Google relations. It's just not fast enough.

331
00:49:32.430 --> 00:49:40.920
Alvin Cheung: Right. And, you know, of course, we learn from surgeon that like, you know, if we have a physical tuner. Then we can add index as materialized views and so on and so forth to speed up execution.

332
00:49:41.880 --> 00:49:59.130
Alvin Cheung: However, every after every application turns out to be different. And sometimes, in addition to invest in vaccines. We also need to store things in the NASA data models and also even duplicate data as we need. And even as crazy as pretty computing group results and and whatnot.

333
00:50:00.900 --> 00:50:10.410
Alvin Cheung: So the idea behind chestnuts is to basically allow applications to be written against a single generic data container API. Think of that as basically one of the faxes

334
00:50:11.550 --> 00:50:18.780
Alvin Cheung: And the idea then is for chestnuts are searched for the blacks for the best implementation of this data container API, based on the query workload.

335
00:50:20.250 --> 00:50:33.990
Alvin Cheung: The search here, as I said, it's not going to follow a rule based mechanism. So in fact, we use a bunch of programs and it says search techniques, followed by formal verification that anything that would come up with can indeed be used to answer the original query.

336
00:50:35.040 --> 00:50:44.880
Alvin Cheung: So just want to flash two different experiment results that we've done so in this case we took these web applications built on top rails and then you can see the amount of speed up here, comparing

337
00:50:45.420 --> 00:50:59.070
Alvin Cheung: Different kind of database back ends as a playing around example we even tried to we implement some part of the TPC H queries and compare that with a well known column based storage system which you can guess which one it is.

338
00:51:00.750 --> 00:51:14.520
Alvin Cheung: So currently we actually tried to extend this to other different kinds of cloud workloads in the context of hydro and hydro logic effort and we're actually looking forward to talking with you guys later on if you're interested.

339
00:51:15.930 --> 00:51:21.660
Alvin Cheung: So in summary, that's just like a simple vision that we have been working on for the past couple of months.

340
00:51:22.440 --> 00:51:29.550
Alvin Cheung: The key takeaway is that like we don't believe that a one size fit all mechanism is going to work for cloud programming going forward.

341
00:51:30.180 --> 00:51:35.430
Alvin Cheung: Based on the reasons that I explained earlier, and we're not going to be building another rule based code generator

342
00:51:35.880 --> 00:51:44.100
Alvin Cheung: In fact, we're basically proposing this new way of the composition of cloud programming applications into these different types of aspects of facets.

343
00:51:44.580 --> 00:51:52.170
Alvin Cheung: And we're going to use search space and learning. Learning based techniques in order to compile these programs to execute on the clap for

344
00:51:52.890 --> 00:52:07.890
Alvin Cheung: The currently building this prototype. Oh. Hi, Joe, as I mentioned, and we're basically running different case studies in a current implementation and also different analysis piece that I'm going to stop and I hope I didn't run too much over time. Thank you.

345
00:52:08.880 --> 00:52:12.690
Joe Hellerstein: THANKS, ALAN whirlwind. That was great. So we are

346
00:52:14.310 --> 00:52:25.920
Joe Hellerstein: We actually haven't completely consumed our break. We have a minute and a half left of it. Why don't we take till 945. So we'll take seven minutes now everybody can do a bio break

347
00:52:26.730 --> 00:52:36.450
Joe Hellerstein: You can turn your video off if you want a little privacy and then we'll come back at 945 and handed off to Jonathan and boundaries and we'll take the five minutes that we've lost out of discussion.

348
00:52:37.680 --> 00:52:41.040
Joe Hellerstein: At the end, so you guys from Microsoft should take your full quantum of time.

349
00:52:42.150 --> 00:52:42.690
Joe Hellerstein: See you soon.

350
00:58:21.360 --> 00:58:23.130
Jonathan Goldstein: Hey, Joe. Are you listening right now.

351
00:58:25.800 --> 00:58:27.660
Joe Hellerstein: I am listening. I am attentive.

352
00:58:28.260 --> 00:58:36.030
Jonathan Goldstein: We have, I have two people that would like to join the meeting. Ken Hinckley and Michelle PAHO both from Microsoft. They're saying they need a password.

353
00:58:37.440 --> 00:58:38.700
Joe Hellerstein: For zoom

354
00:58:39.000 --> 00:58:40.530
Jonathan Goldstein: For zoom. That's what they're saying.

355
00:58:42.300 --> 00:58:44.220
Joe Hellerstein: They need to have a zoom login

356
00:58:45.450 --> 00:58:54.150
Joe Hellerstein: UC Berkeley requires as a matter of zoom bomb security that anyone who wants to join a meeting has to have a zoom account.

357
00:58:54.510 --> 00:58:55.470
Jonathan Goldstein: I think they do.

358
00:58:56.370 --> 00:59:00.060
Joe Hellerstein: So if they log in with their zoom account, they should be able to join.

359
00:59:01.890 --> 00:59:09.870
Donald Kossmann: I think this is the same problem that I had, I mean the end it's really info to create a new account.

360
00:59:10.860 --> 00:59:12.120
Joe Hellerstein: But it's like that.

361
00:59:12.960 --> 00:59:15.150
Joe Hellerstein: Yeah. Did you find a workaround.

362
00:59:15.750 --> 00:59:20.340
Donald Kossmann: That yeah yeah I mean I could I could walk, walk them through

363
00:59:22.440 --> 00:59:22.890
Donald Kossmann: The most

364
00:59:26.400 --> 00:59:31.320
Jonathan Goldstein: But, but what they need to do is create a zoom account and once they have that, then they should be able to do it.

365
00:59:31.650 --> 00:59:34.080
Donald Kossmann: That's great. That's what they need to do. Yeah.

366
00:59:35.190 --> 00:59:36.900
Surajit Chaudhuri: Yeah, that would be the simplest yeah

367
00:59:38.430 --> 00:59:43.320
Joe Hellerstein: And it doesn't have to be a Microsoft Identity account, it can be just a personal account.

368
00:59:44.400 --> 00:59:45.270
Joe Hellerstein: throwaway account.

369
00:59:46.320 --> 00:59:50.970
Joe Hellerstein: For slack. I've restricted our slack group to Microsoft com and berkeley.edu

370
00:59:52.050 --> 00:59:57.360
Joe Hellerstein: So for the slack. They should sign up with their Microsoft Identity. Otherwise, they have to get in touch with me personally.

371
00:59:58.020 --> 01:00:02.910
Surajit Chaudhuri: I think very already signed in for slack I Paul sock in signing

372
01:00:02.940 --> 01:00:03.510
So today.

373
01:00:05.010 --> 01:00:09.960
Joe Hellerstein: So, but are you going to go first go first. I can't remember. Oh, you're showing us a video.

374
01:00:10.290 --> 01:00:12.150
Jonathan Goldstein: I think he must already have

375
01:00:13.830 --> 01:00:18.300
Jonathan Goldstein: must already have zoom because I'm getting from Ken that he's asking, it's asking for a password.

376
01:00:20.820 --> 01:00:24.270
Jonathan Goldstein: So is there a reason why would ask for a password suddenly are

377
01:00:25.590 --> 01:00:30.450
Joe Hellerstein: Lots of my knowledge, I don't have any security on joining here.

378
01:00:31.260 --> 01:00:40.650
Donald Kossmann: Ya know, the problem is that the way the book the Zoom is set up. It's very confusing because it's asking you for Cal ID.

379
01:00:41.880 --> 01:01:00.810
Donald Kossmann: And that is, and that is about it. It's really weird and and but it it does work if you if you use your if you create a new account as part of joining the meeting or if you just use your other account. It's the way we set it up.

380
01:01:01.080 --> 01:01:10.530
Jonathan Goldstein: Yeah, because I am I didn't create an account or anything. I just use my usual zoom plugin that I've used for conferences and it did just work. Yeah.

381
01:01:10.650 --> 01:01:11.940
Joe Hellerstein: If you have a zoom account.

382
01:01:11.970 --> 01:01:13.590
Joe Hellerstein: You should just login with that.

383
01:01:14.190 --> 01:01:16.320
Jonathan Goldstein: Yeah, that's what I did. That's exactly what I did.

384
01:01:17.970 --> 01:01:18.450
Badrish Chandramouli: So,

385
01:01:19.020 --> 01:01:19.230
Like

386
01:01:20.850 --> 01:01:22.710
Joe Hellerstein: I saw this in the background. If

387
01:01:22.710 --> 01:01:26.100
Joe Hellerstein: Possible and get you guys rolling. We'd love to hear what you're working on.

388
01:01:26.640 --> 01:01:28.560
Jonathan Goldstein: Yeah okay, sure, um,

389
01:01:29.460 --> 01:01:30.300
Joe Hellerstein: I think thread.

390
01:01:30.450 --> 01:01:31.680
Joe Hellerstein: To help these folks out

391
01:01:32.550 --> 01:01:33.030
Jonathan Goldstein: What's that

392
01:01:33.540 --> 01:01:36.840
Joe Hellerstein: Can we spend one of you off as an asynchronous thread to help these folks out

393
01:01:37.590 --> 01:01:42.630
Jonathan Goldstein: Yeah. So actually I'm should I start the video boundary. Sure. Do you want to start it.

394
01:01:43.650 --> 01:01:47.280
Jonathan Goldstein: I think I'm on the I'm on the page. Now I can go ahead and start it.

395
01:01:47.490 --> 01:01:53.670
Jonathan Goldstein: Okay, so do you want to just spin off in like talks Academy show and get them. Get them sorted out.

396
01:01:54.480 --> 01:01:56.670
Badrish Chandramouli: Yeah, sure. Yeah, I think we saw this already here.

397
01:01:56.940 --> 01:02:00.060
Jonathan Goldstein: Oh, is he okay maybe Michelle can help can then if I'm

398
01:02:00.060 --> 01:02:00.660
Michel Pahud: Sure. He's

399
01:02:00.720 --> 01:02:04.470
Jonathan Goldstein: He's listening. Hey, Michelle. Yeah, just make sure can can get in

400
01:02:05.160 --> 01:02:07.350
Michel Pahud: Yeah, let me send him an email that I agree.

401
01:02:07.860 --> 01:02:08.730
Michel Pahud: Okay, thank

402
01:02:09.240 --> 01:02:11.670
Jonathan Goldstein: You okay so so

403
01:02:13.260 --> 01:02:18.390
Jonathan Goldstein: One of the projects that's informing, a lot of the work that Patricia and I are doing is called surface fleet.

404
01:02:19.230 --> 01:02:34.500
Jonathan Goldstein: So we published a paper and whist of this year, and there were a bunch of videos. The couple of videos that went with it that we thought we would kind of show it the beginning to just sort of get everybody in in a nice frame of mind for the

405
01:02:36.030 --> 01:02:40.770
Jonathan Goldstein: The, the sorts of things that are motivating the work we do, or at least some of the work that we do.

406
01:02:42.780 --> 01:02:47.940
Jonathan Goldstein: So there's two short videos, the total is about 10 minutes and we thought we would begin with that.

407
01:02:49.620 --> 01:02:52.380
Jonathan Goldstein: Let me see. So let me share

408
01:02:56.580 --> 01:02:57.420
Let's see.

409
01:03:03.750 --> 01:03:04.980
Jonathan Goldstein: Let's share that screen.

410
01:03:07.410 --> 01:03:17.370
Jonathan Goldstein: Okay, so this is actually a so hopefully everybody can see that and this window, this, this right here. And this is just the public

411
01:03:18.000 --> 01:03:28.320
Jonathan Goldstein: Project page on the Microsoft website if you're curious and you want to look at it again later. But we'll start with with this video and then we'll show the demonstration video

412
01:03:37.290 --> 01:03:38.220
Jonathan Goldstein: Can everybody hear

413
01:03:41.850 --> 01:03:42.390
Joe Hellerstein: Yes.

414
01:03:43.200 --> 01:03:44.370
Joe Hellerstein: Excellent. Oh, not. No, I don't.

415
01:03:44.700 --> 01:03:45.060
Joe Hellerstein: I don't

416
01:03:45.240 --> 01:03:46.860
Michel Pahud: Have to do. You don't hear the video.

417
01:03:47.460 --> 01:03:48.030
Surajit Chaudhuri: Where we

418
01:03:48.390 --> 01:03:54.030
Joe Hellerstein: Share screen in zoom. There's a little button on the lower left. So you have to either share your screen.

419
01:03:55.980 --> 01:03:56.490
Jonathan Goldstein: Okay.

420
01:03:56.580 --> 01:03:57.840
Joe Hellerstein: So I guess is the right word.

421
01:03:59.520 --> 01:04:01.530
Jonathan Goldstein: So how do I do that. Stop share. Okay.

422
01:04:01.890 --> 01:04:05.280
Joe Hellerstein: When you press share screen look to your lower left, there's a little box that says share

423
01:04:05.280 --> 01:04:06.180
Joe Hellerstein: Computer audio.

424
01:04:06.540 --> 01:04:09.840
Jonathan Goldstein: Oh, I see. Share computer sound. Perfect. Thank you.

425
01:04:13.980 --> 01:04:15.480
Jonathan Goldstein: Will start the video from the beginning.

426
01:04:16.860 --> 01:04:22.350
Jonathan Goldstein: surface fleet exploring distributed interactions compounded from device application.

427
01:04:22.500 --> 01:04:33.540
Michel Pahud: User and time much information work increasingly relies on multi device workflows and distributed workspaces motivated by this trend we built surface fleet.

428
01:04:34.080 --> 01:04:38.790
Michel Pahud: a distributed system that leverages a robust and performance declarative database foundation

429
01:04:39.150 --> 01:04:50.640
Michel Pahud: Thereby explores novel implications for migration of user experiences across devices. In particular, we use outlets lightweight distributed user interface elements that float on top of the screen.

430
01:04:51.060 --> 01:04:59.160
Michel Pahud: To unbiased interactions from device application user and time the surface fleet system supports the variety of athletes.

431
01:05:01.380 --> 01:05:10.350
So people can work across devices with multiple application collaborate with other users and take action at a synchronous or asynchronous time

432
01:05:17.070 --> 01:05:33.930
One example of a floating outlet is simply a media primitive a piece of content that can be left around at hand for use in the moment. As a reminder, or to afford sharing, but also media primitives can act as placeholders, such as a generic empty image be fulfilled is a later time.

433
01:05:40.680 --> 01:05:47.520
Containers offer another example. These are like floating stacks of paper collecting multiple primitives and arranged as desired.

434
01:05:49.890 --> 01:05:52.920
They can also be synchronized when pastor other devices.

435
01:05:57.390 --> 01:06:04.170
Likewise tools encapsulate generic behaviors such as a color picker to get a color from reference material on another device.

436
01:06:06.390 --> 01:06:07.260
The mouse pointer.

437
01:06:09.180 --> 01:06:09.870
The keyboard.

438
01:06:11.550 --> 01:06:18.330
Screen clipping to capture scraps of information or even an extract tool to conveniently tear out the current page.

439
01:06:22.560 --> 01:06:29.880
Portfolios of resilient way to unwind sharing from users and devices just create a portfolio in your own distinctive style.

440
01:06:31.920 --> 01:06:33.750
You can add any media primitive

441
01:06:37.200 --> 01:06:37.890
Container

442
01:06:41.370 --> 01:06:52.170
Raven Tools and inputs. This girl pastor collaborator or another machine directly through simple drag and drop the portfolio is always there when you need it and fades into the background. We're not a nice

443
01:06:53.640 --> 01:06:56.910
Fire the portfolio. The mouse becomes a remote Teller pointer.

444
01:06:59.010 --> 01:07:02.820
You can pass along the keyboard to remotely teletype and a large display.

445
01:07:05.190 --> 01:07:16.440
Or you can ask a colleague at a construction site to share their camera feed through the portfolio. You can watch live evil from multiple colleagues at the same time extract images and use them in your local documents.

446
01:07:21.510 --> 01:07:26.340
Interaction promises of proxies for content to fulfillment at an unbound time

447
01:07:28.110 --> 01:07:30.240
Just create a placeholder to defer action.

448
01:07:32.460 --> 01:07:33.750
And pass it along to a colleague

449
01:07:36.720 --> 01:07:38.790
Then they can provide the missing information.

450
01:07:40.050 --> 01:07:43.650
Such as a photo need different their site visit whenever they snapped a picture on site.

451
01:07:49.140 --> 01:07:54.840
Or you can change your mind later and swap the picture for another the promise links back. It's still there.

452
01:07:55.980 --> 01:08:05.550
Similarly, you can insert a container promise collect images from multiple collaborators and then select your favorite to use and your document will try out different ones until you find the best fit.

453
01:08:07.980 --> 01:08:17.700
Overall service lead hints that the mobility of modern information work. It's not about any particular device, but rather the transition of user activities from one place to another.

454
01:08:18.870 --> 01:08:29.580
Interaction can be unbound in terms of device application user and time all very simple outlets that float above the operating system shell and its application windows.

455
01:08:30.000 --> 01:08:41.070
And that run this local or remote instances across single or multiple users and devices lighting up exciting Society of devices experiences on a resilient distributed systems foundation

456
01:08:47.670 --> 01:08:49.530
Jonathan Goldstein: Okay, so that's the the

457
01:08:50.610 --> 01:08:56.640
Jonathan Goldstein: First video and then the second video which is also, I think, going to be of interest.

458
01:08:58.650 --> 01:09:00.930
Jonathan Goldstein: Is the demonstration video

459
01:09:14.040 --> 01:09:22.920
containers are collections of content which can be arranged in different layouts containers can be moved around freely and can be shared the fo Leo's

460
01:09:28.020 --> 01:09:31.680
One can add existing athletes to containers, such as this image.

461
01:09:37.440 --> 01:09:44.010
The images don't need to necessarily come from either primitive. It is also possible to add files from the value system directly

462
01:09:45.570 --> 01:09:48.120
Added content gets synchronized across devices.

463
01:09:52.410 --> 01:09:54.450
One can also listen to the clipboard.

464
01:09:57.360 --> 01:10:02.220
This allows people to collect and see material from different sources such as web pages.

465
01:10:07.830 --> 01:10:15.390
Objects in containers can be dragged into existing applications. So for example, we can drag an image into our design and illustrator.

466
01:10:20.730 --> 01:10:23.430
We can then take a screenshot using the Snipping Tool.

467
01:10:25.080 --> 01:10:27.810
And then put it in PowerPoint on a different device.

468
01:10:30.060 --> 01:10:36.330
Screenshots can also be free form here, the user takes a screenshot using their pen to draw the selection.

469
01:10:48.780 --> 01:10:52.500
One can create different media primitives, such as images.

470
01:10:53.580 --> 01:10:59.910
Images can have different sources such as the device camera the clipboard or the file system.

471
01:11:05.340 --> 01:11:09.300
One can also drag an image from a folder directly onto the image primitive

472
01:11:10.350 --> 01:11:21.870
Let's now look at the athletes in action. Suppose we are a travel company, creating a newsletter featuring different deals. We now want to show deals to Europe, North America and South America.

473
01:11:22.950 --> 01:11:26.100
We can drag and drop the images directly onto the document.

474
01:11:33.060 --> 01:11:39.990
However images of South America are missing on this device. Let's get them from another device by creating a folio

475
01:11:41.820 --> 01:11:45.960
I follow is a portal for sharing content and tools across devices.

476
01:11:47.790 --> 01:11:50.970
We give the folio a name and choose the design for it.

477
01:11:59.880 --> 01:12:08.520
Now we create an image primitive this image is empty, and we're going to place it into the document to make it a placeholder.

478
01:12:10.650 --> 01:12:16.320
We now place the empty image into the folio which wiggles to show us we can send content over

479
01:12:20.940 --> 01:12:24.240
The content now shows on all of her shared devices.

480
01:12:28.080 --> 01:12:31.470
We can now retrieve it in drag and drop an image of South America.

481
01:12:36.240 --> 01:12:41.160
Which we can then put back into the folio in our document updates automatically

482
01:12:48.690 --> 01:12:50.220
Jonathan Goldstein: Okay. So can everybody hear me.

483
01:12:52.050 --> 01:12:52.770
Michel Pahud: I can hear you.

484
01:12:53.220 --> 01:12:57.480
Jonathan Goldstein: So, so there's sort of two things that I would like to point out, um,

485
01:12:59.160 --> 01:13:05.580
Jonathan Goldstein: You know, sort of themes that come up over and over there. First of all, is a shared state and the other one is migration.

486
01:13:06.000 --> 01:13:08.880
Jonathan Goldstein: So, so you'll see those

487
01:13:09.120 --> 01:13:14.340
Jonathan Goldstein: You know those themes kind of supported in the distributed systems, things that Patricia and I are going to talk about

488
01:13:15.930 --> 01:13:20.820
Jonathan Goldstein: In in the rest of this. So hopefully that was interesting to everybody and

489
01:13:22.200 --> 01:13:26.940
Jonathan Goldstein: Let's talk about the first of these systems, oops, let me do it the other

490
01:13:26.940 --> 01:13:32.730
Michel Pahud: Way this. The one thing I don't know if you have specific questions about that demonstration.

491
01:13:33.270 --> 01:13:37.440
Jonathan Goldstein: I think we're going to hold the questions until, until 11 is the plan.

492
01:13:37.530 --> 01:13:43.530
Michel Pahud: Okay, yeah, I have a hard stop at 1015 I can try to join later if that helps.

493
01:13:43.890 --> 01:13:51.060
Jonathan Goldstein: I see. Are there other questions right now. Are there questions right now while we have Michelle and can hear.

494
01:13:52.110 --> 01:13:53.490
Jonathan Goldstein: About a video that you just saw.

495
01:13:54.330 --> 01:13:57.510
Jonathan Goldstein: Maybe we should just real quick, real quick. Take any the

496
01:13:59.730 --> 01:14:01.170
Jonathan Goldstein: That we might meet the people

497
01:14:01.950 --> 01:14:04.290
Michel Pahud: So something to point out is

498
01:14:04.530 --> 01:14:14.820
Michel Pahud: Like this demonstration was just a small subset of what we can do with this platform. And I think like based on what Jonathan is talking about on Ambrosia 20 shows like it's

499
01:14:15.150 --> 01:14:31.350
Michel Pahud: It's way wider than what you have seen in the demonstration, but that was just five specific paper we we wanted to show how to use those for you to share files and tools, but you can imagine a lot of other scenarios that you could be the on the top of that.

500
01:14:34.020 --> 01:14:38.970
Jonathan Goldstein: Okay, I think we don't have any questions at the moment. So, so why don't why don't we continue

501
01:14:40.170 --> 01:14:44.610
Jonathan Goldstein: So can everybody see the Ambrosia slide the first name brochure slide. Okay, excellent.

502
01:14:45.150 --> 01:14:57.480
Jonathan Goldstein: So the, so the first project that we're going to talk about is ambrosia which which I mean this whole thing started because Joe and I started talking in the Slack channel associated with the

503
01:14:57.870 --> 01:15:04.050
Jonathan Goldstein: The session that we did. We shared and ambrosia. This was the one of the papers that was presented in that session.

504
01:15:05.460 --> 01:15:08.940
Jonathan Goldstein: So, first of all I want to say the

505
01:15:10.530 --> 01:15:14.430
Jonathan Goldstein: Whoops. Why isn't he doing. Let's see, from beginning

506
01:15:18.300 --> 01:15:19.110
Jonathan Goldstein: That's interesting.

507
01:15:21.270 --> 01:15:23.880
Jonathan Goldstein: Let's try this again from beginning. There we go.

508
01:15:24.900 --> 01:15:28.200
Jonathan Goldstein: So first I want to say that everything that

509
01:15:32.850 --> 01:15:43.230
Jonathan Goldstein: That's funny. Every oh I see everything that you're gonna you're going to see about to see is real. It's all available on GitHub and C sharp is our first supported language.

510
01:15:44.520 --> 01:15:52.950
Jonathan Goldstein: We run on a variety of iOS and architectures and we our most recent release was last week where we had a bunch of new clients scenarios.

511
01:15:53.910 --> 01:16:04.440
Jonathan Goldstein: That will talk about enabling migrated bill applications across PCs and iPads and iPhones and Android cell phones and and Linux. So you can you can write an application.

512
01:16:06.000 --> 01:16:11.730
Jonathan Goldstein: In one time and it can migrate from all of the any of those to any of those

513
01:16:13.560 --> 01:16:22.050
Jonathan Goldstein: Okay, so. So let's start with the the outline. We're going to talk about what Ambrosia is and this core idea.

514
01:16:22.920 --> 01:16:39.630
Jonathan Goldstein: Program ability idea that it introduces called virtual resiliency and then we'll talk about you know how it works and how it performs some of the hardware trends that make it a good idea. Now, even though it probably wasn't a good idea 20 years ago from a practical point of view.

515
01:16:40.770 --> 01:16:47.460
Jonathan Goldstein: And then some ideas about things we could talk about that that might be interesting ways to move forward.

516
01:16:50.700 --> 01:16:52.470
Jonathan Goldstein: Well, I am having

517
01:16:54.990 --> 01:16:59.670
Jonathan Goldstein: Difficulty getting PowerPoint to do the right thing. Let's see. Let's try this again. Okay.

518
01:17:01.470 --> 01:17:12.810
Jonathan Goldstein: Okay, so let's start with a simple program, uh, the, the idea of Ambrosia is that it's trying to make distributed programming easy. So let's start with a very simple distributed program and talk about why it's hard.

519
01:17:13.620 --> 01:17:21.270
Jonathan Goldstein: In this particular program. We just have, it's not distributed to begin with, we just have two classes. It happens to be in C sharp

520
01:17:22.260 --> 01:17:27.630
Jonathan Goldstein: We have a main inside a client class that basically creates one of the server class objects.

521
01:17:28.050 --> 01:17:47.460
Jonathan Goldstein: And then calls this method get ink event which increments a local counter and returns the value and then it just calls that three times and writes the output to the console. So in this example, you should see the output 123 when main runs. So does everybody makes sense to everybody.

522
01:17:48.840 --> 01:17:52.500
Jonathan Goldstein: All right, so let's distributed. What could go wrong. How hard can this be

523
01:17:52.830 --> 01:18:04.110
Jonathan Goldstein: Let's do the obvious thing. Let's just put client class on one machine. Let's put server class on another machine, let's just use some kind of RPC framework to connect them so that one calls the others methods and they connect over TCP

524
01:18:04.620 --> 01:18:13.890
Jonathan Goldstein: Well, so first of all, the TCP connection can blink. You can have intermittent network failure. Oops, I have a broken TCP connection I have to somehow heal that

525
01:18:14.700 --> 01:18:24.690
Jonathan Goldstein: So, so, okay, that's sort of not horrible, but not wonderful either from a program ability point of view that I have to worry about this happening at any time worse than that.

526
01:18:26.070 --> 01:18:31.860
Jonathan Goldstein: The server class that let's suppose the server goes down and comes back up. I could just not easily restart it.

527
01:18:32.250 --> 01:18:38.640
Jonathan Goldstein: Right. But that's not really such a good idea because, you know, first of all, that object had stayed

528
01:18:38.970 --> 01:18:52.440
Jonathan Goldstein: So if the client class got called once if the client class called get ink and once on the server side, the server died and came back. Now my counter. Is it the wrong value. So that's one problem worse than that.

529
01:18:52.980 --> 01:19:01.290
Jonathan Goldstein: What if we were in the middle of an RPC call. So suppose you know the client class sent over to the server class, you know, a request, get ink end

530
01:19:01.680 --> 01:19:09.870
Jonathan Goldstein: And now it's waiting for a return value of server goes down and you bring it back up. Now client classes sitting there waiting for a return value that's never going to come

531
01:19:10.230 --> 01:19:20.670
Jonathan Goldstein: So you have that problem too. Now you may think that the client class is stateless and and all of that. But actually it's not. It has the state of the program counter

532
01:19:21.150 --> 01:19:36.300
Jonathan Goldstein: And if client class similarly goes down in the middle and comes back up, then the counter could have the wrong initial value when it restarts and no longer give us the output 123 it could give us the output 234 or something like that.

533
01:19:37.470 --> 01:19:51.540
Jonathan Goldstein: Also, you can still be, of course, in the middle of an RPC call it may be that server class will be desperately trying to provide a return value to client class when it goes down client class comes up server class gives it the return value and client says, what's that

534
01:19:52.590 --> 01:19:55.470
Jonathan Goldstein: So, you know, you have that problem as well.

535
01:19:57.030 --> 01:20:08.340
Jonathan Goldstein: So, and this is a really simple application, right, and I'm already like pointing out all of these problems associated with connection state application state program counter state. All of these things right.

536
01:20:09.360 --> 01:20:19.050
Jonathan Goldstein: So let's, uh, take something a little bit more complicated than that and and then see what would happen in a usual functions framework today.

537
01:20:19.770 --> 01:20:24.690
Jonathan Goldstein: If we tried to implement this. Let's create a little micro service and message forwarding service.

538
01:20:25.170 --> 01:20:38.580
Jonathan Goldstein: And all it does is it takes receives messages and it forwards them and then every thousand messages, it generates a report and sends it somewhere else about what happened over the last thousand message messages.

539
01:20:39.870 --> 01:20:51.780
Jonathan Goldstein: Okay, so let's suppose the let's let's like be really naive and say let's suppose none of this failure stuff ever happened. And we just like wanted to write the easiest code, we could possibly write

540
01:20:52.080 --> 01:21:01.020
Jonathan Goldstein: You know that that has that functionality, what would it look like, well, it would look something like this. So first of all, can everybody see my mouse pointer that I'm moving around.

541
01:21:01.950 --> 01:21:04.200
Jonathan Goldstein: Yes. Okay, good. So

542
01:21:04.890 --> 01:21:15.660
Jonathan Goldstein: So over here, all we do is we have our forward or class here, you know, it needs to have a place to forward to needs to have a place to send a reports to

543
01:21:15.960 --> 01:21:25.170
Jonathan Goldstein: So that's what these variables are and we're just going to go ahead and get some kind of proxy that gives us some way to do RPC calls to these two places.

544
01:21:25.560 --> 01:21:36.930
Jonathan Goldstein: And and then all we're going to do is, whenever we get a message from this process thing we're going to say, well, you know, well, I have to start the service at some point. So I have some some variable for that.

545
01:21:37.500 --> 01:21:50.190
Jonathan Goldstein: So that I can do a lapse time when every thousand messages and send that off in the report, and then every thousand messages I just generate this report here and send the result to report to hoops to report to

546
01:21:51.990 --> 01:21:52.500
Jonathan Goldstein: And

547
01:21:54.000 --> 01:22:05.970
Jonathan Goldstein: And then, of course, whether I send the report or not. I need to send the actual message itself. I need to forward it to whoever's listening. Okay, so this is probably about as minimal as the code could be

548
01:22:06.870 --> 01:22:17.820
Jonathan Goldstein: Not too hard to think about this. Now let's let's say we wanted to do this in a stateless function thing like as your functions. Your lambda functions or something like that.

549
01:22:19.020 --> 01:22:30.840
Jonathan Goldstein: Well, already you can see this is like a fairly complicated picture. Why is this picture complicated, it's complicated because we can't lose messages, right, we want to we want that kind of resilience to failure.

550
01:22:32.370 --> 01:22:35.700
Jonathan Goldstein: Of the sorts the failures of the sort that we talked about at the very beginning.

551
01:22:36.060 --> 01:22:47.220
Jonathan Goldstein: So what that means is that whenever we receive a message before we can really do anything with it. We better put it somewhere adorable so that we don't lose it. So we send it into something like a vent hub or Kafka, or can ISAs

552
01:22:48.270 --> 01:22:54.030
Jonathan Goldstein: So that we don't lose it, then we have some kind of a function service, which is constantly taking things from here.

553
01:22:54.510 --> 01:23:01.680
Jonathan Goldstein: Loading the state of for instance the various counters and things like that. Like every thousand messages, right, we have to send something

554
01:23:01.920 --> 01:23:12.510
Jonathan Goldstein: So we have some counters, or something that we store in maybe a key value store or something. And in the cloud. So whenever we get one of these, these things we need to load the associated state we do our computation.

555
01:23:13.920 --> 01:23:20.430
Jonathan Goldstein: And, you know, and then of course we have to worry about, well, what happens if we die in the middle of executing that function.

556
01:23:21.510 --> 01:23:31.530
Jonathan Goldstein: It may be that, for instance, we forward a message, but not the report. So that's not okay. We're actually given that we did the report. First, it's more likely would, we would

557
01:23:32.100 --> 01:23:41.310
Jonathan Goldstein: Maybe generate the report, but then not forward the message, right. So, so that means you need some kind of protocol in place to sort of keep track of what you've done so far.

558
01:23:41.670 --> 01:23:50.970
Jonathan Goldstein: You know, so that if you re execute a function on failure. You can see where how far you got the last time and make sure, for instance, that you don't send the same report twice.

559
01:23:51.330 --> 01:23:59.970
Jonathan Goldstein: You know, and to make sure that you don't miss any any forwarded messages that you were supposed to forward. So that's what these messages and sequence numbers are all about here.

560
01:24:01.290 --> 01:24:05.460
Jonathan Goldstein: This is no longer simple. I think it's painfully obvious.

561
01:24:06.510 --> 01:24:11.220
Jonathan Goldstein: So this is what happens to the code. This is what the function execution looks like.

562
01:24:11.640 --> 01:24:21.630
Jonathan Goldstein: And without going into too much detail, you'll see that the, the author of these functions now has to have the sequence numbers in it. They're essentially leaving breadcrumbs as they go.

563
01:24:22.050 --> 01:24:31.380
Jonathan Goldstein: So that if the function dies in the middle and re executes, you can you can figure out how far you got the last time and and make everything item potent in your code.

564
01:24:32.430 --> 01:24:42.420
Jonathan Goldstein: Is essentially what that is. So, this is this is very hard to think about this simple function took me the better part of a day and I'm still not 100% sure it's correct.

565
01:24:43.440 --> 01:24:48.510
Jonathan Goldstein: This is very hard to write and this is simple. This is the simplest possible thing you could think of.

566
01:24:50.340 --> 01:25:01.230
Jonathan Goldstein: Okay. So, is there another way. So yes, there is. You'll, you'll notice that when you write MapReduce code or spark code or a code. It doesn't look like that. Right.

567
01:25:01.740 --> 01:25:09.630
Jonathan Goldstein: And, you know, why is that, well, it's because they have a feature that we're calling out that I think is really important called virtual resiliency.

568
01:25:10.140 --> 01:25:15.630
Jonathan Goldstein: And basically this is a distributed platform capability that enables you develop well deploy.

569
01:25:16.020 --> 01:25:29.130
Jonathan Goldstein: To run failure oblivious code in a failure resilient manner so you know when you read any kind of like a MapReduce job. You don't say, oh, if the node. I'm running on fails, then, you know, check the sequence number, blah, blah, blah, blah, you know the the

570
01:25:29.850 --> 01:25:42.000
Jonathan Goldstein: Infrastructure, just as well. You know, I'll run your naive code again, you know, against input and I'll just deal with making sure that everything happens exactly once in practice.

571
01:25:43.590 --> 01:25:48.630
Jonathan Goldstein: Okay, so, so in some at some very high level. How do they do this.

572
01:25:49.650 --> 01:26:05.490
Jonathan Goldstein: Well, you know, basically they do it by saying, well, let's assume. Let's start by assuming that the logic is deterministic right so so then if you if you intercept the interactions between these components and log them like in the case of MapReduce. It literally

573
01:26:06.600 --> 01:26:14.070
Jonathan Goldstein: Literally logs all the intermediate results, you know, as it goes. So if you go ahead and you log them.

574
01:26:15.330 --> 01:26:24.240
Jonathan Goldstein: Then, and all of the code is deterministic, then you can just recover the state by taking what went into the computation and redoing it

575
01:26:25.620 --> 01:26:42.480
Jonathan Goldstein: And you can do your own kind of duplicate elimination on the output and make it exactly once yourself the infrastructure can do this. This is what was so wonderful about MapReduce is is that it could do this with 1000 computers running simultaneously against the computation.

576
01:26:43.980 --> 01:26:54.060
Jonathan Goldstein: You can ensure the exactly once delivery of messages by essentially automatically doing the item potency by keeping track of what's been sent and making sure you don't send it again.

577
01:26:55.470 --> 01:27:00.360
Jonathan Goldstein: And you know you have determinism to lean on, you know, to help you.

578
01:27:00.630 --> 01:27:09.960
Jonathan Goldstein: Know, it turns out that once you pursue a strategy like this. So, so far, you think of MapReduce. So I generate a file, and then I start something new. But now imagine it's all, it's all live

579
01:27:10.350 --> 01:27:17.490
Jonathan Goldstein: Right. Imagine it's all online right you don't wait until one stage is finished before you start the next. You just, you just keep going. Right.

580
01:27:18.270 --> 01:27:28.470
Jonathan Goldstein: Chain it all together. So it turns out, having a log is great. In fact, when you think about it, we don't even today and the systems that do this, take full advantage of that log

581
01:27:28.860 --> 01:27:36.390
Jonathan Goldstein: Right, so, so, okay, the obvious thing is you can recover from checkpoints and all of that. But you can also get active, active

582
01:27:36.840 --> 01:27:46.500
Jonathan Goldstein: Right because hey, everything is deterministic Lee replaceable. I can, you know, I can just have active standbys consuming the long as it's being written ready to take over on a moment's notice.

583
01:27:46.920 --> 01:27:56.280
Jonathan Goldstein: And get high availability that way. And I don't even have anything in my code to help with that as as someone who wrote the service in the first place. Just something the infrastructure provides

584
01:27:56.730 --> 01:28:01.650
Jonathan Goldstein: You get time travel debugging this turns out to be really big and we don't take advantage of this as much as we

585
01:28:01.950 --> 01:28:13.230
Jonathan Goldstein: Probably should but since everything is deterministic and replaceable. I can go back at any point in time in the computation attach a debugger set breakpoints look at variable values and

586
01:28:13.800 --> 01:28:17.310
Jonathan Goldstein: And debug my application retroactive, you know, in retrospect,

587
01:28:18.240 --> 01:28:26.190
Jonathan Goldstein: Not only that, but since I have a log of what actually happened. I can run it against for instance code that I think I fixed if I fix the bug and watch it.

588
01:28:26.460 --> 01:28:41.250
Jonathan Goldstein: Watch what happens. And see if that fixed my problem. So there's all kinds of ways you can leverage having this log. So why isn't everybody doing this. Okay, we are doing it in certain cases, right. But why don't we do it for like general purpose services or general purpose code events.

589
01:28:42.270 --> 01:28:45.810
Jonathan Goldstein: And, you know, okay, the first big thing is non determinism.

590
01:28:47.130 --> 01:28:54.630
Jonathan Goldstein: A lot of code is filled with sources of non determinism their race conditions that happened from multi threaded programs. There's just user input.

591
01:28:55.050 --> 01:29:06.450
Jonathan Goldstein: Right. I mean, someone's sitting there at the computer typing. If you want to replay. You can replay them. You didn't have them sitting at the computer, saying, yeah, enter that input again and you know exactly the same moment.

592
01:29:07.470 --> 01:29:11.580
Jonathan Goldstein: You have all kinds of things that happen, you have calls to the system clock.

593
01:29:12.000 --> 01:29:17.310
Jonathan Goldstein: Right, that you go look at and the next if you replay that. Well, you're going to get a different result. The next time you do it.

594
01:29:17.670 --> 01:29:34.890
Jonathan Goldstein: So you have to have some reasonable way of handling non determinism that's that's problem number one problem number two is performance. So this is viewed as an expensive thing to do. And hopefully I will change your mind today it was expensive 20 years ago. It's not expensive today.

595
01:29:36.720 --> 01:29:46.320
Jonathan Goldstein: The other thing is if you really want to like like that pretty migration story that I've been talking about that we just had this release for and that you kind of saw hinted at in the western video

596
01:29:46.980 --> 01:29:53.250
Jonathan Goldstein: You need to be able to support across language and machine boundaries and and that's important too.

597
01:29:53.820 --> 01:30:02.460
Jonathan Goldstein: So these are all reasons why people aren't doing this already, and ideally you should be able to solve the following challenge, right, I should be able to run an application that contains non determinism.

598
01:30:02.850 --> 01:30:14.190
Jonathan Goldstein: I should be able to call its public entry points, meaning gets our PCs or something like that from a multitude of running languages, I should be able to run it resilient Lee and it should be performance, meaning it should be also cheap to run

599
01:30:15.330 --> 01:30:22.320
Jonathan Goldstein: And you should be able to migrate it from, you know, a machine like for instance a Windows machine in the cloud to a Raspberry Pi running Linux at the edge.

600
01:30:23.130 --> 01:30:35.070
Jonathan Goldstein: Without you know the developer having to do anything extra really to get this right. So, that is that is something that we can actually do and

601
01:30:35.700 --> 01:30:47.100
Jonathan Goldstein: We've connected Ambrosia is amor in which is the recent release. And so for instance you can you can do a write once, run anywhere program in Zambia and because that's what samurai and gives you

602
01:30:47.550 --> 01:30:53.730
Jonathan Goldstein: And by combining it with Ambrosia you now have a right once migrate anywhere application, which is pretty cool.

603
01:30:55.530 --> 01:31:07.020
Jonathan Goldstein: Okay, so let's talk a little bit about how Ambrosia is is architected so basically we have these distributed components called immortals. In this example, we have two immortals.

604
01:31:07.470 --> 01:31:10.650
Jonathan Goldstein: That are communicating with each other, basically calling each other's our PCs.

605
01:31:11.070 --> 01:31:21.450
Jonathan Goldstein: And you know we stick a log in front of the input of each of these immortals. And that's what's going on. So whenever an immortal gets, you know, gets an RPC. It goes ahead and stashes it in a log

606
01:31:22.080 --> 01:31:35.430
Jonathan Goldstein: In some order relative to all of the other input that it's getting, you know, and then it goes ahead and sends those log pages to the to the language binding. I'm in order to actually execute the application code.

607
01:31:37.080 --> 01:31:43.980
Jonathan Goldstein: The because we've implemented. Implemented this employee x sorry implemented it this way.

608
01:31:45.000 --> 01:31:53.010
Jonathan Goldstein: The actual immortal is is running in the case of C sharp in one process, but it also can run

609
01:31:54.210 --> 01:32:03.210
Jonathan Goldstein: Into processes. And this is important because this is where we get our language independence from which is to say there's a to process way of running this where

610
01:32:03.990 --> 01:32:15.720
Jonathan Goldstein: One process is the C sharp language mining to the other process is this immortal coordinator that has all these like logging goodies and communication goodies. You know, all the sort of platform.

611
01:32:16.650 --> 01:32:23.430
Jonathan Goldstein: Infrastructure support stuff. Time travel debugging all of that all of that lives in the immortal coordinator. If you want to support a new language.

612
01:32:23.670 --> 01:32:39.540
Jonathan Goldstein: You just have to write a language binding for that language and all the heavy lifting for all these platform features. I'm talking about are all in the the reusable section of the system in the immortal coordinator. So there's a nice separation of concerns there.

613
01:32:40.980 --> 01:32:43.050
Jonathan Goldstein: C Sharp is the first implemented language.

614
01:32:44.520 --> 01:32:50.250
Jonathan Goldstein: Etc, etc. So, you know, so obviously there's some kind of contract.

615
01:32:50.820 --> 01:33:06.570
Jonathan Goldstein: Between the language binding and the, the immortal coordinator in order to be able to successfully log things in replay and all of these things and where it gets particularly interesting is when you talk about non determinism, right, and how we handle non determinism. So for the

616
01:33:07.830 --> 01:33:09.090
Jonathan Goldstein: First type of

617
01:33:10.320 --> 01:33:15.960
Jonathan Goldstein: The strongest the strongest type of contract. We could support, although we actually support a weaker contract and this

618
01:33:17.070 --> 01:33:27.090
Jonathan Goldstein: It would be this one right so from from some initial state. Any execution of the same incoming requests in the same order results in in both and equivalent final state.

619
01:33:27.360 --> 01:33:40.230
Jonathan Goldstein: As well as the same outgoing request in the same order. This is if you had no non determinism. This would be a sufficient contract. This is basically saying everything is deterministic absolutely nothing changes. When I rerun it.

620
01:33:41.460 --> 01:33:43.470
Jonathan Goldstein: In replay and all of that.

621
01:33:45.300 --> 01:33:52.740
Jonathan Goldstein: Okay, and the in the for the C sharp language binding the way that we achieve this strong language binding contract.

622
01:33:53.040 --> 01:34:00.120
Jonathan Goldstein: Is that we say, well, you know, we're going to execute the incoming requests RPC requests in a single threaded manner.

623
01:34:00.450 --> 01:34:14.130
Jonathan Goldstein: And then the assumption is that the application writer leverages that request in order to generate deterministic behavior. Right. But this doesn't help us with non determinism. Right.

624
01:34:15.300 --> 01:34:24.570
Jonathan Goldstein: So, so for instance, let's take our naive example that we had before they became so ugly, when we tried to do it with stateless functions. And now let's see.

625
01:34:24.870 --> 01:34:36.270
Jonathan Goldstein: Let's suppose we were kind of naive and took that code and just said, well, let's just do it. Let's just take this code and kind of run it in this environment, what would happen. So does anybody see, you know, a problem obvious problem here.

626
01:34:37.860 --> 01:34:38.640
Jonathan Goldstein: There's a bug.

627
01:34:39.930 --> 01:34:42.030
Jonathan Goldstein: With what I've told you so far. It's a book.

628
01:34:44.820 --> 01:34:52.530
Jonathan Goldstein: What's the bug is there non determinism here if you use the strong language binding contract which requires absolute determinism. Is there a problem here.

629
01:34:53.190 --> 01:35:01.620
Jonathan Goldstein: The problem here is the use of daytime now. Right. So inside when I'm generating this report and I need to know how many, how much time has elapsed.

630
01:35:01.950 --> 01:35:11.250
Jonathan Goldstein: I make a call to date date time. Now if I just blindly do replay in some foolish way then. Then I'm going to do this again and I'm going to have a problem.

631
01:35:11.700 --> 01:35:20.550
Jonathan Goldstein: Right, I'm not going to get the same thing so so well the first thing I would do actually is. I would make a data contract right so that's easy, and C sharp. That's just

632
01:35:20.850 --> 01:35:24.000
Jonathan Goldstein: Literally saying this is classes see realizable and

633
01:35:24.420 --> 01:35:33.840
Jonathan Goldstein: And now I have these data members but i'm i'm still left with this problem here, right. This non determinism problem. So for that we have something called the week language binding.

634
01:35:34.110 --> 01:35:44.490
Jonathan Goldstein: Contract, which is, which just says, well, you don't really need to support the strong language binding contract you this one is efficient and it basically is like the strong one except

635
01:35:44.910 --> 01:35:53.910
Jonathan Goldstein: That when you're producing outgoing requests. You're allowed in different runs to have different values.

636
01:35:54.390 --> 01:36:04.830
Jonathan Goldstein: To fill in different values for parameters in some of those requests, you have to have the same number of requests, and I have to be in the same order to the same destination, you know, etc, etc.

637
01:36:05.220 --> 01:36:15.060
Jonathan Goldstein: But the values can change. Well, why does that help it helps because we have a first replay guarantee. So for instance, if we took that timestamp.

638
01:36:15.450 --> 01:36:29.220
Jonathan Goldstein: And we enshrined it in a call to ourselves, right, then the very first time we ran and were able to successfully log that timestamp. We have a guarantee that during recovery we will always use that timestamp.

639
01:36:31.440 --> 01:36:38.610
Jonathan Goldstein: So the very first time we successfully log that outgoing call to ourselves, that is the timestamp, we would use forever.

640
01:36:40.020 --> 01:36:53.910
Jonathan Goldstein: And that's just a something provided by the system and then you end up with this slightly more complicated version, where, when, instead of just calling daytime. Now over here, which is what we used to have, we now make a call to this method called called

641
01:36:55.620 --> 01:37:00.720
Jonathan Goldstein: Sets set start which goes ahead and takes the daytime now.

642
01:37:02.670 --> 01:37:12.180
Jonathan Goldstein: Let me see. Are you guys seen this. Let me do that which now takes takes this call to daytime. Now over here, sends it to set start, which then assigns this variable.

643
01:37:12.870 --> 01:37:29.880
Jonathan Goldstein: So, so that way the very first time that this method call is logged oops is successfully logged. It will use the date time now that that was passed into it, it will log it and now it will forevermore use that time. So does everybody see how that solves this particular problem.

644
01:37:32.280 --> 01:37:33.840
Jonathan Goldstein: Hopefully I know I'm going

645
01:37:36.060 --> 01:37:41.040
Jonathan Goldstein: Okay but but there's other kinds of non determinism. Okay. That helps with pulled sources of non determinism.

646
01:37:41.460 --> 01:37:52.020
Jonathan Goldstein: Right with like a clock where I say, oh, I want to get a value from somebody, it's going to be different every time I get it, but just the first time I managed to get it and then log it. I just want to use that every time when I replay.

647
01:37:52.500 --> 01:37:58.830
Jonathan Goldstein: Right. But there's other kinds of non determinism. There's like externally initiated non determinism where I'm not pulling something

648
01:37:59.100 --> 01:38:12.750
Jonathan Goldstein: But something just arrives from the outside world. So that includes things like user input. It could be network input and it's a similar strategy where you need to basically log the information before you act on it.

649
01:38:14.010 --> 01:38:19.080
Jonathan Goldstein: So I don't want to get too much into the details of exactly how this work because it's a little bit more subtle.

650
01:38:20.280 --> 01:38:31.830
Jonathan Goldstein: But, but suffice it to say it's it's really not very, very complicated to do and what ends up happening with this example is that this is an example of the forward or

651
01:38:33.000 --> 01:38:44.760
Jonathan Goldstein: Where I've added that messages can come from the console. Right. And I just want to point out that it's not a lot of code. There's no sequence ID. There's no, there's nothing crazy going on here.

652
01:38:45.930 --> 01:38:55.380
Jonathan Goldstein: I I had we added a few lines of code, and that solves the problem and I just created this thing's called impulses. So it's really pretty easy to do.

653
01:38:56.580 --> 01:39:06.990
Jonathan Goldstein: So, you know, so we can handle both kinds of non determinism. So, that settles that issue. And how about performance, right. So I'm going to show you a

654
01:39:07.710 --> 01:39:15.720
Jonathan Goldstein: I'm going to show you something that's going to look crazy a little bit at first, and then I'm going to explain to you why it's not crazy, which is that we

655
01:39:16.680 --> 01:39:24.540
Jonathan Goldstein: We took a very simple service, similar to the forwarding service that I just showed you. And we said, well, you know, if I run this.

656
01:39:25.050 --> 01:39:33.930
Jonathan Goldstein: In the data center as efficiently as I know how. And I do the same thing with with Asher functions and lambda functions.

657
01:39:34.560 --> 01:39:42.540
Jonathan Goldstein: In a fully resilient way and then just for, like, just as a ringer. I'm going to compare it to using G RPC without any state protection whatsoever.

658
01:39:43.050 --> 01:39:53.310
Jonathan Goldstein: Right, so that's like they should beat us right there. We're not doing they're not doing anything. We're not right. So it turns out the orange line is the cost per unit of work.

659
01:39:54.210 --> 01:40:05.520
Jonathan Goldstein: For the Ambrosia implementation blue is GR PC and then those two lines on top that are about three orders of magnitude more expensive are the usual ways of doing it in

660
01:40:06.810 --> 01:40:08.130
Jonathan Goldstein: With cloud providers today.

661
01:40:10.050 --> 01:40:16.800
Jonathan Goldstein: Okay, so the first thing that obviously needs explanation is how is it that GR PC can be slower than us and and the answer is

662
01:40:17.250 --> 01:40:26.220
Jonathan Goldstein: Because we're doing some really smart things architecturally in order to to be efficient. So it turns out that that were CPU efficiency is concerned.

663
01:40:26.550 --> 01:40:34.020
Jonathan Goldstein: When you have lots of small messages. We're just better at it than they are. We're just better at sending stuff sending a lot of small messages than GR PC.

664
01:40:34.500 --> 01:40:42.390
Jonathan Goldstein: And and that's why we beat them things get a little bit more sensible, you know, when you sort of look out here and they start beating us by a factor of two.

665
01:40:42.750 --> 01:40:50.820
Jonathan Goldstein: So why are they beating us by a factor of two, they're beating us by a factor of two, because the messages are large that we're sending because we're varying the payload size.

666
01:40:51.060 --> 01:40:58.710
Jonathan Goldstein: In each RPC call. So the payload that the RP sees as you go to the right in this graph are getting bigger and bigger and bigger and bigger.

667
01:40:58.980 --> 01:41:08.310
Jonathan Goldstein: In terms of the number of bytes that are getting sent eventually they become sufficiently big that you're not CPU bottleneck anymore and you just become a bottleneck on the network card.

668
01:41:08.940 --> 01:41:15.870
Jonathan Goldstein: On the Nick. So if you think about G RPC. It's basically saying, well, I have one client. I have one server. I'm sending

669
01:41:16.200 --> 01:41:23.310
Jonathan Goldstein: A maxing out my Nick sending a bunch of bites along it, I can send however many bytes. The Nick will let me send Ambrosia is doing.

670
01:41:23.850 --> 01:41:35.130
Jonathan Goldstein: Is getting half of the utilization of that network card because not only does it have to send those bites to from the client to the server, but it also has to send those bites to replicated storage.

671
01:41:35.490 --> 01:41:42.600
Jonathan Goldstein: So that it can be logged. So you're having the effect of network throughput and that's why you see this at the end here.

672
01:41:43.950 --> 01:41:53.910
Jonathan Goldstein: Okay. The other thing that's surprising is the latency, which is in milliseconds, which you see in this is this graph below. This is how long it essentially takes to do a ping

673
01:41:54.750 --> 01:42:03.630
Jonathan Goldstein: in these in these various frameworks. You can see the, the, the data center server lists stuff is sort of off the charts. It's very, very high.

674
01:42:05.460 --> 01:42:12.510
Jonathan Goldstein: G RPC, as you would expect, because they don't have to store anything in a log. They beat us at first. Interestingly enough,

675
01:42:13.110 --> 01:42:20.250
Jonathan Goldstein: We're more robust than they are because they have some kind of garbage collection going on that kicks in, you know, roughly every thousand messages.

676
01:42:20.610 --> 01:42:28.830
Jonathan Goldstein: And they're apparently not very efficient. And this is where see sharps background garbage collection just does a better job than whatever they're doing and

677
01:42:29.280 --> 01:42:37.230
Jonathan Goldstein: So we're actually significantly more robust and this is by the way the c++ implement implementation of G RPC, which is their very best one.

678
01:42:38.760 --> 01:42:39.060
Jonathan Goldstein: Okay.

679
01:42:40.650 --> 01:42:51.180
Jonathan Goldstein: So, uh, just a quick thing about the resiliency features fail over time is about two seconds when we're running active, active. So, it fails over very quickly.

680
01:42:53.520 --> 01:42:58.950
Jonathan Goldstein: Okay, so how did we pull this off, I'll just say quickly, since we're running out of time.

681
01:42:59.370 --> 01:43:18.360
Jonathan Goldstein: That over the last seven years CPUs have not gotten much faster. While in terms of price performance CPUs haven't improved much while networks and storage bandwidth have. And as a result, if you want to move bites around and store them not long term but store them.

682
01:43:19.410 --> 01:43:25.560
Jonathan Goldstein: You can do this at very, very low cost compared to the cost of actually processing those, but it's

683
01:43:25.890 --> 01:43:33.780
Jonathan Goldstein: And if you think about what a very smart implementation of Ambrosia would look like we're actually not doing any more work when it comes to actually processing the bytes.

684
01:43:34.110 --> 01:43:40.230
Jonathan Goldstein: We're just packing them on two pages and moving them around. We don't interpret them anymore, then something like G RPC.

685
01:43:42.570 --> 01:43:42.990
Jonathan Goldstein: Okay.

686
01:43:44.190 --> 01:43:47.040
Jonathan Goldstein: So another thing which is a rah rah for our community.

687
01:43:48.720 --> 01:43:53.460
Jonathan Goldstein: Why am I a database person working on this problem. Why does it make sense for me to do this.

688
01:43:54.300 --> 01:43:59.790
Jonathan Goldstein: Well, it turns out that all of the things that we know how to do to make databases run fast if they all work here.

689
01:44:00.330 --> 01:44:13.650
Jonathan Goldstein: And they're all really important optimizations to make all of this work fast with with modern architecture, and that includes things like right ahead logging and group commit and adaptive bashing, you know, and all of this kind of stuff.

690
01:44:15.270 --> 01:44:26.040
Jonathan Goldstein: Okay, it's just a I want to end by just talking about a few of these open open problems that I think could be interesting to us and I think a few more came up during some of the last

691
01:44:27.120 --> 01:44:35.610
Jonathan Goldstein: Talks that we heard in the first part, the first thing is we haven't a fully flushed out and and implemented.

692
01:44:36.270 --> 01:44:50.160
Jonathan Goldstein: How to do elastic scale out for services that do virtual resiliency with minimum minimal downtime for restarting right so this is obviously a tactically hard problem, but we think we have ideas on how to attack it.

693
01:44:51.600 --> 01:44:55.200
Jonathan Goldstein: This is something that I think would be very interesting to work on.

694
01:44:57.180 --> 01:45:01.500
Jonathan Goldstein: Lots of lots of interesting technical issues. And I think it would also just be really valuable.

695
01:45:02.820 --> 01:45:14.340
Jonathan Goldstein: There are some interesting questions associated with, you know, we have these DB MS is that rely on recovery right for replay ability at least

696
01:45:14.910 --> 01:45:22.500
Jonathan Goldstein: Recoverable state, right. So suppose you wanted to implement a very high performance multi core DB MS in the framework of Ambrosia

697
01:45:23.250 --> 01:45:38.280
Jonathan Goldstein: How good can you do are there are there things that would stop us from getting the the levels of performance that databases get today. I think this is an interesting problem because it is such an optimized artifact.

698
01:45:39.510 --> 01:45:43.680
Jonathan Goldstein: That has, you know, things like concurrency and a

699
01:45:45.210 --> 01:45:55.980
Jonathan Goldstein: race conditions in it, but it's also one that that supports that notion of recovery that we do so. So there's an interesting about have we drawn the abstraction lions boundaries.

700
01:45:56.370 --> 01:46:08.100
Jonathan Goldstein: In a place that allows us to achieve the kind of performance that we expect for an artifact like this. I think that's very interesting. And finally, the last thing, especially for Alvin, given what the things that I just saw that he did

701
01:46:09.810 --> 01:46:15.390
Jonathan Goldstein: You know, we have all these ways of dealing with non determinism that we've kind of shoehorned into C sharp

702
01:46:15.930 --> 01:46:22.500
Jonathan Goldstein: But it really opens up interesting questions around. Suppose you know that you're writing, creating a language to run

703
01:46:22.920 --> 01:46:32.670
Jonathan Goldstein: You know, in this kind of virtually resilient framework and you know you want to handle non determinism. You know, I'm not sure that from a language point of view that that what we have is the best way to do it.

704
01:46:33.390 --> 01:46:38.670
Jonathan Goldstein: So, you know, I think there's some very interesting question is around around that.

705
01:46:40.170 --> 01:46:54.120
Jonathan Goldstein: And Alvin strikes me as the kind of person who maybe might be interested in those kinds of things. So these are all threads that I would love to start with you guys if if there's interest on your side. And with that, I'm going to hand it over to Patrice

706
01:47:01.680 --> 01:47:03.780
Badrish Chandramouli: Oh, do I need to stop sharing. You need to stop sharing

707
01:47:04.380 --> 01:47:04.740
Okay.

708
01:47:12.840 --> 01:47:15.210
Badrish Chandramouli: I stopped sharing. So, okay, I'm trying to

709
01:47:25.620 --> 01:47:27.420
Badrish Chandramouli: Do you see the presentation full screen.

710
01:47:27.870 --> 01:47:29.340
Badrish Chandramouli: Yes. All right, perfect.

711
01:47:34.140 --> 01:47:34.920
Badrish Chandramouli: And the laser pointer.

712
01:47:35.610 --> 01:47:39.300
Matthew Milano: Now we just, we just see the PowerPoint, as though you hadn't started the presentation.

713
01:47:39.870 --> 01:47:40.140
Oh,

714
01:47:41.190 --> 01:47:41.460
Badrish Chandramouli: Okay.

715
01:48:10.110 --> 01:48:10.680
Badrish Chandramouli: Try something else.

716
01:48:29.550 --> 01:48:29.880
Badrish Chandramouli: Alright.

717
01:48:33.540 --> 01:48:34.080
Badrish Chandramouli: Okay, perfect.

718
01:48:35.280 --> 01:48:40.710
Badrish Chandramouli: Alright, so I'm going to talk about this umbrella project called simple store, which basically

719
01:48:42.270 --> 01:48:51.390
Badrish Chandramouli: it encapsulates a lot of work that we have been doing in the space of simplifying storage and the access to storage by both modern service applications and analytics applications.

720
01:48:52.770 --> 01:49:01.830
Badrish Chandramouli: This is going to work with a bunch of amazing folks at MSR and interns who are regularly pages visits, including from Berkeley is the one I will call out

721
01:49:04.050 --> 01:49:04.350
Badrish Chandramouli: So,

722
01:49:05.880 --> 01:49:15.930
Badrish Chandramouli: Let me start with the introduction. So we're all familiar that the cloud has not mature as the platform for both compute and data processing is that kind of become the de facto location for performing

723
01:49:16.950 --> 01:49:30.870
Badrish Chandramouli: Computations and data processing. However, the edges also now becoming very important as a source destination and conduit for cloud competitions. So we are ingesting sorting and processing big data with dynamic schemas complex analytics queries over them.

724
01:49:32.070 --> 01:49:42.180
Badrish Chandramouli: And more recently has been shown on multiple projects for that Berkeley and MSR there's an increasing focus on simplicity, ease of adoption deployment auto scaling.

725
01:49:42.690 --> 01:49:49.230
Badrish Chandramouli: But clearly with these new services compute obstructions that are just making it so easy to use computations and deploy competitions and run them.

726
01:49:49.770 --> 01:49:59.370
Badrish Chandramouli: So the question that the simple store project basically wants to answer is his story is getting left behind. Right. Are we kind of providing the same form of simplicity and

727
01:50:00.870 --> 01:50:03.720
Badrish Chandramouli: Ease of use for storage without sacrificing performance.

728
01:50:05.670 --> 01:50:13.590
Badrish Chandramouli: So to kind of elaborate on this compute, storage gap right when you talk about stories. What we really mean it. This could be main memory like where you can get

729
01:50:14.160 --> 01:50:22.380
Badrish Chandramouli: 10 200 nanoseconds of latency to access it. It's the local disk. For example, you get microseconds of latency on the local disk or could be cloud storage where with

730
01:50:22.770 --> 01:50:28.860
Badrish Chandramouli: Redundancy you get several milliseconds of round trip times. So, for example, on as your page blobs.

731
01:50:29.610 --> 01:50:34.860
Badrish Chandramouli: However, the question is, are we keeping up with the advantages or advances and compute simplification.

732
01:50:35.460 --> 01:50:44.760
Badrish Chandramouli: But if it just step back and take a look at how people are using these storage systems today right they use these these newfangled really easy to use auto scaling computer systems like

733
01:50:45.870 --> 01:50:46.860
Badrish Chandramouli: As your lander so

734
01:50:48.510 --> 01:50:53.820
Badrish Chandramouli: As your functions or Amazon lambda communities and so on. And then they end up either keeping everything in memory.

735
01:50:54.300 --> 01:51:06.360
Badrish Chandramouli: And then kind of throwing in some form of resiliency after the fact, or they just simply and abusing it like remote elastic storage systems like big table or on every invocation and event. This is huge.

736
01:51:07.770 --> 01:51:14.280
Badrish Chandramouli: Inefficiencies and then they realize, hey, this is not scaling. Well, so let us throw in a cash on the cash after the fact.

737
01:51:15.060 --> 01:51:25.380
Badrish Chandramouli: And then it ends up being something like readers and then consistency and accountability are all out the window. Right. I mean, what is the consistency guarantees that you get in the system that you had in the old system.

738
01:51:26.490 --> 01:51:30.990
Badrish Chandramouli: And so all of these questions become important. And then you try to solve these problems by

739
01:51:32.250 --> 01:51:43.200
Badrish Chandramouli: By having like these patchwork solutions that try to fix it, but you're never quite there yet. I mean, you're, you end up with an end to end system, who semantics and who's consistency guarantees you don't understand

740
01:51:45.180 --> 01:51:55.770
Badrish Chandramouli: So the goal of the symbols whole project is basically simplify the application view of this memory hierarchy. This is your storage and cash while retaining high performance.

741
01:51:56.640 --> 01:52:06.120
Badrish Chandramouli: And we have been creating building components that can be used by other user applications that can be used by cloud services databases functions, all of the above.

742
01:52:06.900 --> 01:52:21.900
Badrish Chandramouli: And these components can be used address a storage accelerator where the data part of to data could be existing stores, which are highly resilient like things like Cosmos dB or dynamo DB that on the back end or they could themselves be the point of truth for your data.

743
01:52:23.430 --> 01:52:33.600
Badrish Chandramouli: So some of the key insights that we exploit in this line of in this project include things like concurrency and asynchronous recovery, so that we can create fast as Lillian components or tiered storage.

744
01:52:34.530 --> 01:52:44.760
Badrish Chandramouli: We can leverage things like the predicates, that the user provides all the temporal patterns in the in the data that the access to optimize access to that the hardworking set for example is automatically

745
01:52:46.050 --> 01:52:48.450
Badrish Chandramouli: President in memory, and you get really good performance for that.

746
01:52:50.700 --> 01:52:52.470
Badrish Chandramouli: The overall project is

747
01:52:54.270 --> 01:53:07.500
Badrish Chandramouli: Is tackling too broad umbrella. One is compute workloads. This is what I'll be focusing on in this talk. This includes things like faster, which we will talk about. And the way we get consistency likability and performance in the scale out system.

748
01:53:08.970 --> 01:53:17.520
Badrish Chandramouli: Will touch briefly on things like CRA, which is kind of a virtual machine for the edge cloud, which basically things like ambrosiana surface fleet are able to leverage

749
01:53:17.880 --> 01:53:31.050
Badrish Chandramouli: And then there's this whole analytic side of things which are not talked about much. And those will they're basically looking at simplifying storage for the purpose of performing analytics on the on the data that you are either collecting or you're capturing as part of your

750
01:53:32.340 --> 01:53:33.270
Badrish Chandramouli: Application.

751
01:53:35.580 --> 01:53:48.420
Badrish Chandramouli: They don't have the resource pipeline, right. So this is basically kind of how the evolution of the projects has been like we started with something called tranquil, which are the main memory and high performance analytics systems that were built, maybe

752
01:53:50.820 --> 01:53:52.680
Badrish Chandramouli: Eight years back now and then.

753
01:53:53.730 --> 01:54:02.160
Badrish Chandramouli: The findings from those basically led to some of these projects which include this notion of this virtual machine for the edge cloud that can abstract away.

754
01:54:02.670 --> 01:54:11.700
Badrish Chandramouli: The physical components. And then there's faster, which can think of it as the virtual memory for the edge cloud, which basically abstract severe the complexities of tiered storage and resiliency for you.

755
01:54:12.450 --> 01:54:19.560
Badrish Chandramouli: And then Jonathan talked about Ambrosio which again builds on top of theory as kind of the virtual machine and it provides these very nice.

756
01:54:20.010 --> 01:54:26.880
Badrish Chandramouli: End to end guarantees for applications on top this talk, I'll focus on these three components, but really only on mainly on faster and it's

757
01:54:27.570 --> 01:54:34.050
Badrish Chandramouli: Very brief descriptions of the other projects, but they all I think kind of work together in order to work towards what

758
01:54:34.680 --> 01:54:43.890
Badrish Chandramouli: What I call us this future of storage and computing, because I think the way they can interact with each other is really exciting and the surface, the demo. For example, showed you how

759
01:54:44.970 --> 01:54:48.600
Badrish Chandramouli: This form of marketability and client side state.

760
01:54:49.890 --> 01:54:53.730
Badrish Chandramouli: Hand liquid Ambrosia can work very well with states chatting and

761
01:54:55.020 --> 01:55:09.000
Badrish Chandramouli: And virtualized edge cloud is provided by care and faster to be able to really dictate how the future of servers, storage and computing should look like. So, and this will be an interesting topic to discuss after the talk as well.

762
01:55:11.010 --> 01:55:21.030
Badrish Chandramouli: So talk outline itself is such showed. I mean, I only talked about introduction and just going to delve into simple so for Compute, but I'll probably spend one slide on some of the other things, but not nothing more than that.

763
01:55:23.580 --> 01:55:29.550
Badrish Chandramouli: So the theory is basically this particular common runtime for applications. It's. Think of it as the

764
01:55:30.300 --> 01:55:37.530
Badrish Chandramouli: This virtual machine obstruction, on which kind of virtualized this this edge cloud concept because now we have devices that are all over the place.

765
01:55:37.860 --> 01:55:44.940
Badrish Chandramouli: Devices may move from one place to another. And there's the cloud, which were some of the competition may run essentially competition can now be shared.

766
01:55:45.210 --> 01:55:51.720
Badrish Chandramouli: Across these and applications may not talk to each other in this virtual network so theory, very briefly, essentially.

767
01:55:52.260 --> 01:55:59.940
Badrish Chandramouli: Think of it as a framework on which you can create a virtual graph of kind of components or competitions.

768
01:56:00.540 --> 01:56:04.290
Badrish Chandramouli: And it basically creates a self healing network graph on top of them in the sense that

769
01:56:04.980 --> 01:56:09.600
Badrish Chandramouli: It was realizes these endpoints, so that the doesn't matter whether they're on on the edge of the cloud.

770
01:56:09.900 --> 01:56:16.560
Badrish Chandramouli: And if these network connections failed and it heals them. This allows something like Ambrosia to run on top, without having to worry about.

771
01:56:17.040 --> 01:56:29.280
Badrish Chandramouli: The physical placement of where the device look is present or worrying about the complexities of like re establishing connections and the heartbeat mechanisms and all of that, which basically saddened by CRA which which runs on top of

772
01:56:30.360 --> 01:56:33.810
Badrish Chandramouli: cluster computing platforms like abilities or local clusters and so on.

773
01:56:36.300 --> 01:56:38.010
Badrish Chandramouli: So that's what I will talk about in terms of

774
01:56:39.210 --> 01:56:44.580
Badrish Chandramouli: The, the compute layer itself. And now let's talk about storage and the key project that

775
01:56:45.630 --> 01:56:49.770
Badrish Chandramouli: We have been looking at this space is a system called faster, which is also open source.

776
01:56:51.210 --> 01:57:01.410
Badrish Chandramouli: And the challenge that the faster project tries to solve is as follows. Right. You have all these modern applications and services, including the ones that he saw in

777
01:57:01.890 --> 01:57:08.940
Badrish Chandramouli: In the demo and at the previous talks is that there are a lot of common requirements are all of these applications have to address.

778
01:57:09.480 --> 01:57:22.230
Badrish Chandramouli: The access update and cash huge volumes of state or objects and these are usually fine grained objects, maybe per device counters demand base statistics, they might be shared state that may be multiple applications, want to update at the same time.

779
01:57:23.730 --> 01:57:26.250
Badrish Chandramouli: And often the state does not fit in memory, and they require.

780
01:57:27.330 --> 01:57:34.500
Badrish Chandramouli: You to basically like expand out onto secondaries to stories like the cloud storage or local SS IDs and so on.

781
01:57:35.280 --> 01:57:48.750
Badrish Chandramouli: And the same time applications need fast, reliable logging and messaging components are very important building blocks to build your end to end applications. And like I said, they need to work in all of these diverse environments and touring like cloud edge and service.

782
01:57:50.820 --> 01:58:03.810
Badrish Chandramouli: So what is faster, right at it's very core. It's an open source library that we've built that accelerates objects during indexing and logging and, in some sense, we have basically started off from the inside out. We think of this as kind of the

783
01:58:05.070 --> 01:58:08.580
Badrish Chandramouli: The Core Kernel using which I cannot build all of these

784
01:58:09.630 --> 01:58:24.180
Badrish Chandramouli: standout and distributed systems on top. It's a high performance concurrent last free shared memory single node. A key value store and it has really two components. One is a log obstruction, which is the record log, which contains all the records that are being updated and

785
01:58:25.710 --> 01:58:26.790
Badrish Chandramouli: And being operated on

786
01:58:28.140 --> 01:58:34.620
Badrish Chandramouli: And on top of the log is the index, which is because the faster kV which is the key value store. It's a harsh index on top of the log

787
01:58:35.340 --> 01:58:45.660
Badrish Chandramouli: And the interesting thing about the key value store itself is that it shapes the hard working set in memory. So it leverages the temporal access patterns of the data. So the hardworking set is in the chain of the log

788
01:58:46.200 --> 01:58:51.810
Badrish Chandramouli: And in some sense, the log is essentially a single log obstruction that encapsulates main memory and tiered storage.

789
01:58:53.040 --> 01:59:00.120
Badrish Chandramouli: And then the log the index sits on top of this log and turn so that you can get really good performance by being careful about this design in a

790
01:59:00.660 --> 01:59:14.430
Badrish Chandramouli: In a shared memory multicore setting and in the performance we get around 200 million operations per second on one modern machine for standard by CSP benchmarks for example by using last free technology on the single machine.

791
01:59:15.990 --> 01:59:20.370
Badrish Chandramouli: So I won't get into too much details. I'll give you a high level design of like overview of the architecture.

792
01:59:20.880 --> 01:59:27.870
Badrish Chandramouli: So basically the tiered storage array talk about consists of in memory part shown in the bottom and then that is the tiered storage itself. It sits on top.

793
01:59:28.200 --> 01:59:35.580
Badrish Chandramouli: Now this could start off with local necessities that could be persistent memory feeding off to other local facilities or as your premium storage or any

794
01:59:36.660 --> 01:59:38.640
Badrish Chandramouli: Cloud redundant storage.

795
01:59:40.800 --> 01:59:49.800
Badrish Chandramouli: So you have the hash and expert sits in memory and which basically indexes this hybrid log and the habit log, basically. So the content is a virtual address space, like I said, it starts with

796
01:59:51.090 --> 01:59:57.570
Badrish Chandramouli: The SSD or or the remote storage cloud storage and then seamlessly transitions to main memory.

797
01:59:58.590 --> 02:00:04.650
Badrish Chandramouli: And there's of course a staging area between the two of them were the data basically is allowed or given a second chance.

798
02:00:05.010 --> 02:00:09.300
Badrish Chandramouli: To show that it's truly hot and if it is being physically hot it gets copied back to the table.

799
02:00:09.780 --> 02:00:15.300
Badrish Chandramouli: So the important thing here is that for the records that are in the mutable region that is in the state of the log in memory.

800
02:00:15.690 --> 02:00:19.050
Badrish Chandramouli: They get a better in place. So you basically have a subject. So, it behaves as a cache.

801
02:00:19.740 --> 02:00:27.870
Badrish Chandramouli: And as soon as the data is in the either the immutable, the region, either in memory or on storage, it undergoes what is called as read copy update

802
02:00:28.380 --> 02:00:42.060
Badrish Chandramouli: So essentially, there's a combination of in place of dates for the hot data and read copy of date for the call later in this log structured it organization which basically ends up giving us very high performance for the hardworking set and at the same time, it provides you

803
02:00:43.200 --> 02:00:49.710
Badrish Chandramouli: The right copy a bit based capturing of the temporal locality of the data.

804
02:00:50.760 --> 02:00:53.490
Badrish Chandramouli: We call this architecture as a capstone architecture, because it kind of

805
02:00:54.630 --> 02:01:02.610
Badrish Chandramouli: commingle the notion of caching and storage and we'll talk about the resiliency of this later on. But it right now. Think of it as just a costume.

806
02:01:05.550 --> 02:01:12.990
Badrish Chandramouli: So there's a lot of the data, how we actually get this 200 million of operations per second performance in the shared memory setting. And I won't go into the technical details of that but

807
02:01:13.800 --> 02:01:21.300
Badrish Chandramouli: Just suffice that we have several papers on talking about the details of this and I'm happy to chat about it offline as well but

808
02:01:22.410 --> 02:01:31.620
Badrish Chandramouli: In short, we basically get linear scalability on a single machine. So as you scale the number of threads on the x axis here, you see that compared to all the existing key value stores its

809
02:01:32.850 --> 02:01:45.300
Badrish Chandramouli: Orders of magnitude better by providing this this notion of consistent consistency based on cash coherence for you because it's a shared memory system and you're busy depending on CPU or cash coherence for your consistency in the system.

810
02:01:49.410 --> 02:01:55.050
Badrish Chandramouli: So there are these faster. The building glossary talked about this log and the other key value store.

811
02:01:55.590 --> 02:02:05.280
Badrish Chandramouli: Have lots of use cases that we've been working with across Microsoft and outside. So in particular, that has this this the state stores in streaming systems where you want to be able to

812
02:02:06.750 --> 02:02:12.240
Badrish Chandramouli: offload the data that is that the streaming pipelines hard to access and use

813
02:02:13.590 --> 02:02:20.760
Badrish Chandramouli: Are basically now being used center faster, which basically handles all the data for them. And one of the interesting use cases. I want to highlight, which is a

814
02:02:21.240 --> 02:02:32.310
Badrish Chandramouli: More recent one is this system called durable functions. So you guys may be familiar with some this system called as your functions, which is basically Amazon lambda equivalent, which runs on as your

815
02:02:33.600 --> 02:02:49.980
Badrish Chandramouli: There is a circle durable functions that we've built up that that sits on top of it or functions, which is a stateless destruction, you will function basically provides you a very fully reliable state fully programmable programming experience with

816
02:02:51.660 --> 02:03:01.440
Badrish Chandramouli: With concepts such as futures and a sink programming to provide you this, this notion of creating workflows over over the service infrastructure.

817
02:03:02.070 --> 02:03:09.810
Badrish Chandramouli: And we have built a new backend for your bill functions that's based on faster and this because use for both states storage and event logging and this will be

818
02:03:10.860 --> 02:03:19.320
Badrish Chandramouli: Released in the next couple of weeks, and we look forward to that. And yeah, I'd be happy to send you more pointers offline on this. So this is one of the more interesting use cases that

819
02:03:19.950 --> 02:03:27.990
Badrish Chandramouli: We have been working on recently again using faster as a just a simple building block component to handle all the storage and logging.

820
02:03:28.740 --> 02:03:36.210
Badrish Chandramouli: Such it. So this kind of highlights the importance of this component ization so that is completely independently useful to be able to build these larger end to end systems.

821
02:03:38.580 --> 02:03:42.090
Badrish Chandramouli: So faster as a celebrity is kind of fully featured at this point with lots of

822
02:03:43.920 --> 02:03:53.250
Badrish Chandramouli: Features and some of that and kind of go into a little bit more detail on some of the newer work that will be doing on scaling out the single node component and preserving

823
02:03:55.620 --> 02:03:58.200
Badrish Chandramouli: The notion of correctness and durability.

824
02:04:00.450 --> 02:04:02.010
Badrish Chandramouli: And we should talk about next.

825
02:04:04.620 --> 02:04:07.260
Badrish Chandramouli: So the first question we wanted to ask in the in this whole new

826
02:04:08.160 --> 02:04:15.060
Badrish Chandramouli: Direction of scaling or faster is hey, we are getting like 200 million operations or hundred plus million operations per second on a single machine. It sounds great.

827
02:04:15.570 --> 02:04:28.560
Badrish Chandramouli: Because it will be just basically lose all this performance. Once you start going remote right so the basic question is, let's say that we have remote sessions to faster instead of like running this embedded mode. Is there a lost that we were to face on that setting.

828
02:04:29.580 --> 02:04:34.950
Badrish Chandramouli: And interestingly, the answer to that is that not really right so here's a very simple example of having a

829
02:04:35.670 --> 02:04:44.070
Badrish Chandramouli: Remote client that sits on one as your machine. This is like a one of the higher end as your machines, talking to a remote machine and using TCP acceleration

830
02:04:44.580 --> 02:04:54.090
Badrish Chandramouli: For the connections are just standard TCP and there's just a tick mark in as you know that enables hardware acceleration so TCP is kicking off loaded to the network. And it turns out that

831
02:04:54.810 --> 02:05:03.270
Badrish Chandramouli: Faster in embedded mode, which is a red line is almost the same performance as this remote version of a service because shadow facts.

832
02:05:03.660 --> 02:05:08.670
Badrish Chandramouli: And this was quite exciting to us because basically it says that the network is not going to be the bottleneck in these systems.

833
02:05:09.000 --> 02:05:18.090
Badrish Chandramouli: And in fact, we tried running shadow facts against some alternative systems and found that the systems that like just not able to get the same level of performance as faster.

834
02:05:19.650 --> 02:05:22.080
Badrish Chandramouli: And and the reasons of course of that the fact that

835
02:05:23.130 --> 02:05:32.640
Badrish Chandramouli: We have this very high performance of key building block of faster, which handles shared memory very efficiently and then being being very careful about how we built the networking layer, we were able to get this level of performance.

836
02:05:34.350 --> 02:05:46.890
Badrish Chandramouli: Using standard networking principles and touching and so on. Right. So does a paper on this. I won't get into the details. It's on archive, it'll tell you exactly how we managed to pull this off. It's under revision at a conference and hopefully it should be out soon.

837
02:05:49.920 --> 02:05:53.490
Badrish Chandramouli: So now in shadow facts side, there was no notion of durability or

838
02:05:54.630 --> 02:06:02.850
Badrish Chandramouli: Or recover ability. It was just a cash and just backed by the hybrid log, but we were really talking about the performance of this in memory cache.

839
02:06:03.480 --> 02:06:06.600
Badrish Chandramouli: So one of the things that we did was to kind of understand how do we add

840
02:06:07.140 --> 02:06:16.110
Badrish Chandramouli: Meaningful lack of ability. So this nice consistency guarantees that you get in faster can also be present in the presence of failure recovery. Right, so it's not like you just use the cash.

841
02:06:16.470 --> 02:06:20.340
Badrish Chandramouli: And it's, it's basically of no use to that point. So the question was,

842
02:06:21.090 --> 02:06:30.510
Badrish Chandramouli: Like in order to reach that right we kind of. First of all, define this notion of a cache store, which I think I alluded to in the earlier slides, but to be more concrete. It is basically a combination of

843
02:06:31.170 --> 02:06:40.080
Badrish Chandramouli: Ephemeral storage that feeds into persistent layers. So you have this cash, for example, that sits here and then this compute which performs the sequence of operations on the cash.

844
02:06:41.340 --> 02:06:49.320
Badrish Chandramouli: And the interface of the sketch. So it's very simple. Right. It's basically a set of operations. For example, if this is the right ahead log the operations are basically and Q AMP DQ right

845
02:06:49.710 --> 02:06:58.620
Badrish Chandramouli: Or if it's a key value store, then you have like absorbs delete read modify rights or operations say performing this is the contract that you have with the storage system.

846
02:06:59.220 --> 02:07:04.170
Badrish Chandramouli: And then the other contract you have is this notion of commits where you can tell the cache store.

847
02:07:04.560 --> 02:07:11.040
Badrish Chandramouli: To commit the contents of main memory and it will kind of do it legally for you and provide you this commit token that you can now use to restore

848
02:07:11.520 --> 02:07:18.900
Badrish Chandramouli: This is the basic concept that you, you want the system to support and what we really want is that the operations that he performed

849
02:07:19.440 --> 02:07:27.780
Badrish Chandramouli: Are made durable with some notion of for consistency and we call up and the kind of constantly talk about is really this notion of prefix consistency, which I'll talk about in the next slide.

850
02:07:29.340 --> 02:07:40.560
Badrish Chandramouli: Of course, there's no self casters is really agenda that could means I'd had logging. It could mean the faster key value store in particular, which is artifact that we've been working on. It could also be an in memory database, the persistent checkpoints, which is also

851
02:07:42.210 --> 02:07:43.560
Badrish Chandramouli: Kind of another.

852
02:07:45.330 --> 02:07:47.220
Badrish Chandramouli: View of this cast sort obstruction.

853
02:07:49.290 --> 02:07:56.340
Badrish Chandramouli: So what is this notion of accountability that I talked about, right, like I said in this cash. So the memory part of last day. But what you would really like

854
02:07:56.880 --> 02:08:01.140
Badrish Chandramouli: Is the recovery model where these operations are being sent in the form of a session.

855
02:08:01.560 --> 02:08:06.330
Badrish Chandramouli: To discuss stored. So this is again on a single machine for now. So you have a sequence of operations that are being sent.

856
02:08:06.750 --> 02:08:14.190
Badrish Chandramouli: And what would be really nice is that if you could provide this know so prefix recover ability, but the idea is that for every session. You can guarantee that

857
02:08:14.670 --> 02:08:22.350
Badrish Chandramouli: If this cast so fail and came back, that'd be some prefix of his operations that are recovered that the world that he come back to. And then after

858
02:08:22.740 --> 02:08:29.700
Badrish Chandramouli: In some sense, there are no dependencies on this before world on the afterworld at this is very kind of an intuitive guarantee, which is

859
02:08:30.300 --> 02:08:36.390
Badrish Chandramouli: Definitely present in single up in single tenant systems, right, I mean operation logging is a very simple example of this.

860
02:08:36.720 --> 02:08:40.980
Badrish Chandramouli: Of the equivalent of prefixes governability for a single session that is talking to a store.

861
02:08:41.400 --> 02:08:51.750
Badrish Chandramouli: And here is the simplest thing that we want to extend this to a multi threaded system or a multi session system on a single machine. And it turns out that you can actually pull this off without any blocking and that's the the

862
02:08:53.310 --> 02:09:01.380
Badrish Chandramouli: The focus of a paper that we published at sigma 2019 that you can actually get these very nice prefix guarantees across these sessions on a single machine.

863
02:09:02.400 --> 02:09:08.820
Badrish Chandramouli: And it turns out that this guarantees are identical to prefix guarantee from a recovery perspective, because when the system comes back.

864
02:09:09.180 --> 02:09:21.750
Badrish Chandramouli: You can think of all of the operations before this line in the sand, as they call it as being the prefix of your database operations. And by doing this, you actually do not give up on the scalability of the system, but you also but you get

865
02:09:23.130 --> 02:09:28.410
Badrish Chandramouli: Meaningful like every guarantees and then you can do this frequent commits in some senses asynchronous accountability.

866
02:09:28.800 --> 02:09:40.200
Badrish Chandramouli: Where operations are completing synchronously at nanoseconds of time, but they are actually committing at much lower rates. But in some sense, they are like hundred milliseconds behind you. For example, but that's fine because

867
02:09:41.220 --> 02:09:53.070
Badrish Chandramouli: If an operations really is quite something to get committed it can always wait on commit and of course you're going to pay for the cost of commit in some sense it's it's decoupling the operation completion from the operation commit, which is what we pulled off in the CPR work.

868
02:09:55.470 --> 02:10:04.020
Badrish Chandramouli: Now, the thing that we did as the kind of follow up to this is to ask the question, what does it mean to now shot it right so we talked about

869
02:10:05.100 --> 02:10:10.560
Badrish Chandramouli: Recovery ability on a single machine Pacifica video that's all great, that's kind of part of the foster system.

870
02:10:11.010 --> 02:10:16.470
Badrish Chandramouli: Now what happens if you have multiple copies of this caster, right, in some sense, let's say that they are three completely

871
02:10:17.400 --> 02:10:25.020
Badrish Chandramouli: Separated casters since sometimes the caches also shot it equally right so we're not talking about like multiple copies of the cash or anything. Right. It's

872
02:10:25.680 --> 02:10:32.610
Badrish Chandramouli: Just three copies of the caster and have not even said anything about the physical placement, like the cash could be on a different machine. So this could be on a different machine.

873
02:10:33.060 --> 02:10:40.260
Badrish Chandramouli: And compete to be on a different machine, but they are all completely separated from each other. Of course, they could be on the same machine in something like a workflow. Right, very

874
02:10:41.070 --> 02:10:49.710
Badrish Chandramouli: Kind of talking about it if CPR sufficient in such a system that is it okay that is simply provide prefix recovery ability for each of these

875
02:10:50.190 --> 02:10:54.660
Badrish Chandramouli: Shots independently and clearly, the answer is no, because you might be running some

876
02:10:55.080 --> 02:11:01.500
Badrish Chandramouli: Competition on top and competitions, don't run in vacuum right there are definitely workflows that are running on top. Maybe you perform some operation a

877
02:11:02.040 --> 02:11:16.590
Badrish Chandramouli: And then there's there's some service invocation B and C is basically the result of a and b, right. So you really want to make sure that any operations that the performance discuss store do not commit before they are dependencies covered in some sense, you don't want

878
02:11:19.440 --> 02:11:27.990
Badrish Chandramouli: In something like, for example, if you did an operation like a commit on on see and you want to kind of completed in nanoseconds, as opposed to milliseconds of going to storage.

879
02:11:28.740 --> 02:11:40.950
Badrish Chandramouli: Wouldn't it be nice if you could say that I work, I could compete the operation. But from an accountability perspective that operation will not commit unless the things that it depends on, that is the end cues on A and B also commit and that would be the one that we want to live in.

880
02:11:42.030 --> 02:11:50.070
Badrish Chandramouli: And for a distributed key value store this again makes a lot of sense. Right, you have a bunch of compute just talking to a bunch of stories back ends again completely shattered.

881
02:11:50.730 --> 02:11:58.740
Badrish Chandramouli: And again, you have caches that are referencing each of those shards completely separated. So it's like a shattered caching to and you have all these computes talking to them.

882
02:11:59.400 --> 02:12:04.800
Badrish Chandramouli: Wouldn't it be nice, that if these computer could compute nodes could issues sequence of operations.

883
02:12:05.190 --> 02:12:11.070
Badrish Chandramouli: On the distributor ship on the distributed cache. So for example, you do an operation. See, for a better impression on a, b, and so on.

884
02:12:11.550 --> 02:12:17.610
Badrish Chandramouli: But you still get prefix consistency. The prefix of comfortability in some sense that if you do an operation on see from an operation on a

885
02:12:18.300 --> 02:12:29.760
Badrish Chandramouli: Then you do get the results immediately in nanoseconds, which is what it which is microseconds, which was on a cluster, because you can probably do a round trip within a cluster in around hundred microseconds. However,

886
02:12:30.750 --> 02:12:39.630
Badrish Chandramouli: You can get those results, but what you would what you would want to get is the notion of recoverable. It would basically follow us kind of a envelope of

887
02:12:40.020 --> 02:12:50.400
Badrish Chandramouli: Recovery because again prefix consistent. So eventually, you'll be told that, for example, as consistent or everything up to as consistent. But you would never be in a world where if may durable. But see, it's not

888
02:12:51.510 --> 02:12:59.520
Badrish Chandramouli: Right. So for example, I should be able to write to the sketch and someone has to be able to read from this cash submit the guarantee that the read is

889
02:13:00.300 --> 02:13:07.260
Badrish Chandramouli: conditionally committed on the right. So you don't have to wait for all the rights to get have to go through to the storage layer, in some sense, you get basically get

890
02:13:08.940 --> 02:13:09.960
Badrish Chandramouli: This notion of

891
02:13:11.640 --> 02:13:19.740
Badrish Chandramouli: Completion and coming dependencies in this distributed caster volver and that's that, in a nutshell, is what the PRS so DPS basically the

892
02:13:20.370 --> 02:13:24.450
Badrish Chandramouli: Extension of the concurrent with discovery idea which was on a single machine to the distributed system.

893
02:13:25.140 --> 02:13:33.420
Badrish Chandramouli: It gives the identical guarantee a CPR in some sense that you have these sessions that you create with this distributed the shuddered the capstone obstruction.

894
02:13:34.110 --> 02:13:38.340
Badrish Chandramouli: YOU'RE PERFORMING operations and then you basically return operations immediately. So

895
02:13:38.910 --> 02:13:44.250
Badrish Chandramouli: As an application, you can perform an operation come back with an 80 microseconds. If it was a remote operation.

896
02:13:44.700 --> 02:13:53.040
Badrish Chandramouli: And then you can continue your operation so that you get like hundred million operations per second of throughput, potentially, but at the same time you are being given these notions of

897
02:13:54.180 --> 02:13:57.090
Badrish Chandramouli: Recovery ability, a periodic commits in some sense of the

898
02:13:58.740 --> 02:14:03.690
Badrish Chandramouli: Of the prefixes of these sessions and you can think of the session as the building block of corralling the dependencies.

899
02:14:04.020 --> 02:14:10.260
Badrish Chandramouli: Right. For example, in a session, then the natural idea is that every operation is dependent on all the previous completed operations.

900
02:14:10.530 --> 02:14:17.010
Badrish Chandramouli: So if you if as an application, you have two options that don't depend on each other. You could just put them in two separate session. So the session is the

901
02:14:17.790 --> 02:14:24.900
Badrish Chandramouli: Obstruction for crawling away the dependencies between operations, but within the session, you get this very clean perfect scalability guarantees.

902
02:14:26.550 --> 02:14:35.730
Badrish Chandramouli: Of course, the fact that you're committing or you're completing operations before failure or. Before I commit means that you have to do some work control back ready for cash fails.

903
02:14:36.270 --> 02:14:48.390
Badrish Chandramouli: That means that you may have to undo some of the work that you did that you completed in your session. And that's what basically we handle in this work. The paper is not yet public, but it should be open in a couple of months and we send a pointer to you guys when that's ready

904
02:14:50.040 --> 02:14:54.390
Badrish Chandramouli: But we use this as a building block to build a distributed version of faster so you can basically create sessions.

905
02:14:54.780 --> 02:15:10.050
Badrish Chandramouli: That talk to this distributed cluster of foster machine so you can trade as a client, you could be running inside a lambda or a as your function and you could create these sessions that can now create these dependencies across these shuddered competition instances.

906
02:15:11.790 --> 02:15:16.380
Badrish Chandramouli: And of course, if you're a similar to the cloud, plus design. Right. I mean, if

907
02:15:16.710 --> 02:15:21.480
Badrish Chandramouli: These key value stores credible running locally. In this case, you're just having a local session that you're talking to.

908
02:15:21.780 --> 02:15:27.600
Badrish Chandramouli: Or they could be running remotely and then the, the question of where you place compute, for example, is an orthogonal thing to us right

909
02:15:27.810 --> 02:15:36.270
Badrish Chandramouli: It could be using something like or liens or variable functions or whatever to do the placement, but they all have access to this distributed shared storage obstruction.

910
02:15:37.980 --> 02:15:40.200
Badrish Chandramouli: And they interact. We are these prefix consistent sessions.

911
02:15:42.420 --> 02:15:47.130
Badrish Chandramouli: So I won't talk too much about the details. It's just been limited results. It shows that we can actually get like

912
02:15:48.030 --> 02:15:56.820
Badrish Chandramouli: Very nice levels of performance, even with the person so prefix consistency. Right. So for example, if you just run without any checkpoints. So the blue line, you get around 90 million operations per second.

913
02:15:57.300 --> 02:16:10.020
Badrish Chandramouli: In a, in a, in a small cluster of machines, but anyone with persistence and even with the cloud as the backing and later on hundred millisecond check pointing you actually get very good performance you get 30 to 60 million operations per second which is

914
02:16:11.130 --> 02:16:23.790
Badrish Chandramouli: And here we're talking about uniform distribution where the data is completely shattered. So you're actually doing a lot of remote operations in this case. And it turns out that even with reasonable skew or for example of point nine and you can still do a pretty decent job of

915
02:16:24.990 --> 02:16:26.280
Badrish Chandramouli: Getting performance in this setting.

916
02:16:28.500 --> 02:16:30.900
Badrish Chandramouli: And they took and I kept harping on this whole notion of

917
02:16:31.650 --> 02:16:36.750
Badrish Chandramouli: Separating operation completion from operation commit and this guy basically you kind of nailed down the

918
02:16:37.050 --> 02:16:45.930
Badrish Chandramouli: Value of that right. So that is the official completion that has shown on the right side of the graph, which is basically in very few milliseconds or often like hundreds of microseconds of completion time

919
02:16:46.470 --> 02:16:53.190
Badrish Chandramouli: But the commit times a much larger because you have to wait for this durable commit on a replicated file system like as you're right. So, this

920
02:16:53.970 --> 02:17:04.980
Badrish Chandramouli: Actually takes much longer, and you, you do get Layton sees have like 150 or 200 milliseconds for the actual commit sins. Sometimes the envelope of commit is basically lagging behind the envelope of operation completion in this world.

921
02:17:07.260 --> 02:17:16.140
Badrish Chandramouli: So that's all I will talk about in terms of faster and distributed faster and then very quickly talk about our kind of the foray into the other world of analytics in this with just one slide.

922
02:17:16.740 --> 02:17:20.370
Badrish Chandramouli: You can defeat talk to a song hang. For example, who was working with me as an intern.

923
02:17:21.360 --> 02:17:26.460
Badrish Chandramouli: Say, I think. Couple of years back, and we have a paper. This era strict mode, which kind of talks into goes into details of

924
02:17:27.000 --> 02:17:34.800
Badrish Chandramouli: How we simplified storage for analytics, but the very high level of challenge that was, I think, in the sense that it has big work just not right.

925
02:17:35.130 --> 02:17:43.710
Badrish Chandramouli: The, the goal was to basically optimize the storage layer for all of these databases for analytics workloads. So we wanted to be able to like

926
02:17:44.250 --> 02:17:52.260
Badrish Chandramouli: Optimize the data layout and minimize the number of iOS that he would do in this world and we basically came up with the data structure called acuity tree, which basically

927
02:17:53.130 --> 02:18:02.550
Badrish Chandramouli: Places the data on the cluster and gives a very simple storage API to the database with the social pages consists of give me the data that satisfies this hyper rectangle and space and

928
02:18:03.060 --> 02:18:11.790
Badrish Chandramouli: And in some of these simple store obstruction basically takes care of this data layout under the hood and minimizes the number of items that you would have to

929
02:18:12.330 --> 02:18:15.990
Badrish Chandramouli: Perform and we're basically going along this path as well and kind of

930
02:18:16.470 --> 02:18:26.640
Badrish Chandramouli: Taking this basic idea of the layout and applying it to things like caching and secondary indexing as well. And I won't talk into the details of that you can check out our website for some of the more recent work in the space.

931
02:18:29.070 --> 02:18:36.270
Badrish Chandramouli: So with that, I'm going to end. So you can have. So we have enough time for brainstorming. So basically, all of the work that I talked about faster.

932
02:18:37.350 --> 02:18:50.610
Badrish Chandramouli: CRA Ambrosia are all open source and we look forward to engagements from all of you guys as well. And they're open to collaborations. So that, so let's kind of stop at this and we can continue the discussion.

933
02:18:52.440 --> 02:18:52.860
Thanks.

934
02:18:54.870 --> 02:19:09.900
Joe Hellerstein: Awesome. Thank you both of you. That was really, really fun very dense. Why don't we take you guys actually bought us some time that that went quick so appreciate your efficiency. Why don't we take five minutes and stretch and

935
02:19:11.130 --> 02:19:16.050
Joe Hellerstein: If anyone needs to break, take a break and then we'll have our 15 minutes as planned for discussion. Sound good

936
02:19:16.500 --> 02:19:17.040
Sounds good.

937
02:19:19.560 --> 02:19:19.770
Joe Hellerstein: Yep.

938
02:24:57.870 --> 02:24:58.500
Jonathan Goldstein: Everybody hear me.

939
02:25:00.630 --> 02:25:00.990
Matthew Milano: Yep.

940
02:25:01.530 --> 02:25:01.980
Jonathan Goldstein: All right.

941
02:25:02.340 --> 02:25:03.150
Yes.

942
02:25:04.470 --> 02:25:16.740
Joe Hellerstein: I'm trying to lay out to sort of a little framework in the Slack channel discussion session, which you should be able to find if you just go to the general channel I put a link there that you can click on

943
02:25:18.810 --> 02:25:22.770
Jonathan Goldstein: See, yeah. There were two things that sort of popped into my head.

944
02:25:24.000 --> 02:25:27.870
Jonathan Goldstein: As well. I think one or two of them. I put little placeholders in the

945
02:25:29.100 --> 02:25:31.170
Jonathan Goldstein: In the thing. Okay.

946
02:25:32.340 --> 02:25:46.230
Joe Hellerstein: Great. Sorry. I mean, when you and produce get a chance. There's also questions in the MS talks part which we can interactively but some of them are technical and maybe you can just do offline while we're chatting and point us to a paper or whatever.

947
02:25:48.810 --> 02:25:51.360
Joe Hellerstein: So I you know I put I'm putting for like a very

948
02:25:52.680 --> 02:25:59.850
Joe Hellerstein: Simple frame of just like there's some things where we've overlapped. In the past, and it seems like we've got some cleanup work to do to understand each other better

949
02:26:00.480 --> 02:26:10.050
Joe Hellerstein: And then it seems like there's opportunities, you know, and obviously with that there may be opportunities to take next steps along those relatively common existing paths.

950
02:26:10.980 --> 02:26:22.050
Joe Hellerstein: And then it also sounded like we've got some common goals, going forward, where we, you know, if we can identify API's that hide our different pasts. Maybe we can share a future right

951
02:26:22.590 --> 02:26:22.890
Jonathan Goldstein: Yeah.

952
02:26:23.250 --> 02:26:29.220
Joe Hellerstein: If that's kind of the frame. I was thinking about and happy that then handed off to folks to start enumerating

953
02:26:31.980 --> 02:26:35.460
Jonathan Goldstein: I'm so I'm sorry. You said there's a link in

954
02:26:35.850 --> 02:26:43.050
Joe Hellerstein: If you go to the general channel. I said, please join the MS toxin discussion session channels. So if you click on discussion.

955
02:26:43.050 --> 02:26:44.460
Jonathan Goldstein: Oh, I see.

956
02:26:44.520 --> 02:26:46.620
Joe Hellerstein: It will add you to the discussion session channel.

957
02:26:47.250 --> 02:26:55.320
Badrish Chandramouli: Yeah, funding to the threads such do we want to take them here, or should we just continue responding on chat. What's your

958
02:26:55.740 --> 02:27:02.190
Joe Hellerstein: I think, you know, just because we've got a backlog of three hours of talks to respond to maybe a good idea is to do

959
02:27:02.190 --> 02:27:03.210
Badrish Chandramouli: That offline.

960
02:27:03.390 --> 02:27:04.260
Jonathan Goldstein: On. Sure.

961
02:27:04.560 --> 02:27:05.730
Badrish Chandramouli: And yeah.

962
02:27:05.790 --> 02:27:07.560
Joe Hellerstein: And then people who are

963
02:27:07.590 --> 02:27:10.050
Joe Hellerstein: You know, working on unrelated things for the absolutely

964
02:27:10.050 --> 02:27:12.270
Joe Hellerstein: Be ganging up afterwards so that other idea here.

965
02:27:12.270 --> 02:27:13.410
Badrish Chandramouli: Is to form a little community.

966
02:27:16.290 --> 02:27:16.650
Badrish Chandramouli: Yeah.

967
02:27:17.550 --> 02:27:19.740
Joe Hellerstein: But maybe let's use this time to think about kind of

968
02:27:20.250 --> 02:27:25.200
Joe Hellerstein: Can be doing differently, what we can do together. You know, maybe it's as simple as

969
02:27:26.580 --> 02:27:35.010
Joe Hellerstein: What are the goals for the community working in this space. How can we contribute, but I thought maybe we'd look back a little bit first at some of the things we've all done.

970
02:27:35.670 --> 02:27:35.940
Sure.

971
02:27:39.780 --> 02:27:49.350
Joe Hellerstein: So it seems like you know if nothing else, we've been doing storage systems and compute frameworks over them that have a lot of similarities. Right.

972
02:27:50.280 --> 02:27:52.050
Jonathan Goldstein: Definitely I am

973
02:27:53.580 --> 02:27:53.820
Badrish Chandramouli: Yep.

974
02:27:54.180 --> 02:28:01.740
Jonathan Goldstein: Yeah, I mean, there's a lot of layers to it there's there's mechanisms. There's abstractions, there's pain points and, you know,

975
02:28:02.790 --> 02:28:06.000
Jonathan Goldstein: Yeah, there's definitely a lot there's there's some common ground there for sure.

976
02:28:08.130 --> 02:28:08.640
Joe Hellerstein: There are

977
02:28:09.270 --> 02:28:11.040
Joe Hellerstein: A sense that like we can bucket. These

978
02:28:11.040 --> 02:28:12.600
Joe Hellerstein: Into topics that the broader

979
02:28:12.600 --> 02:28:13.680
Joe Hellerstein: CS community is

980
02:28:13.740 --> 02:28:14.910
Pursuing like are we

981
02:28:16.770 --> 02:28:21.030
Joe Hellerstein: Are we trying to define agendas here as a these two

982
02:28:21.420 --> 02:28:22.830
Joe Hellerstein: Given the commonality. Let's just talk

983
02:28:22.830 --> 02:28:22.950
Joe Hellerstein: About

984
02:28:22.980 --> 02:28:24.720
Badrish Chandramouli: US FOR A MINUTE. ARE WE

985
02:28:25.200 --> 02:28:26.880
Joe Hellerstein: Identified sort of a unique

986
02:28:26.880 --> 02:28:27.450
Joe Hellerstein: Agenda

987
02:28:27.540 --> 02:28:28.260
Joe Hellerstein: Between us

988
02:28:28.320 --> 02:28:30.000
Joe Hellerstein: Or is this kind of

989
02:28:31.170 --> 02:28:34.560
Joe Hellerstein: Just broadly aligned with other things we can name in the community like

990
02:28:34.710 --> 02:28:35.730
Badrish Chandramouli: What's the workshop with

991
02:28:36.870 --> 02:28:37.080
Badrish Chandramouli: So,

992
02:28:38.550 --> 02:28:39.870
Jonathan Goldstein: From my point of view.

993
02:28:39.900 --> 02:28:41.130
Badrish Chandramouli: I feel like

994
02:28:41.190 --> 02:28:44.520
Jonathan Goldstein: The big thing that ties all of this together is that

995
02:28:45.780 --> 02:28:48.000
Jonathan Goldstein: You know, distributed state is hard.

996
02:28:50.610 --> 02:28:56.250
Jonathan Goldstein: And this is this is basically, I think all of us are basically exploring

997
02:28:56.970 --> 02:28:58.050
Badrish Chandramouli: What happens

998
02:28:58.410 --> 02:29:01.800
Jonathan Goldstein: To distribute it state in a highly connected environment.

999
02:29:01.920 --> 02:29:03.300
Badrish Chandramouli: You know where the the

1000
02:29:03.300 --> 02:29:04.980
Jonathan Goldstein: constants are changing and the way they're

1001
02:29:04.980 --> 02:29:06.540
Jonathan Goldstein: Changing and

1002
02:29:07.500 --> 02:29:12.270
Jonathan Goldstein: And where because it's because everything all state is distributed. Everything is

1003
02:29:12.270 --> 02:29:14.460
Jonathan Goldstein: Connected just the scale of it.

1004
02:29:14.460 --> 02:29:15.960
Jonathan Goldstein: In the scope of it is just

1005
02:29:15.990 --> 02:29:16.680
Is just knew

1006
02:29:18.420 --> 02:29:26.670
Badrish Chandramouli: I mean, one thing that stands out to me is this some kind of fun. So I think that this comes with consistency, right, that are like these two worlds some sense where

1007
02:29:27.090 --> 02:29:36.480
Badrish Chandramouli: What is the best that you can do with like with one copy of the data. Right. Like, I think you can really push the envelope in terms of the best that you can do with one copy of the data.

1008
02:29:37.140 --> 02:29:43.500
Badrish Chandramouli: But there's an entire kind of orthogonal world around outside it, where you have multiple copies of the data. And then how do you kind of

1009
02:29:44.880 --> 02:29:49.950
Badrish Chandramouli: Like you want to have multiple copies of the data for for multiple reasons. I mean this could be cash locality. This could be

1010
02:29:51.090 --> 02:29:52.200
Badrish Chandramouli: Resiliency is gonna be

1011
02:29:53.370 --> 02:30:03.240
Badrish Chandramouli: Kind of handling extreme skew and all of that. I mean, there are multiple reasons for that. And, and when you take that step in across that line from a single copy of the data to multiple copies

1012
02:30:04.050 --> 02:30:15.660
Badrish Chandramouli: There are huge slew of questions and challenges that come up in some sense, I think of the faster line of work. It's been focusing on the left side of that line. Even the prefix consistency. Right. It's like

1013
02:30:16.110 --> 02:30:25.050
Badrish Chandramouli: And the distributed via prefix recover ability, it's still a single copy of the data as a single cash which is back, which is sitting in front of this of the shot of the data.

1014
02:30:25.560 --> 02:30:30.360
Badrish Chandramouli: And it turns out that you can push that pretty far you can get the 10 200 millions of operations per second, you get

1015
02:30:31.110 --> 02:30:39.510
Badrish Chandramouli: A reasonable amount of skew. I mean, we have gone or like to reasonable amounts of the factors and you can get a good mix of low latency and

1016
02:30:40.050 --> 02:30:49.320
Badrish Chandramouli: High performance while getting the consistency there, but the moment you will kind of get into this world of allowing multiple copies. In fact, we tried this.

1017
02:30:49.890 --> 02:30:56.460
Badrish Chandramouli: Like we went a little bit in the direction of having, for example, multiple copies of the cash and trying to figure out how you would keep them.

1018
02:30:57.270 --> 02:31:07.770
Badrish Chandramouli: synced, and the challenges that become so much more complicated and some and sometimes you have been addressing them with all of the lattice based ways of thinking about it, right. The the

1019
02:31:08.610 --> 02:31:17.010
Badrish Chandramouli: Challenge in my head would be like, how do, how do you take those concepts of consistency in the presence of multiple copies of data and distill them into something that

1020
02:31:17.910 --> 02:31:22.950
Badrish Chandramouli: A people can easily use and then be degenerates very nicely to the single copy case where

1021
02:31:23.340 --> 02:31:35.220
Badrish Chandramouli: If you had a single copy of the data that was sufficient for your workload. Could you actually do as well as we do in the distributed faster, where we didn't have to deal with all these challenges. That's I think that's an interesting way of looking at it and

1022
02:31:36.120 --> 02:31:38.700
Jonathan Goldstein: Spray. So that's so that's the shared state.

1023
02:31:38.880 --> 02:31:47.040
Jonathan Goldstein: That's the shared state side of it. And then there's the whole sort of virtual resiliency side of it, right, which is that you have all these things connected

1024
02:31:49.350 --> 02:31:57.540
Jonathan Goldstein: Yeah, I mean I guess we've we sort of think about it in terms of two distinct problems, right, there's the problem about, you know, making things resilient to failure.

1025
02:31:57.900 --> 02:32:07.770
Jonathan Goldstein: And having a programming model that that is nice that way where you don't have to worry so much about it. It's just taken care of by the system. And then there's the question of

1026
02:32:08.190 --> 02:32:15.570
Jonathan Goldstein: Of sharing, you know, state sharing where you have consistency in that, you know, that sort of rears its head and you know all these kinds of things.

1027
02:32:16.170 --> 02:32:23.910
Jonathan Goldstein: And I think that those are two distinct problems and I saw that, actually, that it seems to me like you have

1028
02:32:24.750 --> 02:32:37.530
Jonathan Goldstein: You have a similar kind of bifurcation of concerns, although they're related, in terms of mechanisms in your group to it seemed like Alvin was working more at the level of programming models and and

1029
02:32:38.550 --> 02:32:51.720
Jonathan Goldstein: You know, sort of, sort of closer to ideas like virtual resiliency and then and on things like that are more like the consistency in state sharing, you know, in that sort of thing. Just. Does that make sense from your point of view.

1030
02:32:53.760 --> 02:33:03.330
Joe Hellerstein: So I like the separation of concerns as a way to think about things that makes a lot of sense, as you say, they often come together and mechanism. So, for example,

1031
02:33:04.680 --> 02:33:18.990
Joe Hellerstein: In some of the work we did and and Cloudburst. This is the atomic fault tolerance work that was a euro CES this year we're using read atomic consistent isolation levels to guarantee resiliency.

1032
02:33:20.100 --> 02:33:33.030
Joe Hellerstein: So it's a you know it's an isolation level in the sense of transactional isolation levels. And that's how we first invented that stuff. But what we're using it for in this context is that, you know, it's an all or nothing appearance of the effects of

1033
02:33:34.080 --> 02:33:44.580
Joe Hellerstein: In this case, competition club functions, right, so that's not really for us at that. Well, it's a semantic GUARANTEE BUT IT'S NOT layered up into a programming model. It's right you know

1034
02:33:44.700 --> 02:33:45.030
Joe Hellerstein: Right.

1035
02:33:45.360 --> 02:33:58.830
Joe Hellerstein: And so you can cut this at different levels in terms of where you put your semantics in the bloom work. We tried to, you know, do whole program things so we wouldn't have storage isolation and consistency guarantees, we would only program guarantees.

1036
02:34:00.030 --> 02:34:05.010
Joe Hellerstein: And I think nowadays, maybe Alvin can speak to how we think about this more flexibly.

1037
02:34:06.120 --> 02:34:09.060
Joe Hellerstein: Across layers with citizen compilation

1038
02:34:11.700 --> 02:34:18.660
Alvin Cheung: Thing, it's basically going to boil down to both the language design problem and also compilations flashlight code generation problem.

1039
02:34:20.040 --> 02:34:34.650
Alvin Cheung: So you can imagine having different program models out front, and then being able to compile them down into single version that makes it a single language that makes it really easy to reason about these different aspects that you were mentioning about and then to code generation.

1040
02:34:34.650 --> 02:34:44.190
Jonathan Goldstein: Afterwards. Yep. Yep. And you can imagine that there could be different mechanisms underneath you know to sort of implement, you know, implement, that kind of thing.

1041
02:34:44.730 --> 02:34:45.270
Jonathan Goldstein: Um,

1042
02:34:45.690 --> 02:34:50.070
Jonathan Goldstein: But yeah, I'm very, you know, I sort of feel like there's enough

1043
02:34:51.450 --> 02:35:01.710
Jonathan Goldstein: There's there's enough systems that have found it valuable. I mean, like I said, we didn't. I mean we coined the term virtual resiliency, but we didn't invent it i mean it's it's it's been around, you know,

1044
02:35:02.010 --> 02:35:14.790
Jonathan Goldstein: floating around in a bunch of systems over time. And I don't think it's an accident that sort of somewhere around the 2000s. It just started getting pulled into stuff, you know, durable functions is another example where they have virtual resiliency and

1045
02:35:15.630 --> 02:35:22.350
Jonathan Goldstein: You know, so people increasingly are adding it to their systems. So it seems like a very valuable programming abstraction.

1046
02:35:22.890 --> 02:35:29.760
Jonathan Goldstein: You know, to tame a bunch of the complexity and then you get all kinds of interesting questions around like we saw that these

1047
02:35:30.180 --> 02:35:38.880
Jonathan Goldstein: These Paxos consistency protocols, you know, well what happens if if you try to build them on top of something that already gives you a virtual resiliency.

1048
02:35:39.300 --> 02:35:48.900
Jonathan Goldstein: Like are you making fundamental performance compromises, or is that a nice decomposition from a system architecture point of view, I don't know.

1049
02:35:49.770 --> 02:35:52.800
Jonathan Goldstein: I don't know the answer to that question. So

1050
02:35:53.370 --> 02:36:08.220
Jonathan Goldstein: But it seems interesting that from a programming language point of view, there is a bunch of systems now that have settled on some form of virtual resiliency is like the answer to team, you know, large scale distributed programming. So I think that's really interesting.

1051
02:36:09.630 --> 02:36:16.530
Badrish Chandramouli: And something I think at the end of the day, our clients have to be involved in this whole process. Right. It's like it. I think decoupling of

1052
02:36:17.730 --> 02:36:26.550
Badrish Chandramouli: Of the client from the state is definitely a good idea, but if you don't involve the clients in this end to end picture you're never going to solve the entire problem because

1053
02:36:27.840 --> 02:36:37.740
Badrish Chandramouli: Because there's always this the last mile problem, right, like you have salt as Lindsay, all the way to the client. But then what happens. And I feel like just thinking of this

1054
02:36:38.760 --> 02:36:44.340
Badrish Chandramouli: This form of virtual resiliency at the client, and the client contracts and things like that, in combination with these stories layers.

1055
02:36:44.910 --> 02:36:50.910
Badrish Chandramouli: As an interview and offering. I think that's where we will reach this vision of service of being able to write code.

1056
02:36:51.780 --> 02:37:01.680
Badrish Chandramouli: That operates on shared state without worrying about this problem. It's, it's, like it's a binary thing anybody have solved the problem or you haven't. Right. I mean, you don't want to be something 90% of the problem.

1057
02:37:03.810 --> 02:37:10.680
Jonathan Goldstein: Yeah, I mean, ideally, it's like virtual memory. I mean, I call it virtual resilience resilience, because it's like virtual memory. Once you have virtual memory.

1058
02:37:11.040 --> 02:37:25.110
Jonathan Goldstein: You just don't worry about like things like segmentation anymore. I mean, it's just like memory segments. For those of you that remember you know what that was like you know that i don't think most people, most young programmers, they probably don't even know what that is.

1059
02:37:26.160 --> 02:37:30.150
Joe Hellerstein: I think. LAUGHTER Now, the oldest person here and I'm too young for that.

1060
02:37:30.570 --> 02:37:32.160
Jonathan Goldstein: No, no you're not.

1061
02:37:32.400 --> 02:37:35.550
Jonathan Goldstein: Yeah, I remember it. You're not. You're not younger than

1062
02:37:36.210 --> 02:37:37.410
Joe Hellerstein: Me, My mom.

1063
02:37:37.770 --> 02:37:39.660
Joe Hellerstein: Taught me about reading overlay programs and

1064
02:37:39.660 --> 02:37:40.320
Jonathan Goldstein: Yeah yeah

1065
02:37:40.710 --> 02:37:41.220
Remember that

1066
02:37:42.960 --> 02:37:43.380
Joe Hellerstein: Anyway,

1067
02:37:43.860 --> 02:37:44.460
Um,

1068
02:37:45.780 --> 02:37:54.210
Joe Hellerstein: This is great. I am taking notes, by the way, I'm just trying, but others should as well. Just trying to capture some of the themes that people are saying in the discussion session slack.

1069
02:37:55.830 --> 02:38:03.450
Matthew Milano: But I think a different way to look at the client thing to be involved, is that the virtual resiliency you're providing is pushing it by what the client actually needs.

1070
02:38:04.050 --> 02:38:13.020
Matthew Milano: So you're saying about decoupling storage and consistency from virtual resiliency, what you're saying is that we're going to bake in a specific kind of storage obstruction that we want to provide the client.

1071
02:38:14.460 --> 02:38:20.490
Matthew Milano: One of the things that we're doing with the fast execution idea is that if the client doesn't need a particular consistency model or doesn't

1072
02:38:20.820 --> 02:38:31.830
Matthew Milano: Need a particular resiliency. That's some information that you can use to also optimize the storage. So if you treat this as a more cross cutting thing there's opportunities for performance optimization that would disappear by just doing them as modules.

1073
02:38:34.950 --> 02:38:50.400
Joe Hellerstein: So you're proposing and we have sort of whatever a complicated mix of consistency and and isolation guarantees for that aspect. You're saying we might have a similarly complicated world of resiliency guarantees that we want to get

1074
02:38:50.820 --> 02:38:57.270
Matthew Milano: Right, it's all basically what does, what does the application need us. It's in variance and how can you push those environments through the entire system.

1075
02:38:58.620 --> 02:39:05.490
Badrish Chandramouli: Yeah, and I think that's kind of relates to one of the findings that I saw personally in the South is fleet work right i mean they have

1076
02:39:05.880 --> 02:39:12.990
Badrish Chandramouli: They definitely have a set of applications where they do need virtual resiliency, but at the same time they were using the site channels, for example, to

1077
02:39:13.350 --> 02:39:21.000
Badrish Chandramouli: Send videos or audios on things like that where it wasn't clear that they wanted the same level of resiliency for that part of the system. And we see this

1078
02:39:21.390 --> 02:39:30.420
Badrish Chandramouli: Team in like things like sensor networks and of course I see you're kind of going along the path of evolution and like networks definitely networks are getting faster. So it is getting

1079
02:39:30.960 --> 02:39:45.300
Badrish Chandramouli: Faster and but there's always going to be this gap right like the speed of light is always going to be, of course, the, the bottleneck, but there are different levels of latency is that you see. I mean, on a single machine you can like hundred nanoseconds of

1080
02:39:46.380 --> 02:39:53.490
Badrish Chandramouli: Look up times for an empty operation on on MDM drive you get hundred microseconds. And then on cloud storage, you're all the way up to a

1081
02:39:53.940 --> 02:40:01.200
Badrish Chandramouli: 10 minute 10 milliseconds. These are orders of magnitude. I mean, these are not just a single order of magnitude. Right. It's huge differences between them and being able to like

1082
02:40:02.100 --> 02:40:11.340
Badrish Chandramouli: Operated all of these skills and basically for a given application to get the best possible performance that you can get. I think viewed basically be able to

1083
02:40:12.450 --> 02:40:23.130
Badrish Chandramouli: Create, easy, easy to use and easy to program systems and at the same time, get the best that this hardware stack offers. I think that would be the Holy Grail in some sense of all of this line of work.

1084
02:40:23.400 --> 02:40:30.930
Jonathan Goldstein: Right investing is a funny thing here. Right. It's basically the cheapest way to deliver the experience that's considered the target experience.

1085
02:40:31.110 --> 02:40:33.930
Badrish Chandramouli: That's true, yeah. I think the cost is a huge role in this as well.

1086
02:40:37.830 --> 02:40:43.200
Alvin Cheung: So perhaps a related question is also like you know how to solicit that input from the user, right, because

1087
02:40:43.740 --> 02:40:51.870
Alvin Cheung: Even if they're all these different choices that can be made at the implementation level. I mean, someone's gonna have to tell us. I mean, what kind of application guarantees, they would like.

1088
02:40:52.530 --> 02:40:53.730
Jonathan Goldstein: And defaults matter a lot.

1089
02:40:54.210 --> 02:40:54.450
Alvin Cheung: Yeah.

1090
02:40:54.810 --> 02:41:02.610
Alvin Cheung: Yeah, definitely. Right. Because like sometimes they don't know what the defaults are they kind of walked into that and then no one. Don't knowing what they actually expecting

1091
02:41:03.090 --> 02:41:03.510
Matthew Milano: Yeah yeah

1092
02:41:03.930 --> 02:41:04.500
Matthew Milano: So that's

1093
02:41:04.590 --> 02:41:07.170
Alvin Cheung: Why I think it also becomes a user interface, perhaps

1094
02:41:07.170 --> 02:41:09.450
Badrish Chandramouli: Question in terms of these inputs and

1095
02:41:10.860 --> 02:41:12.930
Matthew Milano: I really think that's where the language design pops in, as well.

1096
02:41:13.980 --> 02:41:23.100
Badrish Chandramouli: And then, yeah, let's talk on this. The hydrolysis and the that's super exciting. I think that's kind of getting to the heart in some sense of this problem, right, like how do you

1097
02:41:23.640 --> 02:41:35.610
Badrish Chandramouli: How do you expose all of this complexity to the application in this very clear kind of compartmentalize pillars and then let the system take care of the details that the gory details of how to pull it off under the hood.

1098
02:41:37.320 --> 02:41:42.600
Jonathan Goldstein: Yeah. And even when even when to expose them to the compartmentalization right i mean

1099
02:41:44.070 --> 02:41:53.280
Jonathan Goldstein: I think the best programming languages for for writing complex software are the ones where there's always like an easy experience that makes it very easy to do something, but

1100
02:41:53.730 --> 02:42:11.640
Jonathan Goldstein: And then if you want to do something very performant. You know that's hard. You know, you can always learn what you need to learn in order to do that, and then make it happen. But this is like when you first go to it. The that you can use to just sort of get something done reasonably quickly.

1101
02:42:12.120 --> 02:42:13.800
Badrish Chandramouli: Right, it's as you go in some sense.

1102
02:42:13.830 --> 02:42:26.640
Jonathan Goldstein: Yeah, I mean, I think that's the great thing about like object oriented languages like c++ and C sharp and Java and things like, well, maybe not Java, but certainly c++ and C sharp if you if you want to have that high performance.

1103
02:42:28.110 --> 02:42:28.650
Jonathan Goldstein: You know,

1104
02:42:30.300 --> 02:42:34.920
Jonathan Goldstein: If you want to have high performance. You can you just need to know how to how to write your code.

1105
02:42:36.030 --> 02:42:45.270
Badrish Chandramouli: That we have a local expert Sebastian here has been working on a detection of programming languages and these kind of shows the DNC. I mean, do you have something to chime in.

1106
02:42:45.840 --> 02:42:54.960
Sebastian Burckhardt: Yeah, I find it very interesting. Sorry. I wasn't here earlier. But yeah, I totally agree. So personally, a lot of the problems that came up are things that I've looked at in the past, or I'm looking at now.

1107
02:42:55.350 --> 02:43:05.430
Sebastian Burckhardt: So one thing I'm not looking at, which is the replication part. So, like, like, like you said earlier, it is possible to separate all the complications that come from multiple copies

1108
02:43:05.940 --> 02:43:17.310
Sebastian Burckhardt: From consistency that purely comes from reconciling storage and compute. So right now I'm sort of say forget about replication. Just look at single copy consistency, but in a context of

1109
02:43:17.790 --> 02:43:24.480
Sebastian Burckhardt: Several as programming and having, you know, working on a compute, storage infrastructure where you have the separation to deal with.

1110
02:43:24.990 --> 02:43:30.900
Sebastian Burckhardt: And that's actually hard enough to start with. And even there. You have you have consistency issues there.

1111
02:43:31.350 --> 02:43:47.280
Sebastian Burckhardt: So like for for workflow executions, you know, explaining to people. What does consistency mean if you have multiple steps that cause only depend on each other actually require some it's not as complicated as for application, but it's still not trivial for us to understand

1112
02:43:48.810 --> 02:44:00.120
Sebastian Burckhardt: That's currently they're putting my, my focus. And I think the service part is also like one thing that that continuously strikes me as to how many of the systems we build have

1113
02:44:00.870 --> 02:44:11.490
Sebastian Burckhardt: And products are designed for certain level of scale. And if you put you put much less load on a stage. They're not good implementation side or you put much more load on it, they're not

1114
02:44:12.030 --> 02:44:19.320
Sebastian Burckhardt: So being able to build something that actually delivers good price to performance apart provides section and upload is actually not easy at all.

1115
02:44:20.040 --> 02:44:22.950
Sebastian Burckhardt: And I don't think I ever typical design through they've worked for that.

1116
02:44:23.850 --> 02:44:24.810
Sebastian Burckhardt: So, yeah.

1117
02:44:26.400 --> 02:44:34.800
Joe Hellerstein: Yeah, Sebastian, you missed the, the early talking. Then on some of the work we've done to try to get some any scale sorts of designs.

1118
02:44:35.070 --> 02:44:36.120
Badrish Chandramouli: So this was recorded right

1119
02:44:36.180 --> 02:44:42.150
Joe Hellerstein: To thank Lisa, unfortunately. That part was not recorded. But when we will send along and you Anna system that was one of its goals.

1120
02:44:42.600 --> 02:44:48.360
Joe Hellerstein: And one of the keys to that was to try to write things that are basically message passing and coordination free

1121
02:44:48.990 --> 02:44:56.670
Joe Hellerstein: So that across threads and across machines. Same implementation and then you know you try to get the single threaded piece really fast.

1122
02:44:57.210 --> 02:45:12.360
Joe Hellerstein: Right, so the multi multi threaded single machine implementation is getting maximum until the out of the box, but the architecture schools up anyway. We've one example and a design principle that might be fun to toss around. Yeah, yeah.

1123
02:45:27.420 --> 02:45:28.830
Joe Hellerstein: One thing I think that, you know,

1124
02:45:29.850 --> 02:45:31.140
Joe Hellerstein: May have been

1125
02:45:32.910 --> 02:45:34.950
Joe Hellerstein: Something that struck me in all our toxins, we didn't

1126
02:45:35.040 --> 02:45:36.090
Joe Hellerstein: Use a lot of

1127
02:45:36.390 --> 02:45:47.070
Joe Hellerstein: Terminology from the literature and distributed systems. I'm going to maybe use assume that we all knew what we were doing. But you know, I want to make sure we're not off reinventing at the same time. And I wondered

1128
02:45:48.330 --> 02:45:50.100
Joe Hellerstein: If there are thoughts here about kind of

1129
02:45:51.600 --> 02:46:03.480
Joe Hellerstein: What's, what's net new what's old wine in new bottles. And why is that a helpful conversation now like what's really different. So produce, you talked a bit about technology trends for sure.

1130
02:46:04.530 --> 02:46:04.740
Yeah.

1131
02:46:07.560 --> 02:46:13.200
Jonathan Goldstein: So I don't know, to, to my way of thinking. The thing that's really, that's really different is

1132
02:46:13.740 --> 02:46:24.090
Jonathan Goldstein: I sort of alluded to it a little bit in the slides is just the relative costs of things are changing so differently. And you know what, we're kind of ending up with is a world where

1133
02:46:25.050 --> 02:46:32.910
Jonathan Goldstein: You know CPUs are the expensive resource and, you know, moving bites around in and storing them. And all of that is almost free of charge.

1134
02:46:34.050 --> 02:46:40.110
Jonathan Goldstein: And and of course you're limited, you know, in latency by speed of light, but bandwidth is is

1135
02:46:41.010 --> 02:46:49.560
Jonathan Goldstein: It requires a different way of writing your system frequently than what people are doing today, but if you program in a way that's performance oriented.

1136
02:46:50.010 --> 02:47:00.480
Jonathan Goldstein: And, you know, like in Ambrosia. What did that mean well, that meant doing adaptive badging, you know, for instance, and as long as you're, you know, careful to implement that way then.

1137
02:47:02.100 --> 02:47:18.090
Jonathan Goldstein: I feel like the world we're moving to is, is the you know the CPUs are are the expensive component and they're getting further and further away from each other, you know, relative to the conversation. So, you know, more of the conversation can happen before they can coordinate

1138
02:47:19.710 --> 02:47:24.360
Jonathan Goldstein: And I think that that's having, you know, Deep Impact on

1139
02:47:26.100 --> 02:47:37.800
Jonathan Goldstein: what's desirable and on the algorithms that we used to coordinate things and and all of that. And I think that you see evidence of that in in everything that that both groups are doing

1140
02:47:40.830 --> 02:47:41.520
Badrish Chandramouli: To me those like

1141
02:47:43.980 --> 02:47:49.590
Badrish Chandramouli: I approach to research as more of a bottom up. Like, you start with a single code on a single machine and see how that

1142
02:47:50.070 --> 02:48:02.640
Badrish Chandramouli: What's the best that you can push it to right and then kind of take those clean steps along the way, going from Tier one call to an end course. And then, of course, to maybe two machines to machines and machines and then geo distributed right like

1143
02:48:04.050 --> 02:48:13.740
Badrish Chandramouli: It's, it's interesting that like at each of those steps as this huge jump right i mean there's like an order of magnitude or more jumps that you see and

1144
02:48:14.640 --> 02:48:17.670
Badrish Chandramouli: Just natively putting these systems together. You make the first

1145
02:48:18.330 --> 02:48:28.380
Badrish Chandramouli: The first attempt at going from one code to encode you realize that you only get one call performance, right, and then you have to kind of get into the weeds and figure out how do you solve the problem. So you actually get linear scalability.

1146
02:48:29.040 --> 02:48:37.320
Badrish Chandramouli: And I saw the same thing or Calvin going from one machine to two machines, right, like the like when we had like the first two machine deployment of faster.

1147
02:48:37.740 --> 02:48:46.890
Badrish Chandramouli: The performance was abysmally slower than a single machine. I mean, it was like hundred times slower than what you got on a single machine, but then go back to the drawing board. Try to understand what's going on and

1148
02:48:47.310 --> 02:48:55.770
Badrish Chandramouli: Kind of solve it from a bottom up perspective. And finally, we got back the performance. And the same thing happens in the tiered storage setting right like

1149
02:48:57.150 --> 02:49:03.450
Badrish Chandramouli: As long as things for a main memory. You were like you were getting great performance. But the moment things dropped off to SSD. You saw this huge

1150
02:49:04.080 --> 02:49:12.330
Badrish Chandramouli: Drop in performance and then you have to realize okay the CPU path of IO is the most important thing in the world right so now let's just kind of figure that out and

1151
02:49:12.930 --> 02:49:24.540
Badrish Chandramouli: Optimize it to the best that you can, and then you add Cloud Storage. And that's a whole new slew of craziness that comes in with respect to consistency reliability and kind of thing so

1152
02:49:26.100 --> 02:49:30.540
Badrish Chandramouli: Having a more careful bottom up approach to solving these and then you end up

1153
02:49:30.990 --> 02:49:39.150
Badrish Chandramouli: Getting to the constants that Jonathan talked about, right, I mean yeah you figured out how to build the systems. You're really good at it at in terms of building systems, you're getting to the bare metal

1154
02:49:39.780 --> 02:49:53.670
Badrish Chandramouli: Now you get to the constants, like where are the systems and what other fundamental a trade offs are the differences like will main memory. Always be hundred thousand times faster than storage and these are the fundamental questions that now you can grapple at that point.

1155
02:49:54.810 --> 02:50:01.350
Badrish Chandramouli: And then that kind of leads to this whole consistency challenges slightly in some sense, giving up consistency or kind of looking at, like,

1156
02:50:02.370 --> 02:50:09.330
Badrish Chandramouli: Looking at things like replication is is something we are accepting. As a matter of fact, because the single

1157
02:50:09.750 --> 02:50:14.910
Badrish Chandramouli: Systems are not able to scale sufficiently are now knowing exactly where you kind of make that choice like

1158
02:50:15.330 --> 02:50:28.290
Badrish Chandramouli: For a given application when when when do the needs of that application become strong enough that you that you would know that you will have to make that leap is a very interesting question, and it's kind of like the the

1159
02:50:29.490 --> 02:50:37.710
Badrish Chandramouli: The line between like strongly consistent systems and kind of eventually have a consistent systems, right. So I think that's

1160
02:50:38.070 --> 02:50:47.370
Badrish Chandramouli: The laying of that space. I mean, maybe a simple workshop paper that just talks about this space will be super interesting. I think maybe you guys have done it already. But I think that would be really valuable.

1161
02:50:48.930 --> 02:51:01.230
Joe Hellerstein: Yeah, I would definitely refer you to Peter Bayless is working on highly available transactions which does layout various isolation and consistency levels also to Natasha is longer version of her paper on

1162
02:51:02.940 --> 02:51:09.780
Joe Hellerstein: On our client centric isolation, there's a discussion of consistency or you can read the front of the pieces.

1163
02:51:10.290 --> 02:51:13.560
Joe Hellerstein: So there's two two takes on these problems.

1164
02:51:14.730 --> 02:51:25.380
Joe Hellerstein: But they're all, with respect to traditional storage consistency guarantees as opposed to application level guarantees and I think, you know, part of the work that Matthew and Alvin and others.

1165
02:51:25.770 --> 02:51:32.100
Joe Hellerstein: On our end myself focused on over the last little while it's been up leveling to take applications methods and

1166
02:51:33.690 --> 02:51:39.660
Jonathan Goldstein: There's, there's another I think really important thing going on here.

1167
02:51:41.010 --> 02:51:42.510
Jonathan Goldstein: And I feel like the

1168
02:51:44.400 --> 02:51:54.870
Jonathan Goldstein: This again this goes back, it's not it's not Ambrosia wasn't the first to make this observation by any means, but I think it's become very current and important in the

1169
02:51:56.040 --> 02:51:57.900
Jonathan Goldstein: If, if what you they're

1170
02:51:59.160 --> 02:52:07.020
Jonathan Goldstein: Fundamentally contentions expensive contention is expensive to handle and you know you have multiple things going at the same state.

1171
02:52:07.470 --> 02:52:17.460
Jonathan Goldstein: Then automatically you're in a box that is fundamentally expensive expensive box to be a part of the reason why Ambrosia gets the performance that it gets

1172
02:52:17.820 --> 02:52:27.600
Jonathan Goldstein: Is it makes the observation that if what you really want just to protect yourself from failure and you don't actually have contention. You just want recover ability

1173
02:52:28.020 --> 02:52:38.280
Jonathan Goldstein: Right, you don't. It's not about going it's shared state. It's just about making you know protecting yourself from failure, making sure you can recover creating deterministic replay ability

1174
02:52:38.610 --> 02:52:46.380
Jonathan Goldstein: You can fully solve that problem without solving the contention problem and that has fundamental performance consequences.

1175
02:52:46.980 --> 02:53:00.420
Jonathan Goldstein: So, you know, for so trying to solve that problem by turning it into a contention problem by using mechanisms that you use in order to address the problem of shared state is probably not a good idea.

1176
02:53:00.900 --> 02:53:05.190
Jonathan Goldstein: It's, I think, a fundamentally performance limiting one and it goes back to those trends.

1177
02:53:05.580 --> 02:53:13.320
Jonathan Goldstein: That I was talking about. Because if your CPUs are getting further and further away from each other, relative to the length of the conversation. They can have

1178
02:53:13.620 --> 02:53:21.990
Jonathan Goldstein: Before they coordinate, then that means that any coordination coordination dependent protocol that you use is fundamentally throughput limiting

1179
02:53:22.500 --> 02:53:29.760
Jonathan Goldstein: Because if if the data that you send is sort of invalidated by, you know, some kind of request response.

1180
02:53:30.240 --> 02:53:42.900
Jonathan Goldstein: You know paradigm right, then that means that more and more of what you send is wasted. And if you can get away from a request response kind of paradigm to solve your problem.

1181
02:53:43.440 --> 02:53:52.650
Jonathan Goldstein: Then you are in a much better place in terms of performance. And so I think that one thing that has emerged from this

1182
02:53:53.160 --> 02:53:59.010
Jonathan Goldstein: Is that when what you're trying to do is get recover ability from something that's deterministic.

1183
02:53:59.460 --> 02:54:14.010
Jonathan Goldstein: You really really don't want to use these these the same mechanisms that you use to handle contention over shared state. And I think that's actually really important and maybe not the way that traditional distributed systems like to talk about things.

1184
02:54:17.340 --> 02:54:19.830
Joe Hellerstein: That's an interesting hypothesis. I don't know if I agree.

1185
02:54:21.540 --> 02:54:28.380
Joe Hellerstein: Let me, let me make a counter argument for what it's worth, and this is based on part also I think my misunderstanding of

1186
02:54:29.190 --> 02:54:44.490
Joe Hellerstein: Which I put in the MS toxic of the cartoon for prefix what he called CPR and CPR. But let's say for example that I have a set of independent clients that have no intention whatsoever. And I want to give them or liability.

1187
02:54:45.180 --> 02:54:53.370
Joe Hellerstein: I can essentially just you know spool off their actions to any storage in the world, and it's fine. And I'm bottleneck, but nothing. Yeah.

1188
02:54:54.690 --> 02:54:55.050
Joe Hellerstein: Right.

1189
02:54:55.110 --> 02:55:02.970
Joe Hellerstein: That's right. Sure. Okay. So it's only interesting when there's contention and if your solution to contention is

1190
02:55:04.920 --> 02:55:10.980
Joe Hellerstein: Coordination pre in the style that many of these potential schemes. We've been playing with our then you do it in the background.

1191
02:55:12.720 --> 02:55:27.450
Joe Hellerstein: So, in essence, that's what Anna and Cloudburst are doing today is they're saying you can do your rights to the local cache, you can back them up as you like in the background. They will all get rectified and merged using monotonic computations.

1192
02:55:27.840 --> 02:55:32.220
Jonathan Goldstein: So, so that's an interesting point. So, so I think so. So maybe I need to add a footnote.

1193
02:55:33.330 --> 02:55:46.560
Jonathan Goldstein: To what I to what I said before, and that footnote is something along the lines of, if you're going to use the same piece of software to handle both kinds of situations, then you better build into your protocol, a

1194
02:55:47.640 --> 02:56:02.520
Jonathan Goldstein: You know you better build into your protocol. A a branch that when there isn't contention makes sure that there's no kind of request response a dependency on making forward progress.

1195
02:56:03.390 --> 02:56:05.700
Jonathan Goldstein: Indeed, so, so maybe that's another way of

1196
02:56:05.700 --> 02:56:06.180
Jonathan Goldstein: Saying it

1197
02:56:06.480 --> 02:56:15.900
Joe Hellerstein: Is this idea of coordination trina's that everybody makes progress on their own. And that's why this content stuff's important because any monotone computation you want to do can be achieved that way.

1198
02:56:17.280 --> 02:56:22.050
Joe Hellerstein: Yeah. Yeah. So I think we're definitely agreeing on I'm not coordinating where you don't need to

1199
02:56:22.350 --> 02:56:34.560
Joe Hellerstein: You know thing that struck me, though, and some of the work and I didn't understand this, I think, is um well attorney and ambrosia a linear log is by definition a point of coordination, right, because you have to contend for the tail.

1200
02:56:34.890 --> 02:56:49.020
Jonathan Goldstein: Yes. Yep. Yeah. So, so there is a so in Ambrose's design. Yeah, I saw, I saw your question about that. So there is a single immortal coordinator that

1201
02:56:50.310 --> 02:57:06.660
Jonathan Goldstein: That for Ambrosia basically takes all the incoming requests and linear rises them into a log and that what we're doing there is exactly the same as like what a database would do with right ahead logging and all of that, you know, in order to kind of do that as efficiently as we can.

1202
02:57:08.340 --> 02:57:12.120
Jonathan Goldstein: You know, in terms of designs for having a

1203
02:57:13.230 --> 02:57:20.520
Jonathan Goldstein: scaled out, like, like I said, elastic scalability. You know, it's something that's interesting to us. That is not a problem that we fully solved yet.

1204
02:57:21.840 --> 02:57:30.840
Jonathan Goldstein: But we're thinking that it will go along the lines of a design where you know each shard is one of these things right and then you do partitioning across them.

1205
02:57:31.860 --> 02:57:34.590
Jonathan Goldstein: With some kind of routing, you know, to avoid

1206
02:57:35.700 --> 02:57:41.160
Jonathan Goldstein: So that so that the the the writing to the tale of the log isn't something that happens globally.

1207
02:57:44.430 --> 02:57:51.870
Badrish Chandramouli: Yeah, and from the fastest shared memory perfect scalability standpoint, right, the bottleneck is really

1208
02:57:54.330 --> 02:58:06.900
Badrish Chandramouli: The cost of cash coherence. Right. I mean, if there's a single piece of memory in which everyone is contending, then there will be the cost of cash coherence on that and there's definitely limits to what

1209
02:58:08.430 --> 02:58:16.620
Badrish Chandramouli: Scalability you can achieve on a single cache line right and that becomes the eventual bottleneck that you would see. But the hope is that

1210
02:58:17.670 --> 02:58:24.090
Badrish Chandramouli: What we have seen in practice is that for the typical skills. If you look at the worst case contention, it doesn't

1211
02:58:24.720 --> 02:58:38.760
Badrish Chandramouli: Doesn't get so bad that that becomes the overall bottleneck of the distributed system, as you see it right and and and be you think there's some in place updates and kind of making sure that you're

1212
02:58:40.200 --> 02:58:50.670
Badrish Chandramouli: You're not, for example, having an interlock to increment that you're doing at the tail off a log. For each of those operations. That's where you go from 10 million to 100 million. Right. That's like, that's a very clear.

1213
02:58:52.530 --> 02:59:01.620
Badrish Chandramouli: Line in the cover. Maybe you can get 10 million interlocked increments per second. You can get hundred million concurrent cash current operations per second. So that's exactly what faster. Is it is just

1214
02:59:01.950 --> 02:59:12.270
Badrish Chandramouli: Directly exposing that Harvard differential between contended rights to the tale of the log versus on contended updates to also say contended updates to in memory cache lines.

1215
02:59:12.930 --> 02:59:19.230
Badrish Chandramouli: And. And the question is, in a distributed system, as you have all of these sessions, talking to or creating transactions that are talking

1216
02:59:19.890 --> 02:59:27.810
Badrish Chandramouli: What is the contending point here that may be one piece of item when everybody's trying to run the transaction on that particular item. Now of course you have to linear is

1217
02:59:28.680 --> 02:59:37.020
Badrish Chandramouli: The sequence of operations on that. And the best you can do there is really to say that, let's try to use CPU cache coherence, the best we can but just keeping one copy of the data.

1218
02:59:37.440 --> 02:59:47.640
Badrish Chandramouli: And letting CPU cache coherence handle the dependencies and then using database transaction mechanisms on top to to give you that.

1219
02:59:48.420 --> 03:00:05.970
Badrish Chandramouli: Lexi liability or linear visibility of operations on that contended cache line right and cache lines. If you read if it was a multi step transaction, but sometimes the the CPR guarantees, it's really one step one level below the notion of transactions, because what it says is that

1220
03:00:07.980 --> 03:00:15.270
Badrish Chandramouli: If you're given a sequence of operations that might independently be transactions, for example, saying that some prefix of those transactions are committed

1221
03:00:15.930 --> 03:00:25.440
Badrish Chandramouli: By non after committed. I know transactions themselves might be using to face logging on a single machine. It could be a distributed lock that it takes or could be some consensus protocol that running on a distributed machine.

1222
03:00:25.830 --> 03:00:33.900
Badrish Chandramouli: But it's really about before and after that respect to that sequence of operations that what the protocol itself cares about

1223
03:00:36.150 --> 03:00:42.210
Jonathan Goldstein: Right, there's sort of a meta point that actually that I'd like to make to which is that what Ambrosia does

1224
03:00:43.470 --> 03:00:54.450
Jonathan Goldstein: Relative to all these protocols, is it factors out a part of it that by necessity ends up being part of all of them but but it says we're going to do just that part.

1225
03:00:54.870 --> 03:01:06.330
Jonathan Goldstein: And maybe you want to layer on top of it, you know, these other things. So, so the one part of the contention handling that it does is the linear ization of operations.

1226
03:01:07.080 --> 03:01:15.990
Jonathan Goldstein: You know a bunch of operations arrive somewhere and you linear eyes them, you make and you put them in order. Somewhere you have to decide what happened what goes first. Right.

1227
03:01:16.500 --> 03:01:29.670
Jonathan Goldstein: Now of course you can chart it per core. You can chart it per, per computer you can you can do it any way you like. But it basically says at some somewhere in the system. There's going to be some point

1228
03:01:30.060 --> 03:01:40.410
Jonathan Goldstein: Where you where you linear is access to something, some resource and and it's that linear ization that you're going to be deterministic relative to

1229
03:01:41.130 --> 03:02:01.020
Jonathan Goldstein: And anything that you want to do beyond that you could layer on top of it right so you could take something like that multi Paxos protocol, you know, and layer it on top of that assumption. Right. So. So in a way, the kind of contention handling that Ambrosia is doing is this very targeted.

1230
03:02:02.730 --> 03:02:05.430
Jonathan Goldstein: Thing that shows up in all of these systems.

1231
03:02:06.450 --> 03:02:14.310
Jonathan Goldstein: And it only does that one part of it and and there's so much value in doing that, one part of it that it's worth kind of calling it out.

1232
03:02:14.790 --> 03:02:19.680
Jonathan Goldstein: You know, as its own thing. Now the question is if you wanted to make a high performance of these other things.

1233
03:02:20.070 --> 03:02:32.430
Jonathan Goldstein: You know, and that you had some kind of like suppose you wanted to build shared state, you know, on top of Ambrosia right. Would you be somehow performance limited by by separating out this one mechanism.

1234
03:02:33.210 --> 03:02:46.410
Jonathan Goldstein: Right and saying that this mechanism is important enough that it has its own you know its own implementation and its own its own place you know layering in the hierarchy of things. And it's that mechanism that linear rises things that logs them.

1235
03:02:48.870 --> 03:02:50.220
Jonathan Goldstein: That's one way you can look at it.

1236
03:02:50.700 --> 03:02:57.540
Badrish Chandramouli: And then deleted thing is to let nothing is it I mean as database logs. They're kind of trying to really optimize these systems and get hundred million operations per second but

1237
03:02:57.960 --> 03:03:01.620
Badrish Chandramouli: Look at the actual applications that are running on top. They might be heavy enough that maybe

1238
03:03:02.190 --> 03:03:12.510
Badrish Chandramouli: Maybe those differences don't matter, right, like maybe those bottlenecks don't matter anymore. And it's like if you're if each of your steps in your workflow is it's a complex operation then then maybe

1239
03:03:13.440 --> 03:03:20.730
Badrish Chandramouli: All of this effort is wasted. But the thing is that there's the other side of that coin is that header or there's going to be some examples of

1240
03:03:21.120 --> 03:03:29.340
Badrish Chandramouli: applications where it does matter. Maybe there's like a streaming workflow, whereas simply implementing counters. Right. And in those cases, these bottlenecks start

1241
03:03:29.910 --> 03:03:38.490
Badrish Chandramouli: Showing up. So I think it's important to consider the range of applications we are targeting here as well to understand what would welcome what would not

1242
03:03:41.310 --> 03:03:43.590
Matthew Milano: Something that just confuses me in this discussion a little bit

1243
03:03:44.130 --> 03:03:57.300
Matthew Milano: Is we have two points being made once. One is that contention is fundamental and we want to have some linear. So another point that's being made, is we fundamentally are willing to give up on consistency for the white area, and we're going to be less than, than irascibility

1244
03:03:57.330 --> 03:04:06.420
Jonathan Goldstein: Well, well, no, that's not what I'm saying. What I'm saying is that if you want consistency in the white area, you, you, you build on top of this other mechanism. So, so for instance.

1245
03:04:07.380 --> 03:04:15.690
Jonathan Goldstein: I mean, here's a really naive way that you could do that. Right. You could introduce some kind of transaction mechanism is the input to

1246
03:04:16.080 --> 03:04:26.550
Jonathan Goldstein: an ambrosia service, right, and then just use pessimistic locking you know inside the actual, you know, pessimistic locking and distributed two phase commit right to

1247
03:04:27.240 --> 03:04:39.570
Jonathan Goldstein: You know, to. Now, I'm not saying that would be a good way to do it necessarily that that would be like, efficient and all of that. But you, you certainly could do that, you know, on top of the mechanisms that Ambrosia gives you so

1248
03:04:40.890 --> 03:04:50.730
Joe Hellerstein: Let me, let me try something a little different. See if I can pull Matthews point out. So I think in the discussion today particularly across the two groups where the focuses are a bit different.

1249
03:04:52.140 --> 03:05:03.630
Joe Hellerstein: In one case, I think, with a lot of the earlier Berkeley work prior to and vaguely, including the the consensus work. There's this focus on how fast can we go even with contention.

1250
03:05:04.590 --> 03:05:12.540
Joe Hellerstein: If we don't do linear as ability serialized ability we do a different notion of what we need for coordination and consistency.

1251
03:05:13.470 --> 03:05:19.950
Joe Hellerstein: And that takes a couple of flavors and take storage consistency flavors like causal consistency and then it takes full application flavors like

1252
03:05:20.250 --> 03:05:27.600
Joe Hellerstein: You do application program analysis to realize that your whole program doesn't need linear visibility of its variable since doing something monotone

1253
03:05:28.230 --> 03:05:33.600
Joe Hellerstein: So that's a, that's a body of work. And I think, as illustrated by Anna because there is no contention.

1254
03:05:34.020 --> 03:05:41.730
Joe Hellerstein: Under contended work quote unquote contended workloads. You can play games with versions and things like that where you can go arbitrarily fast, essentially, you're getting

1255
03:05:42.000 --> 03:05:47.970
Joe Hellerstein: If you look Deanna paper you're getting like 90% utilization of putting get in the key value store, regardless of contention.

1256
03:05:48.630 --> 03:05:54.510
Joe Hellerstein: Okay, and that's because you've sacrificed on the extreme of consistency. That's one body of work.

1257
03:05:55.440 --> 03:06:05.100
Joe Hellerstein: And then there's a separate body of work, which I think overlaps with the taxes work in Berkeley and ambrosia work. It says, hey, lots of times we like to have the strike guarantees there a nice program contract.

1258
03:06:05.580 --> 03:06:12.330
Joe Hellerstein: Upwards. And in some cases, you really need them. What are good abstractions for that, and how can we push the

1259
03:06:12.780 --> 03:06:20.460
Joe Hellerstein: Overhead of coordination into the corners, so to speak, or into the background to try to make those systems very fast scale up to things that can scale.

1260
03:06:20.850 --> 03:06:29.310
Joe Hellerstein: And postpone the work of coordination or ordering or contention call which you will as much as possible. And I don't think these things are

1261
03:06:30.420 --> 03:06:34.140
Joe Hellerstein: In conflict in a fundamental sense of like one is better than the other.

1262
03:06:35.400 --> 03:06:42.810
Joe Hellerstein: I just think we recognize that there are different and they have uses in different places. Does that make sense. Matthew is that kind of what we're getting at.

1263
03:06:43.020 --> 03:06:49.170
Matthew Milano: Yeah. Sorry, I'm Jonathan, I wasn't trying to say that you were saying both those things I was trying to say these are the two ideas that were in the air and the last

1264
03:06:49.170 --> 03:06:49.350
Jonathan Goldstein: Time.

1265
03:06:49.380 --> 03:06:50.190
Badrish Chandramouli: Yeah, yeah, sure.

1266
03:06:51.540 --> 03:06:55.860
Badrish Chandramouli: I think it catches the overall topic of a bookshop yeah

1267
03:06:56.100 --> 03:06:58.740
Jonathan Goldstein: The interesting question, I guess from my

1268
03:06:58.740 --> 03:06:59.940
Joe Hellerstein: Mind in

1269
03:07:00.030 --> 03:07:09.750
Jonathan Goldstein: From my point of view, I mean, in the end of this all comes down to performance, right, because I mean there's they're two different ways of doing the same thing. You can put the same programming model on top of both of them.

1270
03:07:10.200 --> 03:07:18.270
Jonathan Goldstein: You know, if you like so. So I guess the question becomes, from a performance point of view, you know, what's the bet. You want to make

1271
03:07:18.810 --> 03:07:33.780
Jonathan Goldstein: Now, interestingly enough, if all of your requests are coming in over the network. You know, there is this big linear ization going on anyway, which is those bites are coming in over your neck and your Nick, can only take one bite at a time.

1272
03:07:33.990 --> 03:07:34.260
Matthew Milano: But that

1273
03:07:34.560 --> 03:07:36.060
Matthew Milano: seemed a massive centralization.

1274
03:07:37.800 --> 03:07:39.690
Matthew Milano: That just assume there's only going to be one neck.

1275
03:07:40.680 --> 03:07:40.890
Yeah.

1276
03:07:44.130 --> 03:07:53.760
Sebastian Burckhardt: I think, I think it's important to distinguish between contention that is in the application logic and contention that is sort of an artifact of how we pack things onto servers.

1277
03:07:54.180 --> 03:08:05.820
Sebastian Burckhardt: So ideally, in a server less application rattle, you really can express most applications without contention and any contention that results is really sort of you're doing something wrong in the implementation.

1278
03:08:09.690 --> 03:08:09.900
Joe Hellerstein: Yeah.

1279
03:08:10.290 --> 03:08:14.760
Joe Hellerstein: I mean, I think there's. And again, I think there's two points of view of this. SEBASTIAN I would tend to

1280
03:08:15.210 --> 03:08:19.920
Joe Hellerstein: Like to agree with you, because a lot of the work we did in bloom and other language designs Matthews gala framework.

1281
03:08:20.520 --> 03:08:25.470
Joe Hellerstein: Is trying to get programmers to not accidentally introduce ordering and contention.

1282
03:08:26.460 --> 03:08:39.660
Joe Hellerstein: At the same time, we know that lots of programming models. It's sort of all over the place in the model and to get people off those models is hard and so systems that give you very fast, you know, illusions of a single thread.

1283
03:08:40.680 --> 03:08:41.280
Joe Hellerstein: Pretty handy.

1284
03:08:42.900 --> 03:08:49.410
Joe Hellerstein: So these are both noble goals and effect programmer productivity and kind of different evolution versus revolution kind of ways. Yeah.

1285
03:08:49.920 --> 03:08:56.370
Sebastian Burckhardt: Yeah, so we can take away good things that they already have. But we can we can give them new things that maybe are good in other ways.

1286
03:08:56.850 --> 03:09:06.630
Sebastian Burckhardt: And better with respect to that contention issue. So if you're designing several less programming models with, you know, trying to avoid contention that we can't eliminate

1287
03:09:08.160 --> 03:09:12.990
Sebastian Burckhardt: From the start, then maybe that's a better chance of actually happening.

1288
03:09:13.680 --> 03:09:24.810
Joe Hellerstein: Right. One of the file goals of hydro is that using things like Alvin's verified lifting, we can take sequential code and find places where we can remove some of that contention.

1289
03:09:25.410 --> 03:09:34.740
Joe Hellerstein: Through synthesis and that's part of the goal of our stack having a monotone language at the bottom, or at least the language we're monitoring this is explicit in the type system at the bottom.

1290
03:09:35.430 --> 03:09:43.230
Joe Hellerstein: Is that we have a compiler target that we can do synthesis to from these more traditional programming models.

1291
03:09:44.190 --> 03:09:54.060
Matthew Milano: And in terms of moving programmers on we happen to be lucky that there isn't some large distributed framework that has one. There's no programming model. Everyone's like, this is just how I write distributed code.

1292
03:09:54.480 --> 03:10:01.470
Matthew Milano: There's, there are many sequential imperative models that people want to program against for their single CPUs, but there's sequel.

1293
03:10:02.130 --> 03:10:10.260
Matthew Milano: There's a raft and re style things sorry not left and right. There's raft style things for consensus to spray style things for planning, there was all sorts of programming models that are out there.

1294
03:10:11.010 --> 03:10:21.390
Matthew Milano: This gives us an opportunity to say here's a new toolbox, just like the other doesn't. New two boxes that have been presented to you, that doesn't have contention without having to worry about grabbing someone in yanking them off their c++ hill.

1295
03:10:23.400 --> 03:10:23.610
Yeah.

1296
03:10:24.630 --> 03:10:26.550
Natacha Crooks: I would say maybe that is a little bit of a

1297
03:10:28.050 --> 03:10:31.650
Natacha Crooks: One place to be careful. Is that contention matters when you start contending and at scale.

1298
03:10:32.160 --> 03:10:36.720
Natacha Crooks: And I've seen a number of sort of contention for algorithm that actually perform quite a bit worse in sequential ones.

1299
03:10:37.080 --> 03:10:46.230
Natacha Crooks: At low skills. And I think there's an interesting point at which you have to be careful to say sometimes the sequential thing is that right thing to do. Up until you have 10 machines and that's only when

1300
03:10:47.460 --> 03:10:57.240
Natacha Crooks: I'm being sort of the extra metadata. They need to be contention free becomes relevant and I am curious to see what like how do you switch from one to the other.

1301
03:10:58.290 --> 03:11:04.350
Natacha Crooks: And that's where thing I was point of being able to switch from sequential to coordination free fits in kind of nicely.

1302
03:11:11.010 --> 03:11:19.470
Joe Hellerstein: So one another connection. I think that I'd like to make before we lose each other very soon is between the work we've been doing on multi Paxos optimization

1303
03:11:20.160 --> 03:11:27.840
Joe Hellerstein: And Ambrosia because those are both essentially logging systems. Yep. And you know, I know you had suggested, Jonathan, it might be interesting to do.

1304
03:11:29.430 --> 03:11:40.170
Joe Hellerstein: The paksas work on top of Ambrosia I actually think the other direction might also or instead be interesting, which is if you have to replicate this immortal, that is at the head of the log of Ambrosia you need a

1305
03:11:41.040 --> 03:11:52.440
Joe Hellerstein: You know, you need a log replication protocol our taxes and I think we're doing them well there so it might be fun to try to put some of this together at that level.

1306
03:11:52.980 --> 03:12:04.260
Jonathan Goldstein: But I have to think about what that would mean so. So are you saying that that would mean the the replicas could have lower latency when they fail over. Is that kind of the idea

1307
03:12:05.970 --> 03:12:11.940
Joe Hellerstein: If I understood what you said. Right. You know, the thing that manages the head of the login Ambrosia is a single point of

1308
03:12:13.590 --> 03:12:16.980
Joe Hellerstein: Failure, as well as a single sort of bottleneck in the system.

1309
03:12:17.160 --> 03:12:26.100
Joe Hellerstein: Yes. And so it is a log sequencer which is exactly what multiplexes is it's a state machine replication lag sequencer.

1310
03:12:26.520 --> 03:12:30.930
Joe Hellerstein: Run and we have a we have one that scales very, very well.

1311
03:12:31.560 --> 03:12:43.230
Jonathan Goldstein: Oh, I see what you're saying. But, but the thing is the the protocol that's running for to do the right ahead login Ambrosia is not distributed. It's all within a single

1312
03:12:43.650 --> 03:12:56.040
Jonathan Goldstein: Within a missing a single machine in a single core. The replicas in the Ambrosia model our basic there they're only there to take over, you know they're they're not. You can't like issue reads against them or

1313
03:12:56.640 --> 03:13:08.880
Badrish Chandramouli: What's getting out that single bottleneck. It doesn't matter if it's a single machine, but I think it's can think of it as a multi log like it's it's it's logically a single log, but you don't actually face the bottleneck of that components for on the table.

1314
03:13:08.970 --> 03:13:21.630
Jonathan Goldstein: So, so I guess I guess the need to do that depends on your contention footprint if if there's if there's enough contention that over shared things that you need to span machines at that level.

1315
03:13:22.920 --> 03:13:27.540
Jonathan Goldstein: But is yeah okay let me, let me think about that. Let me think about that. But what that would mean

1316
03:13:29.370 --> 03:13:44.520
Sebastian Burckhardt: That is correct. Like the consensus layer is probably the need the virtualization layer. And I think for Ambrosia what it would replace it. If it would replace the Azure Storage retirement, because you're using Azure storage as you're

1317
03:13:46.140 --> 03:13:54.750
Jonathan Goldstein: Ready, but I'm trying to figure out, but that's not my bottleneck right my bottleneck isn't how quickly I can write to replicated storage unless we're saying that it would reduce latency.

1318
03:13:55.290 --> 03:14:04.290
Jonathan Goldstein: That was, that was my that was why I asked that question. So basically Ambrosia model is sufficiently adaptive in terms of bashing

1319
03:14:04.830 --> 03:14:16.680
Jonathan Goldstein: That whatever the current latency is to get a cert to get a certain level of throughput out of the storage system it will sort of it will grow the batches and and just

1320
03:14:17.730 --> 03:14:24.810
Jonathan Goldstein: You know, to whatever size, they need to be, you know, in order to accommodate storage so me. So that's why I'm trying to understand

1321
03:14:26.220 --> 03:14:39.870
Jonathan Goldstein: So, so that's my storage right now isn't a bottleneck for Ambrosia so that's that's why I'm but but there is a latency cost to going to storage. So that's why I'm trying to understand what the benefit be reducing latency.

1322
03:14:42.540 --> 03:14:43.980
Joe Hellerstein: Sounds like we just take this offline.

1323
03:14:44.250 --> 03:14:44.610
Jonathan Goldstein: Yeah.

1324
03:14:44.940 --> 03:14:48.300
Badrish Chandramouli: And it just one deleted thought, I'm gonna park there later. This is that

1325
03:14:49.740 --> 03:14:53.430
Badrish Chandramouli: I mean, I think there's a there's a lot of parallels between what we're talking about, and

1326
03:14:54.060 --> 03:15:02.340
Badrish Chandramouli: The difference between operation logging and physiological logging in databases, right, if you think of what faster is doing. It is really physiological

1327
03:15:03.270 --> 03:15:10.440
Badrish Chandramouli: And center. It's not like civilizing the operations to the store, but it's just realizing the deltas to the to the database.

1328
03:15:11.070 --> 03:15:23.040
Badrish Chandramouli: Internet physiological sense. So that's an interesting trade off there. And a question is how well that works for servers or state. It doesn't kind of percolate back to the clients. And that's a very interesting trade off there.

1329
03:15:26.280 --> 03:15:33.960
Jonathan Goldstein: Yeah, that's actually something that's kind of interesting. There's a meta thing here. That's interesting, which is that these kinds of recovery.

1330
03:15:34.740 --> 03:15:41.820
Jonathan Goldstein: Protocols, they can kind of be chopped into two two categories, right there, the categories that get recover ability by

1331
03:15:42.210 --> 03:15:51.780
Jonathan Goldstein: Doing some kind of logging of state changes. That's the result of running code versus something like Ambrosia where you're logging the the

1332
03:15:52.290 --> 03:16:05.130
Jonathan Goldstein: stimuli that cause code to execute that then causes state changes so performance considerations are very, very different for those two styles of logging and recovery.

1333
03:16:06.210 --> 03:16:15.870
Jonathan Goldstein: For instance, if you have an application that modifies you know you're making light room you know and and you modify images, then

1334
03:16:16.530 --> 03:16:32.400
Jonathan Goldstein: You know, with one you know simple instruction to run a piece of code, you may modify get, you know, megabytes and megabytes and megabytes of memory. So if you have to log the state change to memory that's a huge amount of data, but um

1335
03:16:33.600 --> 03:16:43.050
Jonathan Goldstein: But if you only have to log the, the fact that, well, I applied this filter to this image, then it's it's a lot less so. So maybe some of

1336
03:16:43.500 --> 03:16:48.660
Jonathan Goldstein: where I'm coming from. And maybe with the previous conversation is that when I think about

1337
03:16:49.260 --> 03:16:57.180
Jonathan Goldstein: Ambrosia performance, both in terms of getting things over a network card and writing things to storage. I'm not writing the changes to state.

1338
03:16:57.720 --> 03:17:01.950
Jonathan Goldstein: I'm I'm writing the requests to execute code and

1339
03:17:02.880 --> 03:17:17.970
Jonathan Goldstein: So if you assume that that all originates from outside the place that you're running from that the the the decision to execute that code is coming from the outside, then that sort of places a bound on how big storage can get relative to what's coming in.

1340
03:17:19.770 --> 03:17:22.920
Jonathan Goldstein: On how many bites you need to move around relative to what comes in.

1341
03:17:24.360 --> 03:17:29.160
Jonathan Goldstein: And that's why we're only a factor of two relative to GR PC. It's exactly because of that.

1342
03:17:31.020 --> 03:17:31.950
Jonathan Goldstein: Does that make sense.

1343
03:17:33.030 --> 03:17:36.300
Joe Hellerstein: I think we're passing each other in the night on the points we're making

1344
03:17:37.740 --> 03:17:38.130
Joe Hellerstein: A bad

1345
03:17:39.630 --> 03:17:53.850
Joe Hellerstein: Thing. And so that's something we can try to sort out offline perhaps try to capture some brief notes in the slack and I'm going to wrangle the horrible JSON that that slack gives you out of this and turn it into a doc and some point

1346
03:17:55.080 --> 03:18:04.380
Joe Hellerstein: This has been fun. One thing we didn't do yet but maybe we can do offline is figure out if there is a like an obvious next step we know a little bit more about each other's work, which is really good.

1347
03:18:05.640 --> 03:18:13.470
Joe Hellerstein: You know, conveniently, Sergio and I jointly run foundations and trends and databases. So one thing we can try to do

1348
03:18:13.980 --> 03:18:24.150
Joe Hellerstein: Is to see if there's an outline for a trends article that we might want to be writing here another possibilities to think about a much shorter thing for some place like data engineering building

1349
03:18:25.260 --> 03:18:27.720
Joe Hellerstein: Where we just even just report out on this conversation.

1350
03:18:28.320 --> 03:18:36.030
Joe Hellerstein: So let's put those as ideas for going forward and at minimum, let's let's keep in touch.

1351
03:18:36.390 --> 03:18:36.780
Jonathan Goldstein: Yeah.

1352
03:18:36.840 --> 03:18:39.060
Jonathan Goldstein: Sounds good. In the end, like I said, there were

1353
03:18:39.090 --> 03:18:54.480
Jonathan Goldstein: Like a few points where we said, hey, if there are threads that we want to keep going. That where there's mutual interest in working on stuff, then I, you know, like I said any anybody that's anybody that wants to do that, you know, come, come talk to us.

1354
03:18:55.530 --> 03:19:02.580
Jonathan Goldstein: One thing that's nice about Ambrosia is that a lot of the work that we do on Ambrosia is out in the open and you know it.

1355
03:19:03.420 --> 03:19:18.720
Jonathan Goldstein: It doesn't have to be. It can be, but it doesn't have to be somebody coming actually coming here working under NDA and all of that, there they are things that we can do the that are outside that framework. So, you know, just keep it in mind.

1356
03:19:19.230 --> 03:19:27.510
Badrish Chandramouli: And say manifested as well. So I think that echo system is open source, open your contributions and just ping us if you have any sort of collaborate

1357
03:19:29.610 --> 03:19:30.030
Joe Hellerstein: Awesome.

1358
03:19:30.390 --> 03:19:33.690
Joe Hellerstein: I will point out that many of the students here undergrads and first year.

1359
03:19:33.690 --> 03:19:34.230
Badrish Chandramouli: grads.

1360
03:19:34.320 --> 03:19:35.670
Joe Hellerstein: We have a really young group so

1361
03:19:35.670 --> 03:19:36.750
Joe Hellerstein: Let's people be looking for.

1362
03:19:36.750 --> 03:19:38.250
Joe Hellerstein: Interesting. Well, so

1363
03:19:38.880 --> 03:19:39.210
Joe Hellerstein: Picking up

1364
03:19:41.310 --> 03:19:41.730
Joe Hellerstein: I will

1365
03:19:41.880 --> 03:19:44.100
Joe Hellerstein: Send out the membership list and all that.

1366
03:19:44.100 --> 03:19:45.960
Joe Hellerstein: Stuff. It's in the calendar invite

1367
03:19:45.960 --> 03:19:47.190
Joe Hellerstein: Actually, so you

1368
03:19:47.460 --> 03:19:47.970
Badrish Chandramouli: Kind of have it.

1369
03:19:48.510 --> 03:19:49.050
As follows

1370
03:19:50.220 --> 03:19:50.580
Joe Hellerstein: Okay.

1371
03:19:52.860 --> 03:19:53.070
Badrish Chandramouli: Yeah.

1372
03:19:53.100 --> 03:20:01.500
Jonathan Goldstein: Thanks so much for putting you know I like you did a lot of the logistics to kind of get this going. And initiated it, you know, and all of that, and I really appreciate

1373
03:20:01.500 --> 03:20:03.360
Jonathan Goldstein: It so. So thanks a lot for this has been

1374
03:20:03.360 --> 03:20:08.070
Badrish Chandramouli: Great. Yeah. Likewise, his great presentation and organization. Thanks everyone.

1375
03:20:08.310 --> 03:20:09.510
Joe Hellerstein: Thanks to everyone who presented

1376
03:20:09.570 --> 03:20:11.490
Badrish Chandramouli: On both sides. Bye bye.

1377
03:20:11.880 --> 03:20:12.660
Badrish Chandramouli: See ya. Thank you.

1378
03:20:13.170 --> 03:20:13.530
Michel Pahud: Bye bye.

1379
03:20:14.400 --> 03:20:15.540
Michel Pahud: Thank you, everyone. Bye.

